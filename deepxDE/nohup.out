Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna.py", line 74, in <module>
    study = optuna.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
AttributeError: module 'optuna' has no attribute 'load_study'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna.py", line 74, in <module>
    study = optuna.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
AttributeError: module 'optuna' has no attribute 'load_study'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna.py", line 75, in <module>
    study = optuna.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
AttributeError: module 'optuna' has no attribute 'load_study'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna.py", line 75, in <module>
    study = optuna.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
AttributeError: module 'optuna' has no attribute 'load_study'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna.py", line 75, in <module>
    study = optuna.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
AttributeError: module 'optuna' has no attribute 'load_study'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000179 s

[W 2023-10-08 14:03:07,094] Trial 0 failed with parameters: {'num_domain': 66585, 'num_boundary': 2681, 'lr': 1.2990436759744085e-05} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 7.77 GiB already allocated; 71.62 MiB free; 7.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 63, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 610, in train
    self._test()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 797, in _test
    ) = self._outputs_losses(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 522, in _outputs_losses
    outs = outputs_losses(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 303, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 291, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/pde.py", line 127, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 92, in pde2d_vm
    dw_dt = dde.grad.jacobian(y, x, i=1, j=2)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 181, in jacobian
    return jacobian._Jacobians(ys, xs, i=i, j=j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 153, in __call__
    return self.Js[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 7.77 GiB already allocated; 71.62 MiB free; 7.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-08 14:03:07,192] Trial 0 failed with value None.
Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 76, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 63, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 610, in train
    self._test()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 797, in _test
    ) = self._outputs_losses(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 522, in _outputs_losses
    outs = outputs_losses(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 303, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 291, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/pde.py", line 127, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 92, in pde2d_vm
    dw_dt = dde.grad.jacobian(y, x, i=1, j=2)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 181, in jacobian
    return jacobian._Jacobians(ys, xs, i=i, j=j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 153, in __call__
    return self.Js[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 7.77 GiB already allocated; 71.62 MiB free; 7.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 2, in <module>
    import heart.MSc.deepxDE.optuna_tune as optuna_tune
ModuleNotFoundError: No module named 'heart'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 76, in <module>
    study = optuna_tune.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
NameError: name 'optuna_tune' is not defined
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000150 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [5.57e+01, 7.77e-05, 3.29e-07, 3.61e+03, 3.61e+03]    [5.57e+01, 7.77e-05, 3.29e-07, 3.61e+03, 3.61e+03]    []  
[W 2023-10-08 14:10:45,221] Trial 1 failed with parameters: {'num_domain': 72357, 'num_boundary': 1093, 'lr': 0.006672856174473702} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 130.00 MiB (GPU 1; 10.75 GiB total capacity; 9.67 GiB already allocated; 19.62 MiB free; 9.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 64, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 335, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 303, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 291, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/pde.py", line 127, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 91, in pde2d_vm
    dv_dyy = dde.grad.hessian(y, x, component=0, i=1, j=1)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 283, in hessian
    return hessian._Hessians(ys, xs, component=component, i=i, j=j, grad_y=grad_y)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 251, in __call__
    return self.Hs[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 228, in __call__
    return self.H(i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 1; 10.75 GiB total capacity; 9.67 GiB already allocated; 19.62 MiB free; 9.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-08 14:10:45,254] Trial 1 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 64, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 335, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 303, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 291, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/pde.py", line 127, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 91, in pde2d_vm
    dv_dyy = dde.grad.hessian(y, x, component=0, i=1, j=1)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 283, in hessian
    return hessian._Hessians(ys, xs, component=component, i=i, j=j, grad_y=grad_y)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 251, in __call__
    return self.Hs[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 228, in __call__
    return self.H(i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 1; 10.75 GiB total capacity; 9.67 GiB already allocated; 19.62 MiB free; 9.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000123 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.15e+01, 1.29e-04, 2.05e-07, 3.61e+03, 3.61e+03]    [3.15e+01, 1.29e-04, 2.05e-07, 3.61e+03, 3.61e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000143 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.51e+02, 2.05e-03, 1.06e-07, 3.60e+03, 3.60e+03]    [1.51e+02, 2.05e-03, 1.06e-07, 3.60e+03, 3.60e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000148 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [6.07e+01, 1.08e-04, 1.47e-07, 3.61e+03, 3.61e+03]    [6.07e+01, 1.08e-04, 1.47e-07, 3.61e+03, 3.61e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000396 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.99e+01, 2.27e-05, 2.29e-07, 3.58e+03, 3.58e+03]    [3.99e+01, 2.27e-05, 2.29e-07, 3.58e+03, 3.58e+03]    []  
1000      [6.46e-07, 1.09e-03, 4.31e-16, 2.11e+03, 2.11e+03]    [6.46e-07, 1.09e-03, 4.31e-16, 2.11e+03, 2.11e+03]    []  
1000      [3.27e-07, 1.07e-03, 8.20e-16, 2.18e+03, 2.18e+03]    [3.27e-07, 1.07e-03, 8.20e-16, 2.18e+03, 2.18e+03]    []  
1000      [1.47e-07, 8.72e-04, 7.01e-13, 2.49e+03, 2.49e+03]    [1.47e-07, 8.72e-04, 7.01e-13, 2.49e+03, 2.49e+03]    []  
1000      [1.13e-03, 4.14e-04, 2.92e-05, 1.11e+01, 1.11e+01]    [1.13e-03, 4.14e-04, 2.92e-05, 1.11e+01, 1.11e+01]    []  
2000      [1.74e-09, 1.05e-03, 3.56e-16, 2.09e+03, 2.09e+03]    [1.74e-09, 1.05e-03, 3.56e-16, 2.09e+03, 2.09e+03]    []  
2000      [4.63e-08, 1.07e-03, 1.57e-16, 2.10e+03, 2.10e+03]    [4.63e-08, 1.07e-03, 1.57e-16, 2.10e+03, 2.10e+03]    []  
2000      [2.42e-07, 1.01e-03, 1.36e-14, 2.28e+03, 2.28e+03]    [2.42e-07, 1.01e-03, 1.36e-14, 2.28e+03, 2.28e+03]    []  
2000      [8.17e-05, 1.22e-04, 1.73e-08, 5.04e+00, 5.04e+00]    [8.17e-05, 1.22e-04, 1.73e-08, 5.04e+00, 5.04e+00]    []  
3000      [8.12e-11, 1.05e-03, 4.01e-17, 2.09e+03, 2.09e+03]    [8.12e-11, 1.05e-03, 4.01e-17, 2.09e+03, 2.09e+03]    []  
3000      [3.58e-10, 1.05e-03, 1.22e-16, 2.09e+03, 2.09e+03]    [3.58e-10, 1.05e-03, 1.22e-16, 2.09e+03, 2.09e+03]    []  
3000      [3.86e-04, 6.05e-05, 1.14e-07, 2.49e+00, 2.49e+00]    [3.86e-04, 6.05e-05, 1.14e-07, 2.49e+00, 2.49e+00]    []  
3000      [4.12e-07, 1.08e-03, 2.54e-15, 2.16e+03, 2.16e+03]    [4.12e-07, 1.08e-03, 2.54e-15, 2.16e+03, 2.16e+03]    []  
4000      [7.27e-11, 1.05e-03, 9.72e-17, 2.09e+03, 2.09e+03]    [7.27e-11, 1.05e-03, 9.72e-17, 2.09e+03, 2.09e+03]    []  
4000      [5.71e-11, 1.05e-03, 3.12e-17, 2.09e+03, 2.09e+03]    [5.71e-11, 1.05e-03, 3.12e-17, 2.09e+03, 2.09e+03]    []  
4000      [1.73e-04, 5.45e-05, 1.31e-06, 6.69e+00, 6.69e+00]    [1.73e-04, 5.45e-05, 1.31e-06, 6.69e+00, 6.69e+00]    []  
5000      [8.74e-11, 1.05e-03, 3.34e-16, 2.09e+03, 2.09e+03]    [8.74e-11, 1.05e-03, 3.34e-16, 2.09e+03, 2.09e+03]    []  
4000      [2.13e-07, 1.09e-03, 2.48e-15, 2.11e+03, 2.11e+03]    [2.13e-07, 1.09e-03, 2.48e-15, 2.11e+03, 2.11e+03]    []  
5000      [6.40e-11, 1.05e-03, 7.81e-18, 2.09e+03, 2.09e+03]    [6.40e-11, 1.05e-03, 7.81e-18, 2.09e+03, 2.09e+03]    []  
6000      [4.00e-03, 3.58e-04, 6.92e-07, 8.14e+01, 8.14e+01]    [4.00e-03, 3.58e-04, 6.92e-07, 8.14e+01, 8.14e+01]    []  
5000      [2.05e-04, 6.27e-05, 7.31e-07, 1.08e+00, 1.08e+00]    [2.05e-04, 6.27e-05, 7.31e-07, 1.08e+00, 1.08e+00]    []  
5000      [4.41e-08, 1.07e-03, 6.33e-16, 2.10e+03, 2.10e+03]    [4.41e-08, 1.07e-03, 6.33e-16, 2.10e+03, 2.10e+03]    []  
6000      [6.15e-11, 1.05e-03, 2.36e-18, 2.09e+03, 2.09e+03]    [6.15e-11, 1.05e-03, 2.36e-18, 2.09e+03, 2.09e+03]    []  
7000      [3.63e-03, 2.68e-04, 2.83e-05, 1.20e+01, 1.20e+01]    [3.63e-03, 2.68e-04, 2.83e-05, 1.20e+01, 1.20e+01]    []  
6000      [1.38e-04, 4.19e-05, 7.51e-07, 6.03e-01, 6.03e-01]    [1.38e-04, 4.19e-05, 7.51e-07, 6.03e-01, 6.03e-01]    []  
7000      [6.67e-11, 1.05e-03, 6.11e-19, 2.09e+03, 2.09e+03]    [6.67e-11, 1.05e-03, 6.11e-19, 2.09e+03, 2.09e+03]    []  
6000      [3.00e-09, 1.05e-03, 2.25e-16, 2.09e+03, 2.09e+03]    [3.00e-09, 1.05e-03, 2.25e-16, 2.09e+03, 2.09e+03]    []  
8000      [1.56e-03, 6.06e-04, 1.24e-05, 9.78e+00, 9.78e+00]    [1.56e-03, 6.06e-04, 1.24e-05, 9.78e+00, 9.78e+00]    []  
8000      [5.84e-11, 1.05e-03, 7.66e-19, 2.09e+03, 2.09e+03]    [5.84e-11, 1.05e-03, 7.66e-19, 2.09e+03, 2.09e+03]    []  
7000      [2.25e-04, 2.66e-05, 8.16e-07, 6.42e+00, 6.42e+00]    [2.25e-04, 2.66e-05, 8.16e-07, 6.42e+00, 6.42e+00]    []  
7000      [1.28e-10, 1.05e-03, 6.22e-17, 2.09e+03, 2.09e+03]    [1.28e-10, 1.05e-03, 6.22e-17, 2.09e+03, 2.09e+03]    []  
9000      [1.36e-03, 5.31e-04, 2.18e-05, 8.59e+00, 8.59e+00]    [1.36e-03, 5.31e-04, 2.18e-05, 8.59e+00, 8.59e+00]    []  
9000      [2.61e-01, 5.42e-04, 2.77e-03, 3.62e+02, 3.62e+02]    [2.61e-01, 5.42e-04, 2.77e-03, 3.62e+02, 3.62e+02]    []  
8000      [8.11e-05, 3.04e-05, 6.66e-07, 1.66e+00, 1.66e+00]    [8.11e-05, 3.04e-05, 6.66e-07, 1.66e+00, 1.66e+00]    []  
8000      [6.79e-11, 1.05e-03, 5.03e-17, 2.09e+03, 2.09e+03]    [6.79e-11, 1.05e-03, 5.03e-17, 2.09e+03, 2.09e+03]    []  
10000     [6.55e-04, 4.75e-04, 6.19e-06, 7.31e+00, 7.31e+00]    [6.55e-04, 4.75e-04, 6.19e-06, 7.31e+00, 7.31e+00]    []  
10000     [2.82e-02, 1.82e-04, 3.26e-04, 2.73e+01, 2.73e+01]    [2.82e-02, 1.82e-04, 3.26e-04, 2.73e+01, 2.73e+01]    []  
9000      [4.65e-07, 1.29e-06, 2.26e-09, 1.01e+00, 1.01e+00]    [4.65e-07, 1.29e-06, 2.26e-09, 1.01e+00, 1.01e+00]    []  
9000      [7.87e-11, 1.05e-03, 2.21e-17, 2.09e+03, 2.09e+03]    [7.87e-11, 1.05e-03, 2.21e-17, 2.09e+03, 2.09e+03]    []  
11000     [5.38e-04, 3.58e-04, 6.81e-06, 6.76e+00, 6.76e+00]    [5.38e-04, 3.58e-04, 6.81e-06, 6.76e+00, 6.76e+00]    []  
11000     [5.39e-03, 6.37e-04, 9.18e-05, 9.96e+00, 9.96e+00]    [5.39e-03, 6.37e-04, 9.18e-05, 9.96e+00, 9.96e+00]    []  
10000     [2.61e-06, 1.19e-07, 9.75e-10, 7.44e+00, 7.44e+00]    [2.61e-06, 1.19e-07, 9.75e-10, 7.44e+00, 7.44e+00]    []  
10000     [7.38e-11, 1.05e-03, 1.92e-16, 2.09e+03, 2.09e+03]    [7.38e-11, 1.05e-03, 1.92e-16, 2.09e+03, 2.09e+03]    []  
12000     [5.63e-04, 3.47e-04, 1.44e-05, 5.14e+00, 5.14e+00]    [5.63e-04, 3.47e-04, 1.44e-05, 5.14e+00, 5.14e+00]    []  
12000     [1.95e-03, 8.53e-04, 6.63e-05, 8.48e+00, 8.48e+00]    [1.95e-03, 8.53e-04, 6.63e-05, 8.48e+00, 8.48e+00]    []  
11000     [9.50e-07, 5.81e-08, 9.87e-09, 3.27e-01, 3.27e-01]    [9.50e-07, 5.81e-08, 9.87e-09, 3.27e-01, 3.27e-01]    []  
13000     [6.14e-04, 3.34e-04, 1.30e-06, 3.74e+00, 3.74e+00]    [6.14e-04, 3.34e-04, 1.30e-06, 3.74e+00, 3.74e+00]    []  
13000     [2.21e-03, 1.01e-03, 5.07e-05, 7.46e+00, 7.46e+00]    [2.21e-03, 1.01e-03, 5.07e-05, 7.46e+00, 7.46e+00]    []  
11000     [2.81e-01, 9.85e-04, 1.29e-03, 7.81e+02, 7.81e+02]    [2.81e-01, 9.85e-04, 1.29e-03, 7.81e+02, 7.81e+02]    []  
12000     [1.78e-06, 2.26e-07, 1.25e-09, 2.46e-01, 2.46e-01]    [1.78e-06, 2.26e-07, 1.25e-09, 2.46e-01, 2.46e-01]    []  
14000     [6.78e-04, 4.96e-04, 5.03e-06, 3.75e+00, 3.75e+00]    [6.78e-04, 4.96e-04, 5.03e-06, 3.75e+00, 3.75e+00]    []  
14000     [1.10e-03, 1.02e-03, 1.57e-05, 6.30e+00, 6.30e+00]    [1.10e-03, 1.02e-03, 1.57e-05, 6.30e+00, 6.30e+00]    []  
12000     [1.26e-02, 4.18e-04, 6.29e-04, 3.36e+02, 3.36e+02]    [1.26e-02, 4.18e-04, 6.29e-04, 3.36e+02, 3.36e+02]    []  
13000     [4.73e-06, 5.55e-07, 1.27e-09, 1.83e-01, 1.83e-01]    [4.73e-06, 5.55e-07, 1.27e-09, 1.83e-01, 1.83e-01]    []  
15000     [9.39e-04, 1.15e-03, 3.41e-06, 5.20e+00, 5.20e+00]    [9.39e-04, 1.15e-03, 3.41e-06, 5.20e+00, 5.20e+00]    []  
15000     [6.57e-04, 4.89e-04, 9.50e-07, 2.42e+00, 2.42e+00]    [6.57e-04, 4.89e-04, 9.50e-07, 2.42e+00, 2.42e+00]    []  
                                                                                                                                                                                          13000     [2.91e-02, 3.22e-04, 7.55e-03, 1.59e+02, 1.59e+02]    [2.91e-02, 3.22e-04, 7.55e-03, 1.59e+02, 1.59e+02]    []  
                                                                                                                                                                                          16000     [1.10e-03, 1.34e-03, 2.32e-06, 4.05e+00, 4.05e+00]    [1.10e-03, 1.34e-03, 2.32e-06, 4.05e+00, 4.05e+00]    []  
14000     [1.33e-05, 8.77e-07, 7.81e-09, 1.87e-01, 1.87e-01]    [1.33e-05, 8.77e-07, 7.81e-09, 1.87e-01, 1.87e-01]    []  
16000     [6.56e-04, 4.90e-04, 1.53e-06, 2.09e+00, 2.09e+00]    [6.56e-04, 4.90e-04, 1.53e-06, 2.09e+00, 2.09e+00]    []  
14000     [1.36e-02, 3.46e-04, 1.68e-03, 7.13e+01, 7.13e+01]    [1.36e-02, 3.46e-04, 1.68e-03, 7.13e+01, 7.13e+01]    []  
17000     [1.40e-03, 1.07e-03, 2.47e-06, 3.09e+00, 3.09e+00]    [1.40e-03, 1.07e-03, 2.47e-06, 3.09e+00, 3.09e+00]    []  
17000     [6.31e-04, 5.08e-04, 6.18e-07, 1.88e+00, 1.88e+00]    [6.31e-04, 5.08e-04, 6.18e-07, 1.88e+00, 1.88e+00]    []  
15000     [3.09e-05, 5.24e-06, 3.24e-06, 3.14e+00, 3.14e+00]    [3.09e-05, 5.24e-06, 3.24e-06, 3.14e+00, 3.14e+00]    []  
15000     [5.78e-03, 5.73e-04, 1.44e-03, 3.00e+01, 3.00e+01]    [5.78e-03, 5.73e-04, 1.44e-03, 3.00e+01, 3.00e+01]    []  
18000     [1.24e-03, 1.12e-03, 1.50e-06, 2.50e+00, 2.50e+00]    [1.24e-03, 1.12e-03, 1.50e-06, 2.50e+00, 2.50e+00]    []  
18000     [6.37e-04, 4.91e-04, 1.27e-06, 1.57e+00, 1.57e+00]    [6.37e-04, 4.91e-04, 1.27e-06, 1.57e+00, 1.57e+00]    []  
16000     [5.67e-05, 7.91e-06, 9.06e-10, 3.42e+00, 3.42e+00]    [5.67e-05, 7.91e-06, 9.06e-10, 3.42e+00, 3.42e+00]    []  
19000     [1.28e-03, 1.15e-03, 3.43e-06, 2.15e+00, 2.15e+00]    [1.28e-03, 1.15e-03, 3.43e-06, 2.15e+00, 2.15e+00]    []  
16000     [5.80e-03, 9.64e-04, 1.02e-03, 1.30e+01, 1.30e+01]    [5.80e-03, 9.64e-04, 1.02e-03, 1.30e+01, 1.30e+01]    []  
19000     [8.59e-04, 7.09e-04, 1.02e-06, 1.40e+00, 1.40e+00]    [8.59e-04, 7.09e-04, 1.02e-06, 1.40e+00, 1.40e+00]    []  
17000     [1.69e-05, 5.21e-07, 1.62e-07, 1.85e-01, 1.85e-01]    [1.69e-05, 5.21e-07, 1.62e-07, 1.85e-01, 1.85e-01]    []  
20000     [1.10e-03, 9.75e-04, 5.97e-06, 1.80e+00, 1.80e+00]    [1.10e-03, 9.75e-04, 5.97e-06, 1.80e+00, 1.80e+00]    []  
20000     [8.28e-04, 8.64e-04, 4.21e-07, 1.22e+00, 1.22e+00]    [8.28e-04, 8.64e-04, 4.21e-07, 1.22e+00, 1.22e+00]    []  
17000     [4.72e-03, 1.06e-03, 9.11e-04, 8.13e+00, 8.13e+00]    [4.72e-03, 1.06e-03, 9.11e-04, 8.13e+00, 8.13e+00]    []  
18000     [5.96e-06, 3.12e-07, 2.66e-09, 6.86e-02, 6.86e-02]    [5.96e-06, 3.12e-07, 2.66e-09, 6.86e-02, 6.86e-02]    []  
21000     [9.28e-04, 7.84e-04, 1.02e-05, 1.60e+00, 1.60e+00]    [9.28e-04, 7.84e-04, 1.02e-05, 1.60e+00, 1.60e+00]    []  
21000     [1.05e-03, 8.02e-04, 8.51e-06, 1.57e+00, 1.57e+00]    [1.05e-03, 8.02e-04, 8.51e-06, 1.57e+00, 1.57e+00]    []  
18000     [6.68e-03, 1.06e-03, 4.60e-04, 6.85e+00, 6.85e+00]    [6.68e-03, 1.06e-03, 4.60e-04, 6.85e+00, 6.85e+00]    []  
19000     [6.37e-06, 1.64e-07, 4.16e-09, 1.28e-01, 1.28e-01]    [6.37e-06, 1.64e-07, 4.16e-09, 1.28e-01, 1.28e-01]    []  
22000     [8.71e-04, 6.82e-04, 1.10e-05, 1.32e+00, 1.32e+00]    [8.71e-04, 6.82e-04, 1.10e-05, 1.32e+00, 1.32e+00]    []  
22000     [7.31e-04, 6.44e-04, 6.29e-07, 1.21e+00, 1.21e+00]    [7.31e-04, 6.44e-04, 6.29e-07, 1.21e+00, 1.21e+00]    []  
19000     [2.46e-03, 1.00e-03, 4.63e-05, 6.00e+00, 6.00e+00]    [2.46e-03, 1.00e-03, 4.63e-05, 6.00e+00, 6.00e+00]    []  
                                                               20000     [8.09e-06, 7.51e-07, 3.61e-07, 8.35e-02, 8.35e-02]    [8.09e-06, 7.51e-07, 3.61e-07, 8.35e-02, 8.35e-02]    []  
                           23000     [7.09e-04, 6.65e-04, 1.23e-05, 1.09e+00, 1.09e+00]    [7.09e-04, 6.65e-04, 1.23e-05, 1.09e+00, 1.09e+00]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000288 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.93e+01, 3.37e-05, 1.26e-07, 3.61e+03, 3.61e+03]    [4.93e+01, 3.37e-05, 1.26e-07, 3.61e+03, 3.61e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000339 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.38e+01, 1.45e-04, 1.14e-07, 3.60e+03, 3.60e+03]    [1.38e+01, 1.45e-04, 1.14e-07, 3.60e+03, 3.60e+03]    []  
                                                                                                                           Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000358 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.37e+01, 1.79e-04, 2.80e-07, 3.60e+03, 3.60e+03]    [1.37e+01, 1.79e-04, 2.80e-07, 3.60e+03, 3.60e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000404 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.27e+01, 6.08e-05, 2.00e-07, 3.61e+03, 3.61e+03]    [4.27e+01, 6.08e-05, 2.00e-07, 3.61e+03, 3.61e+03]    []  
[W 2023-10-08 15:20:39,151] Trial 9 failed with parameters: {'num_domain': 62221, 'num_boundary': 3084, 'lr': 0.0037607501004752394} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 80.00 MiB (GPU 1; 39.43 GiB total capacity; 5.40 GiB already allocated; 40.31 MiB free; 5.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 64, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 632, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 650, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 544, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 346, in train_step
    self.opt.step(closure)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/optim/adam.py", line 92, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 340, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 308, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 296, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/data/pde.py", line 140, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 91, in pde2d_vm
    dv_dyy = dde.grad.hessian(y, x, component=0, i=1, j=1)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 283, in hessian
    return hessian._Hessians(ys, xs, component=component, i=i, j=j, grad_y=grad_y)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 251, in __call__
    return self.Hs[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 228, in __call__
    return self.H(i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 1; 39.43 GiB total capacity; 5.40 GiB already allocated; 40.31 MiB free; 5.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-08 15:20:39,231] Trial 9 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 64, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 632, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 650, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 544, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 346, in train_step
    self.opt.step(closure)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/optim/adam.py", line 92, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 340, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 308, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 296, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/data/pde.py", line 140, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 91, in pde2d_vm
    dv_dyy = dde.grad.hessian(y, x, component=0, i=1, j=1)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 283, in hessian
    return hessian._Hessians(ys, xs, component=component, i=i, j=j, grad_y=grad_y)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 251, in __call__
    return self.Hs[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 228, in __call__
    return self.H(i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 1; 39.43 GiB total capacity; 5.40 GiB already allocated; 40.31 MiB free; 5.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       21000     [9.17e-04, 8.10e-04, 5.78e-05, 4.39e+00, 4.39e+00]    [9.17e-04, 8.10e-04, 5.78e-05, 4.39e+00, 4.39e+00]    []  
22000     [3.29e-06, 6.79e-07, 2.03e-09, 1.39e-01, 1.39e-01]    [3.29e-06, 6.79e-07, 2.03e-09, 1.39e-01, 1.39e-01]    []  
25000     [4.37e-04, 1.63e-04, 1.24e-06, 6.61e-01, 6.61e-01]    [4.37e-04, 1.63e-04, 1.24e-06, 6.61e-01, 6.61e-01]    []  
                                                                                                                           26000     [3.16e-04, 3.44e-04, 9.51e-06, 6.53e-01, 6.53e-01]    [3.16e-04, 3.44e-04, 9.51e-06, 6.53e-01, 6.53e-01]    []  
22000     [1.13e-03, 5.73e-04, 3.71e-04, 3.64e+00, 3.64e+00]    [1.13e-03, 5.73e-04, 3.71e-04, 3.64e+00, 3.64e+00]    []  
23000     [4.93e-06, 3.45e-07, 5.37e-09, 5.40e-02, 5.40e-02]    [4.93e-06, 3.45e-07, 5.37e-09, 5.40e-02, 5.40e-02]    []  
26000     [2.95e-04, 8.12e-05, 1.22e-06, 7.22e-01, 7.22e-01]    [2.95e-04, 8.12e-05, 1.22e-06, 7.22e-01, 7.22e-01]    []  
                                                                                                                           27000     [3.32e-04, 3.43e-04, 7.11e-06, 5.98e-01, 5.98e-01]    [3.32e-04, 3.43e-04, 7.11e-06, 5.98e-01, 5.98e-01]    []  
                                                                                                                           23000     [1.05e-03, 5.20e-04, 2.65e-04, 2.91e+00, 2.91e+00]    [1.05e-03, 5.20e-04, 2.65e-04, 2.91e+00, 2.91e+00]    []  
24000     [5.26e-06, 2.20e-07, 6.22e-09, 4.68e-02, 4.68e-02]    [5.26e-06, 2.20e-07, 6.22e-09, 4.68e-02, 4.68e-02]    []  
27000     [2.20e-04, 5.28e-05, 4.09e-07, 9.60e-01, 9.60e-01]    [2.20e-04, 5.28e-05, 4.09e-07, 9.60e-01, 9.60e-01]    []  
28000     [2.93e-04, 3.17e-04, 8.50e-06, 5.25e-01, 5.25e-01]    [2.93e-04, 3.17e-04, 8.50e-06, 5.25e-01, 5.25e-01]    []  
28000     [1.73e-04, 4.37e-05, 4.93e-07, 4.83e-01, 4.83e-01]    [1.73e-04, 4.37e-05, 4.93e-07, 4.83e-01, 4.83e-01]    []  
25000     [5.90e-06, 1.53e-07, 1.57e-09, 5.38e-02, 5.38e-02]    [5.90e-06, 1.53e-07, 1.57e-09, 5.38e-02, 5.38e-02]    []  
24000     [1.16e-03, 6.90e-04, 2.20e-04, 2.38e+00, 2.38e+00]    [1.16e-03, 6.90e-04, 2.20e-04, 2.38e+00, 2.38e+00]    []  
29000     [2.56e-04, 2.93e-04, 7.29e-06, 4.78e-01, 4.78e-01]    [2.56e-04, 2.93e-04, 7.29e-06, 4.78e-01, 4.78e-01]    []  
                                                                                                                                                                                                                                                      29000     [1.75e-04, 4.55e-05, 3.35e-07, 4.34e-01, 4.34e-01]    [1.75e-04, 4.55e-05, 3.35e-07, 4.34e-01, 4.34e-01]    []  
26000     [9.65e-06, 5.24e-07, 7.15e-09, 5.50e-02, 5.50e-02]    [9.65e-06, 5.24e-07, 7.15e-09, 5.50e-02, 5.50e-02]    []  
25000     [9.22e-04, 1.26e-03, 1.82e-04, 2.04e+00, 2.04e+00]    [9.22e-04, 1.26e-03, 1.82e-04, 2.04e+00, 2.04e+00]    []  
30000     [2.47e-04, 2.63e-04, 8.39e-06, 4.86e-01, 4.86e-01]    [2.47e-04, 2.63e-04, 8.39e-06, 4.86e-01, 4.86e-01]    []  
30000     [1.64e-04, 4.15e-05, 4.09e-07, 4.30e-01, 4.30e-01]    [1.64e-04, 4.15e-05, 4.09e-07, 4.30e-01, 4.30e-01]    []  
27000     [8.30e-06, 3.89e-07, 4.83e-09, 4.41e-02, 4.41e-02]    [8.30e-06, 3.89e-07, 4.83e-09, 4.41e-02, 4.41e-02]    []  
31000     [2.28e-04, 2.22e-04, 7.00e-06, 4.10e-01, 4.10e-01]    [2.28e-04, 2.22e-04, 7.00e-06, 4.10e-01, 4.10e-01]    []  
26000     [1.14e-03, 1.08e-03, 1.92e-04, 1.81e+00, 1.81e+00]    [1.14e-03, 1.08e-03, 1.92e-04, 1.81e+00, 1.81e+00]    []  
                                                                                                                           31000     [1.45e-04, 3.51e-05, 5.88e-07, 5.63e-01, 5.63e-01]    [1.45e-04, 3.51e-05, 5.88e-07, 5.63e-01, 5.63e-01]    []  
32000     [1.89e-04, 1.89e-04, 7.11e-06, 3.83e-01, 3.83e-01]    [1.89e-04, 1.89e-04, 7.11e-06, 3.83e-01, 3.83e-01]    []  
28000     [2.76e-05, 4.87e-07, 6.83e-09, 4.97e-01, 4.97e-01]    [2.76e-05, 4.87e-07, 6.83e-09, 4.97e-01, 4.97e-01]    []  
27000     [9.40e-04, 1.24e-03, 1.79e-04, 1.61e+00, 1.61e+00]    [9.40e-04, 1.24e-03, 1.79e-04, 1.61e+00, 1.61e+00]    []  
32000     [1.35e-04, 3.36e-05, 2.68e-07, 3.50e-01, 3.50e-01]    [1.35e-04, 3.36e-05, 2.68e-07, 3.50e-01, 3.50e-01]    []  
5000      [1.45e-02, 1.05e-03, 4.11e-24, 2.09e+03, 2.09e+03]    [1.45e-02, 1.05e-03, 4.11e-24, 2.09e+03, 2.09e+03]    []  
5000      [1.02e-04, 1.05e-03, 3.75e-24, 2.09e+03, 2.09e+03]    [1.02e-04, 1.05e-03, 3.75e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            4000      [6.67e-04, 1.00e-03, 1.49e-05, 8.15e+00, 8.15e+00]    [6.67e-04, 1.00e-03, 1.49e-05, 8.15e+00, 8.15e+00]    []  
6000      [3.80e-07, 1.05e-03, 5.71e-23, 2.09e+03, 2.09e+03]    [3.80e-07, 1.05e-03, 5.71e-23, 2.09e+03, 2.09e+03]    []  
6000      [8.77e-02, 1.03e-03, 3.73e-24, 2.09e+03, 2.09e+03]    [8.77e-02, 1.03e-03, 3.73e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       7000      [6.17e-11, 1.05e-03, 1.02e-24, 2.09e+03, 2.09e+03]    [6.17e-11, 1.05e-03, 1.02e-24, 2.09e+03, 2.09e+03]    []  
7000      [1.32e+00, 9.01e-04, 3.61e-24, 2.09e+03, 2.09e+03]    [1.32e+00, 9.01e-04, 3.61e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                      5000      [6.30e-04, 5.48e-04, 4.21e-06, 6.62e+00, 6.62e+00]    [6.30e-04, 5.48e-04, 4.21e-06, 6.62e+00, 6.62e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            8000      [6.48e-01, 9.78e-04, 3.55e-24, 2.09e+03, 2.09e+03]    [6.48e-01, 9.78e-04, 3.55e-24, 2.09e+03, 2.09e+03]    []  
8000      [5.41e-01, 9.91e-04, 1.65e-24, 2.09e+03, 2.09e+03]    [5.41e-01, 9.91e-04, 1.65e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       6000      [4.63e-04, 3.28e-04, 5.75e-06, 6.83e+00, 6.83e+00]    [4.63e-04, 3.28e-04, 5.75e-06, 6.83e+00, 6.83e+00]    []  
                                                                                                                           9000      [6.28e-04, 1.05e-03, 3.54e-24, 2.09e+03, 2.09e+03]    [6.28e-04, 1.05e-03, 3.54e-24, 2.09e+03, 2.09e+03]    []  
9000      [4.32e-10, 1.05e-03, 7.46e-24, 2.09e+03, 2.09e+03]    [4.32e-10, 1.05e-03, 7.46e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             10000     [2.81e-02, 1.04e-03, 3.80e-24, 2.09e+03, 2.09e+03]    [2.81e-02, 1.04e-03, 3.80e-24, 2.09e+03, 2.09e+03]    []  
10000     [2.16e-02, 1.05e-03, 1.54e-24, 2.09e+03, 2.09e+03]    [2.16e-02, 1.05e-03, 1.54e-24, 2.09e+03, 2.09e+03]    []  
7000      [3.32e-04, 2.00e-04, 1.59e-06, 4.14e+00, 4.14e+00]    [3.32e-04, 2.00e-04, 1.59e-06, 4.14e+00, 4.14e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       11000     [3.54e-04, 1.05e-03, 3.53e-24, 2.09e+03, 2.09e+03]    [3.54e-04, 1.05e-03, 3.53e-24, 2.09e+03, 2.09e+03]    []  
11000     [4.71e-06, 1.05e-03, 5.91e-30, 2.09e+03, 2.09e+03]    [4.71e-06, 1.05e-03, 5.91e-30, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            8000      [2.72e-04, 9.44e-05, 3.59e-06, 3.46e+00, 3.46e+00]    [2.72e-04, 9.44e-05, 3.59e-06, 3.46e+00, 3.46e+00]    []  
                                                                                                                           12000     [2.02e-03, 1.05e-03, 5.46e-24, 2.09e+03, 2.09e+03]    [2.02e-03, 1.05e-03, 5.46e-24, 2.09e+03, 2.09e+03]    []  
12000     [1.47e-04, 1.05e-03, 1.03e-29, 2.09e+03, 2.09e+03]    [1.47e-04, 1.05e-03, 1.03e-29, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       13000     [1.63e-04, 1.05e-03, 1.23e-23, 2.09e+03, 2.09e+03]    [1.63e-04, 1.05e-03, 1.23e-23, 2.09e+03, 2.09e+03]    []  
48000     [8.69e-05, 8.77e-05, 3.45e-06, 2.07e-01, 2.07e-01]    [8.69e-05, 8.77e-05, 3.45e-06, 2.07e-01, 2.07e-01]    []  
40000     [2.90e-04, 2.08e-04, 9.23e-05, 4.05e-01, 4.05e-01]    [2.90e-04, 2.08e-04, 9.23e-05, 4.05e-01, 4.05e-01]    []  
42000     [1.14e-04, 1.72e-05, 1.25e-08, 4.26e-02, 4.26e-02]    [1.14e-04, 1.72e-05, 1.25e-08, 4.26e-02, 4.26e-02]    []  
47000     [9.25e-05, 3.37e-05, 3.66e-07, 1.72e-01, 1.72e-01]    [9.25e-05, 3.37e-05, 3.66e-07, 1.72e-01, 1.72e-01]    []  
                                                                                                                           49000     [1.01e-04, 8.68e-05, 3.95e-06, 2.37e-01, 2.37e-01]    [1.01e-04, 8.68e-05, 3.95e-06, 2.37e-01, 2.37e-01]    []  
41000     [2.90e-04, 1.90e-04, 9.92e-05, 4.57e-01, 4.57e-01]    [2.90e-04, 1.90e-04, 9.92e-05, 4.57e-01, 4.57e-01]    []  
43000     [6.10e-05, 7.50e-06, 4.18e-08, 4.04e-02, 4.04e-02]    [6.10e-05, 7.50e-06, 4.18e-08, 4.04e-02, 4.04e-02]    []  
48000     [9.06e-05, 3.54e-05, 3.20e-07, 1.63e-01, 1.63e-01]    [9.06e-05, 3.54e-05, 3.20e-07, 1.63e-01, 1.63e-01]    []  
                                                                                                                           50000     [9.01e-05, 8.39e-05, 3.74e-06, 2.07e-01, 2.07e-01]    [9.01e-05, 8.39e-05, 3.74e-06, 2.07e-01, 2.07e-01]    []  

Best model at step 47000:
  train loss: 4.04e-01
  test loss: 4.04e-01
  test metric: []

'train' took 8262.175883 s

[W 2023-10-08 16:32:51,387] Trial 5 failed with parameters: {'num_domain': 9081, 'num_boundary': 9551, 'lr': 0.00016031268529168992} because of the following error: ValueError('operands could not be broadcast together with shapes (423763,) (847526,) ').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 71, in objective
    RMSE = np.sqrt(np.mean((v_test - v_pred_test)**2))
ValueError: operands could not be broadcast together with shapes (423763,) (847526,) 
[W 2023-10-08 16:32:51,391] Trial 5 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 71, in objective
    RMSE = np.sqrt(np.mean((v_test - v_pred_test)**2))
ValueError: operands could not be broadcast together with shapes (423763,) (847526,) 
                                                                                                                           42000     [2.34e-04, 1.77e-04, 8.36e-05, 3.50e-01, 3.50e-01]    [2.34e-04, 1.77e-04, 8.36e-05, 3.50e-01, 3.50e-01]    []  
44000     [3.99e-05, 4.18e-06, 5.38e-09, 3.94e-02, 3.94e-02]    [3.99e-05, 4.18e-06, 5.38e-09, 3.94e-02, 3.94e-02]    []  
49000     [9.11e-05, 3.69e-05, 3.14e-07, 2.19e-01, 2.19e-01]    [9.11e-05, 3.69e-05, 3.14e-07, 2.19e-01, 2.19e-01]    []  
                                                                                                                                                                                                                                                      43000     [2.42e-04, 1.68e-04, 8.42e-05, 3.70e-01, 3.70e-01]    [2.42e-04, 1.68e-04, 8.42e-05, 3.70e-01, 3.70e-01]    []  
45000     [2.55e-05, 1.48e-06, 5.13e-08, 4.73e-02, 4.73e-02]    [2.55e-05, 1.48e-06, 5.13e-08, 4.73e-02, 4.73e-02]    []  
50000     [9.16e-05, 3.96e-05, 3.79e-07, 1.79e-01, 1.79e-01]    [9.16e-05, 3.96e-05, 3.79e-07, 1.79e-01, 1.79e-01]    []  

Best model at step 48000:
  train loss: 3.26e-01
  test loss: 3.26e-01
  test metric: []

'train' took 8645.890795 s

[W 2023-10-08 16:37:26,557] Trial 2 failed with parameters: {'num_domain': 27385, 'num_boundary': 3765, 'lr': 0.0002225195523262972} because of the following error: ValueError('operands could not be broadcast together with shapes (423763,) (847526,) ').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 71, in objective
    RMSE = np.sqrt(np.mean((v_test - v_pred_test)**2))
ValueError: operands could not be broadcast together with shapes (423763,) (847526,) 
[W 2023-10-08 16:37:26,562] Trial 2 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 71, in objective
    RMSE = np.sqrt(np.mean((v_test - v_pred_test)**2))
ValueError: operands could not be broadcast together with shapes (423763,) (847526,) 
                                                                                                                           46000     [2.80e-05, 2.43e-06, 1.14e-07, 5.83e-02, 5.83e-02]    [2.80e-05, 2.43e-06, 1.14e-07, 5.83e-02, 5.83e-02]    []  
44000     [2.04e-04, 1.53e-04, 7.81e-05, 3.15e-01, 3.15e-01]    [2.04e-04, 1.53e-04, 7.81e-05, 3.15e-01, 3.15e-01]    []  
                                                                                                                           47000     [3.02e-01, 9.31e-03, 5.35e-07, 1.03e+02, 1.03e+02]    [3.02e-01, 9.31e-03, 5.35e-07, 1.03e+02, 1.03e+02]    []  
45000     [1.76e-04, 1.44e-04, 8.09e-05, 2.96e-01, 2.96e-01]    [1.76e-04, 1.44e-04, 8.09e-05, 2.96e-01, 2.96e-01]    []  
                                                                                                                           48000     [5.17e-04, 5.95e-04, 1.04e-06, 2.36e+00, 2.36e+00]    [5.17e-04, 5.95e-04, 1.04e-06, 2.36e+00, 2.36e+00]    []  
46000     [1.60e-04, 1.40e-04, 8.03e-05, 2.82e-01, 2.82e-01]    [1.60e-04, 1.40e-04, 8.03e-05, 2.82e-01, 2.82e-01]    []  
                                                                                                                           49000     [3.79e-04, 4.49e-04, 1.23e-06, 1.14e-01, 1.14e-01]    [3.79e-04, 4.49e-04, 1.23e-06, 1.14e-01, 1.14e-01]    []  
47000     [1.48e-04, 1.36e-04, 7.90e-05, 2.69e-01, 2.69e-01]    [1.48e-04, 1.36e-04, 7.90e-05, 2.69e-01, 2.69e-01]    []  
                                                                                                                                                                                                                                                      50000     [9.76e-05, 1.70e-05, 2.15e-07, 9.27e-02, 9.27e-02]    [9.76e-05, 1.70e-05, 2.15e-07, 9.27e-02, 9.27e-02]    []  

Best model at step 29000:
  train loss: 6.86e-02
  test loss: 6.86e-02
  test metric: []

'train' took 9491.563651 s

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1965, in _exec_single_context
    self.dialect.do_execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 921, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 73, in _create_scoped_session
    session.commit()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1923, in commit
    trans.commit(_to_root=True)
  File "<string>", line 2, in commit
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1239, in commit
    self._prepare_impl()
  File "<string>", line 2, in _prepare_impl
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1214, in _prepare_impl
    self.session.flush()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4179, in flush
    self._flush(objects)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4314, in _flush
    with util.safe_reraise():
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4275, in _flush
    flush_context.execute()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 85, in save_obj
    _emit_update_statements(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 909, in _emit_update_statements
    c = connection.execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1412, in execute
    return meth(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 516, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1635, in _execute_clauseelement
    ret = self._execute_context(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1844, in _execute_context
    return self._exec_single_context(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1984, in _exec_single_context
    self._handle_dbapi_exception(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2339, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1965, in _exec_single_context
    self.dialect.do_execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 921, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE trials SET state=?, datetime_complete=? WHERE trials.trial_id = ?]
[parameters: ('FAIL', '2023-10-08 16:53:12.095482', 5)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 212, in _run_trial
    frozen_trial = _tell_with_warning(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_tell.py", line 176, in _tell_with_warning
    study._storage.set_trial_state_values(frozen_trial._trial_id, state, values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_cached_storage.py", line 187, in set_trial_state_values
    return self._backend.set_trial_state_values(trial_id, state=state, values=values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 640, in set_trial_state_values
    with _create_scoped_session(self.scoped_session) as session:
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 90, in _create_scoped_session
    raise optuna.exceptions.StorageInternalError(message) from e
optuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 244, in _run_trial
    assert False, "Should not reach."
AssertionError: Should not reach.
48000     [1.37e-04, 1.32e-04, 7.05e-05, 2.57e-01, 2.57e-01]    [1.37e-04, 1.32e-04, 7.05e-05, 2.57e-01, 2.57e-01]    []  
                                                                                                                           49000     [1.26e-04, 1.26e-04, 6.12e-05, 2.47e-01, 2.47e-01]    [1.26e-04, 1.26e-04, 6.12e-05, 2.47e-01, 2.47e-01]    []  
                                                                                                                                                                                                                                                      50000     [1.17e-04, 1.21e-04, 5.55e-05, 2.36e-01, 2.36e-01]    [1.17e-04, 1.21e-04, 5.55e-05, 2.36e-01, 2.36e-01]    []  

Best model at step 50000:
  train loss: 4.72e-01
  test loss: 4.72e-01
  test metric: []

'train' took 9958.460586 s

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1965, in _exec_single_context
    self.dialect.do_execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 921, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 73, in _create_scoped_session
    session.commit()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1923, in commit
    trans.commit(_to_root=True)
  File "<string>", line 2, in commit
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1239, in commit
    self._prepare_impl()
  File "<string>", line 2, in _prepare_impl
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1214, in _prepare_impl
    self.session.flush()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4179, in flush
    self._flush(objects)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4314, in _flush
    with util.safe_reraise():
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4275, in _flush
    flush_context.execute()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 85, in save_obj
    _emit_update_statements(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 909, in _emit_update_statements
    c = connection.execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1412, in execute
    return meth(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 516, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1635, in _execute_clauseelement
    ret = self._execute_context(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1844, in _execute_context
    return self._exec_single_context(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1984, in _exec_single_context
    self._handle_dbapi_exception(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2339, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1965, in _exec_single_context
    self.dialect.do_execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 921, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE trials SET state=?, datetime_complete=? WHERE trials.trial_id = ?]
[parameters: ('FAIL', '2023-10-08 17:00:36.068653', 4)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 212, in _run_trial
    frozen_trial = _tell_with_warning(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_tell.py", line 176, in _tell_with_warning
    study._storage.set_trial_state_values(frozen_trial._trial_id, state, values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_cached_storage.py", line 187, in set_trial_state_values
    return self._backend.set_trial_state_values(trial_id, state=state, values=values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 640, in set_trial_state_values
    with _create_scoped_session(self.scoped_session) as session:
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 90, in _create_scoped_session
    raise optuna.exceptions.StorageInternalError(message) from e
optuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 244, in _run_trial
    assert False, "Should not reach."
AssertionError: Should not reach.
19000     [2.58e-03, 1.05e-03, 7.79e-31, 2.09e+03, 2.09e+03]    [2.58e-03, 1.05e-03, 7.79e-31, 2.09e+03, 2.09e+03]    []  
20000     [3.65e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.65e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
20000     [6.05e-04, 1.05e-03, 1.02e-31, 2.09e+03, 2.09e+03]    [6.05e-04, 1.05e-03, 1.02e-31, 2.09e+03, 2.09e+03]    []  
14000     [6.86e-04, 5.55e-04, 5.40e-06, 1.11e+00, 1.11e+00]    [6.86e-04, 5.55e-04, 5.40e-06, 1.11e+00, 1.11e+00]    []  
21000     [2.08e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.08e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
21000     [9.13e-02, 1.04e-03, 9.89e-32, 2.09e+03, 2.09e+03]    [9.13e-02, 1.04e-03, 9.89e-32, 2.09e+03, 2.09e+03]    []  
22000     [2.02e-02, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.02e-02, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
15000     [5.77e-04, 4.64e-04, 6.94e-06, 7.15e-01, 7.15e-01]    [5.77e-04, 4.64e-04, 6.94e-06, 7.15e-01, 7.15e-01]    []  
22000     [3.98e-02, 1.04e-03, 5.52e-32, 2.09e+03, 2.09e+03]    [3.98e-02, 1.04e-03, 5.52e-32, 2.09e+03, 2.09e+03]    []  
23000     [8.16e-02, 1.03e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [8.16e-02, 1.03e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
23000     [1.43e-03, 1.05e-03, 3.80e-32, 2.09e+03, 2.09e+03]    [1.43e-03, 1.05e-03, 3.80e-32, 2.09e+03, 2.09e+03]    []  
16000     [3.80e-04, 3.74e-04, 2.20e-05, 5.53e-01, 5.53e-01]    [3.80e-04, 3.74e-04, 2.20e-05, 5.53e-01, 5.53e-01]    []  
24000     [1.26e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.26e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
24000     [7.82e-05, 1.05e-03, 7.07e-33, 2.09e+03, 2.09e+03]    [7.82e-05, 1.05e-03, 7.07e-33, 2.09e+03, 2.09e+03]    []  
25000     [8.94e-01, 9.50e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [8.94e-01, 9.50e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  
17000     [2.76e-04, 3.27e-04, 9.84e-06, 8.52e-01, 8.52e-01]    [2.76e-04, 3.27e-04, 9.84e-06, 8.52e-01, 8.52e-01]    []  
25000     [1.13e-01, 1.03e-03, 3.57e-33, 2.09e+03, 2.09e+03]    [1.13e-01, 1.03e-03, 3.57e-33, 2.09e+03, 2.09e+03]    []  
26000     [9.29e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.29e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:40:03,624] A new study created in RDB with name: heartpinn
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:40:12,500] Using an existing study with name 'heartpinn' instead of creating a new one.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000145 s

[I 2023-10-08 17:40:16,703] Using an existing study with name 'heartpinn' instead of creating a new one.
Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [9.16e+01, 4.54e-04, 3.89e-07, 3.62e+03, 3.62e+03]    [9.16e+01, 4.54e-04, 3.89e-07, 3.62e+03, 3.62e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:40:21,132] Using an existing study with name 'heartpinn' instead of creating a new one.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000146 s

Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000230 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.79e+01, 3.83e-05, 3.71e-07, 3.60e+03, 3.60e+03]    [3.79e+01, 3.83e-05, 3.71e-07, 3.60e+03, 3.60e+03]    []  
Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [8.35e+01, 4.63e-04, 2.68e-07, 3.61e+03, 3.61e+03]    [8.35e+01, 4.63e-04, 2.68e-07, 3.61e+03, 3.61e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:40:29,679] Using an existing study with name 'heartpinn' instead of creating a new one.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000146 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [2.12e+01, 1.59e-04, 1.81e-07, 3.60e+03, 3.60e+03]    [2.12e+01, 1.59e-04, 1.81e-07, 3.60e+03, 3.60e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:40:39,076] Using an existing study with name 'heartpinn' instead of creating a new one.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000583 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [5.08e+00, 2.39e-04, 1.90e-07, 3.60e+03, 3.60e+03]    [5.08e+00, 2.39e-04, 1.90e-07, 3.60e+03, 3.60e+03]    []  
[I 2023-10-08 17:40:45,561] Using an existing study with name 'heartpinn' instead of creating a new one.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000124 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.89e+01, 9.80e-05, 4.34e-07, 3.59e+03, 3.59e+03]    [1.89e+01, 9.80e-05, 4.34e-07, 3.59e+03, 3.59e+03]    []  
[I 2023-10-08 17:40:49,444] Using an existing study with name 'heartpinn' instead of creating a new one.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000337 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.28e+01, 5.58e-05, 8.19e-08, 3.60e+03, 3.60e+03]    [3.28e+01, 5.58e-05, 8.19e-08, 3.60e+03, 3.60e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000185 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [6.53e+01, 1.77e-04, 9.70e-08, 3.62e+03, 3.62e+03]    [6.53e+01, 1.77e-04, 9.70e-08, 3.62e+03, 3.62e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:41:34,996] Using an existing study with name 'heartpinn' instead of creating a new one.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:41:43,787] Using an existing study with name 'heartpinn' instead of creating a new one.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000487 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.67e+01, 5.48e-05, 3.96e-08, 3.61e+03, 3.61e+03]    [4.67e+01, 5.48e-05, 3.96e-08, 3.61e+03, 3.61e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000363 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.03e+01, 5.01e-05, 3.43e-07, 3.61e+03, 3.61e+03]    [4.03e+01, 5.01e-05, 3.43e-07, 3.61e+03, 3.61e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1000      [1.01e-06, 1.05e-03, 2.39e-13, 2.09e+03, 2.09e+03]    [1.01e-06, 1.05e-03, 2.39e-13, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1000      [4.33e-09, 1.05e-03, 4.11e-15, 2.09e+03, 2.09e+03]    [4.33e-09, 1.05e-03, 4.11e-15, 2.09e+03, 2.09e+03]    []  
2000      [2.73e-03, 7.26e-04, 8.70e-05, 9.94e+00, 9.94e+00]    [2.73e-03, 7.26e-04, 8.70e-05, 9.94e+00, 9.94e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   3000      [6.08e-03, 1.29e-04, 2.43e-04, 1.52e+01, 1.52e+01]    [6.08e-03, 1.29e-04, 2.43e-04, 1.52e+01, 1.52e+01]    []  
3000      [9.35e-07, 8.43e-04, 9.24e-13, 2.54e+03, 2.54e+03]    [9.35e-07, 8.43e-04, 9.24e-13, 2.54e+03, 2.54e+03]    []  
3000      [8.39e-01, 6.95e-04, 8.61e-09, 2.82e+03, 2.82e+03]    [8.39e-01, 6.95e-04, 8.61e-09, 2.82e+03, 2.82e+03]    []  
4000      [7.71e-11, 1.05e-03, 6.42e-24, 2.09e+03, 2.09e+03]    [7.71e-11, 1.05e-03, 6.42e-24, 2.09e+03, 2.09e+03]    []  
5000      [6.00e-10, 1.05e-03, 1.41e-23, 2.09e+03, 2.09e+03]    [6.00e-10, 1.05e-03, 1.41e-23, 2.09e+03, 2.09e+03]    []  
5000      [4.51e-08, 9.17e-04, 4.50e-15, 2.42e+03, 2.42e+03]    [4.51e-08, 9.17e-04, 4.50e-15, 2.42e+03, 2.42e+03]    []  
4000      [2.09e-03, 6.56e-04, 2.26e-05, 8.12e+00, 8.12e+00]    [2.09e-03, 6.56e-04, 2.26e-05, 8.12e+00, 8.12e+00]    []  
5000      [1.19e-04, 1.05e-03, 2.21e-25, 2.09e+03, 2.09e+03]    [1.19e-04, 1.05e-03, 2.21e-25, 2.09e+03, 2.09e+03]    []  
6000      [1.27e-08, 1.05e-03, 1.27e-23, 2.09e+03, 2.09e+03]    [1.27e-08, 1.05e-03, 1.27e-23, 2.09e+03, 2.09e+03]    []  
5000      [7.71e-11, 1.05e-03, 6.50e-24, 2.09e+03, 2.09e+03]    [7.71e-11, 1.05e-03, 6.50e-24, 2.09e+03, 2.09e+03]    []  
6000      [4.10e-09, 9.54e-04, 9.14e-16, 2.37e+03, 2.37e+03]    [4.10e-09, 9.54e-04, 9.14e-16, 2.37e+03, 2.37e+03]    []  
4000      [5.29e-04, 8.92e-04, 6.36e-06, 9.17e+00, 9.17e+00]    [5.29e-04, 8.92e-04, 6.36e-06, 9.17e+00, 9.17e+00]    []  
4000      [6.64e-08, 8.77e-04, 1.17e-13, 2.48e+03, 2.48e+03]    [6.64e-08, 8.77e-04, 1.17e-13, 2.48e+03, 2.48e+03]    []  
4000      [3.81e-01, 7.42e-04, 4.82e-10, 2.73e+03, 2.73e+03]    [3.81e-01, 7.42e-04, 4.82e-10, 2.73e+03, 2.73e+03]    []  
6000      [3.89e-07, 1.05e-03, 6.55e-26, 2.09e+03, 2.09e+03]    [3.89e-07, 1.05e-03, 6.55e-26, 2.09e+03, 2.09e+03]    []  
7000      [1.18e-06, 1.05e-03, 1.08e-23, 2.09e+03, 2.09e+03]    [1.18e-06, 1.05e-03, 1.08e-23, 2.09e+03, 2.09e+03]    []  
5000      [2.29e-03, 4.78e-04, 1.26e-05, 5.76e+00, 5.76e+00]    [2.29e-03, 4.78e-04, 1.26e-05, 5.76e+00, 5.76e+00]    []  
6000      [6.17e-11, 1.05e-03, 6.65e-24, 2.09e+03, 2.09e+03]    [6.17e-11, 1.05e-03, 6.65e-24, 2.09e+03, 2.09e+03]    []  
7000      [2.56e-08, 9.86e-04, 5.07e-16, 2.32e+03, 2.32e+03]    [2.56e-08, 9.86e-04, 5.07e-16, 2.32e+03, 2.32e+03]    []  
                                                                                                                           7000      [3.55e-05, 1.05e-03, 1.21e-28, 2.09e+03, 2.09e+03]    [3.55e-05, 1.05e-03, 1.21e-28, 2.09e+03, 2.09e+03]    []  
5000      [3.81e-04, 4.47e-04, 3.41e-06, 7.33e+00, 7.33e+00]    [3.81e-04, 4.47e-04, 3.41e-06, 7.33e+00, 7.33e+00]    []  
5000      [4.59e-09, 9.07e-04, 2.75e-14, 2.44e+03, 2.44e+03]    [4.59e-09, 9.07e-04, 2.75e-14, 2.44e+03, 2.44e+03]    []  
8000      [1.20e-05, 1.05e-03, 9.06e-24, 2.09e+03, 2.09e+03]    [1.20e-05, 1.05e-03, 9.06e-24, 2.09e+03, 2.09e+03]    []  
5000      [2.29e-01, 7.69e-04, 2.17e-11, 2.68e+03, 2.68e+03]    [2.29e-01, 7.69e-04, 2.17e-11, 2.68e+03, 2.68e+03]    []  
8000      [3.42e-08, 1.01e-03, 7.56e-17, 2.27e+03, 2.27e+03]    [3.42e-08, 1.01e-03, 7.56e-17, 2.27e+03, 2.27e+03]    []  
6000      [1.33e-03, 3.13e-04, 1.38e-05, 3.48e+00, 3.48e+00]    [1.33e-03, 3.13e-04, 1.38e-05, 3.48e+00, 3.48e+00]    []  
7000      [6.17e-11, 1.05e-03, 6.91e-24, 2.09e+03, 2.09e+03]    [6.17e-11, 1.05e-03, 6.91e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                           9000      [6.92e-07, 1.05e-03, 8.73e-24, 2.09e+03, 2.09e+03]    [6.92e-07, 1.05e-03, 8.73e-24, 2.09e+03, 2.09e+03]    []  
8000      [5.27e-02, 1.04e-03, 4.04e-29, 2.09e+03, 2.09e+03]    [5.27e-02, 1.04e-03, 4.04e-29, 2.09e+03, 2.09e+03]    []  
9000      [2.96e-08, 1.04e-03, 2.08e-17, 2.23e+03, 2.23e+03]    [2.96e-08, 1.04e-03, 2.08e-17, 2.23e+03, 2.23e+03]    []  
6000      [5.74e-04, 2.27e-04, 5.18e-06, 5.79e+00, 5.79e+00]    [5.74e-04, 2.27e-04, 5.18e-06, 5.79e+00, 5.79e+00]    []  
6000      [6.23e-10, 9.35e-04, 8.09e-15, 2.39e+03, 2.39e+03]    [6.23e-10, 9.35e-04, 8.09e-15, 2.39e+03, 2.39e+03]    []  
6000      [3.95e-02, 7.86e-04, 8.66e-12, 2.64e+03, 2.64e+03]    [3.95e-02, 7.86e-04, 8.66e-12, 2.64e+03, 2.64e+03]    []  
8000      [1.36e-08, 1.05e-03, 1.05e-23, 2.09e+03, 2.09e+03]    [1.36e-08, 1.05e-03, 1.05e-23, 2.09e+03, 2.09e+03]    []  
7000      [1.56e-03, 2.60e-04, 1.68e-04, 3.49e+00, 3.49e+00]    [1.56e-03, 2.60e-04, 1.68e-04, 3.49e+00, 3.49e+00]    []  
10000     [4.72e-06, 1.05e-03, 1.21e-23, 2.09e+03, 2.09e+03]    [4.72e-06, 1.05e-03, 1.21e-23, 2.09e+03, 2.09e+03]    []  
9000      [4.47e-02, 1.04e-03, 1.64e-29, 2.09e+03, 2.09e+03]    [4.47e-02, 1.04e-03, 1.64e-29, 2.09e+03, 2.09e+03]    []  
10000     [2.17e-08, 1.06e-03, 6.30e-18, 2.20e+03, 2.20e+03]    [2.17e-08, 1.06e-03, 6.30e-18, 2.20e+03, 2.20e+03]    []  
11000     [5.10e-04, 1.05e-03, 2.20e-23, 2.09e+03, 2.09e+03]    [5.10e-04, 1.05e-03, 2.20e-23, 2.09e+03, 2.09e+03]    []  
9000      [1.93e-09, 1.05e-03, 1.81e-22, 2.09e+03, 2.09e+03]    [1.93e-09, 1.05e-03, 1.81e-22, 2.09e+03, 2.09e+03]    []  
7000      [1.02e-03, 8.46e-05, 2.02e-05, 4.04e+00, 4.04e+00]    [1.02e-03, 8.46e-05, 2.02e-05, 4.04e+00, 4.04e+00]    []  
7000      [1.41e-09, 9.60e-04, 2.11e-15, 2.36e+03, 2.36e+03]    [1.41e-09, 9.60e-04, 2.11e-15, 2.36e+03, 2.36e+03]    []  
10000     [1.60e-03, 1.05e-03, 5.35e-30, 2.09e+03, 2.09e+03]    [1.60e-03, 1.05e-03, 5.35e-30, 2.09e+03, 2.09e+03]    []  
7000      [1.50e-03, 7.97e-04, 1.73e-12, 2.61e+03, 2.61e+03]    [1.50e-03, 7.97e-04, 1.73e-12, 2.61e+03, 2.61e+03]    []  
8000      [1.82e-03, 2.87e-04, 6.62e-04, 2.37e+00, 2.37e+00]    [1.82e-03, 2.87e-04, 6.62e-04, 2.37e+00, 2.37e+00]    []  
11000     [1.37e-08, 1.07e-03, 4.48e-18, 2.17e+03, 2.17e+03]    [1.37e-08, 1.07e-03, 4.48e-18, 2.17e+03, 2.17e+03]    []  
                                                                                                                           12000     [4.91e-07, 1.05e-03, 4.72e-22, 2.09e+03, 2.09e+03]    [4.91e-07, 1.05e-03, 4.72e-22, 2.09e+03, 2.09e+03]    []  
10000     [9.74e-03, 1.04e-03, 1.10e-27, 2.09e+03, 2.09e+03]    [9.74e-03, 1.04e-03, 1.10e-27, 2.09e+03, 2.09e+03]    []  
11000     [3.48e-02, 1.04e-03, 1.64e-30, 2.09e+03, 2.09e+03]    [3.48e-02, 1.04e-03, 1.64e-30, 2.09e+03, 2.09e+03]    []  
12000     [8.84e-09, 1.08e-03, 1.40e-18, 2.15e+03, 2.15e+03]    [8.84e-09, 1.08e-03, 1.40e-18, 2.15e+03, 2.15e+03]    []  
8000      [7.73e-04, 1.46e-04, 1.76e-06, 2.80e+00, 2.80e+00]    [7.73e-04, 1.46e-04, 1.76e-06, 2.80e+00, 2.80e+00]    []  
9000      [2.25e-03, 4.11e-04, 1.10e-04, 2.77e+00, 2.77e+00]    [2.25e-03, 4.11e-04, 1.10e-04, 2.77e+00, 2.77e+00]    []  
8000      [1.83e-09, 9.84e-04, 2.52e-16, 2.32e+03, 2.32e+03]    [1.83e-09, 9.84e-04, 2.52e-16, 2.32e+03, 2.32e+03]    []  
8000      [5.48e-05, 8.10e-04, 1.19e-13, 2.59e+03, 2.59e+03]    [5.48e-05, 8.10e-04, 1.19e-13, 2.59e+03, 2.59e+03]    []  
13000     [1.03e-10, 1.05e-03, 8.40e-26, 2.09e+03, 2.09e+03]    [1.03e-10, 1.05e-03, 8.40e-26, 2.09e+03, 2.09e+03]    []  
                                                                                                                           12000     [5.03e-01, 9.65e-04, 5.77e-31, 2.09e+03, 2.09e+03]    [5.03e-01, 9.65e-04, 5.77e-31, 2.09e+03, 2.09e+03]    []  
11000     [1.29e-07, 1.05e-03, 3.38e-26, 2.09e+03, 2.09e+03]    [1.29e-07, 1.05e-03, 3.38e-26, 2.09e+03, 2.09e+03]    []  
13000     [5.39e-09, 1.09e-03, 5.97e-19, 2.13e+03, 2.13e+03]    [5.39e-09, 1.09e-03, 5.97e-19, 2.13e+03, 2.13e+03]    []  
14000     [2.56e-02, 1.05e-03, 9.73e-25, 2.09e+03, 2.09e+03]    [2.56e-02, 1.05e-03, 9.73e-25, 2.09e+03, 2.09e+03]    []  
9000      [8.16e-04, 2.70e-04, 3.37e-06, 2.08e+00, 2.08e+00]    [8.16e-04, 2.70e-04, 3.37e-06, 2.08e+00, 2.08e+00]    []  
10000     [1.21e-03, 5.70e-04, 1.52e-04, 1.10e+00, 1.10e+00]    [1.21e-03, 5.70e-04, 1.52e-04, 1.10e+00, 1.10e+00]    []  
9000      [2.06e-09, 1.01e-03, 1.53e-16, 2.29e+03, 2.29e+03]    [2.06e-09, 1.01e-03, 1.53e-16, 2.29e+03, 2.29e+03]    []  
9000      [3.55e-06, 8.23e-04, 5.39e-14, 2.57e+03, 2.57e+03]    [3.55e-06, 8.23e-04, 5.39e-14, 2.57e+03, 2.57e+03]    []  
14000     [2.93e-09, 1.09e-03, 2.15e-19, 2.12e+03, 2.12e+03]    [2.93e-09, 1.09e-03, 2.15e-19, 2.12e+03, 2.12e+03]    []  
13000     [3.65e-03, 1.05e-03, 2.59e-31, 2.09e+03, 2.09e+03]    [3.65e-03, 1.05e-03, 2.59e-31, 2.09e+03, 2.09e+03]    []  
12000     [1.76e-03, 1.05e-03, 1.78e-25, 2.09e+03, 2.09e+03]    [1.76e-03, 1.05e-03, 1.78e-25, 2.09e+03, 2.09e+03]    []  
15000     [2.09e-05, 1.05e-03, 2.38e-27, 2.09e+03, 2.09e+03]    [2.09e-05, 1.05e-03, 2.38e-27, 2.09e+03, 2.09e+03]    []  
15000     [1.63e-09, 1.08e-03, 3.89e-20, 2.11e+03, 2.11e+03]    [1.63e-09, 1.08e-03, 3.89e-20, 2.11e+03, 2.11e+03]    []  
11000     [8.40e-04, 6.11e-04, 1.09e-04, 8.91e-01, 8.91e-01]    [8.40e-04, 6.11e-04, 1.09e-04, 8.91e-01, 8.91e-01]    []  
14000     [6.19e-07, 1.05e-03, 1.26e-31, 2.09e+03, 2.09e+03]    [6.19e-07, 1.05e-03, 1.26e-31, 2.09e+03, 2.09e+03]    []  
10000     [1.12e-03, 7.74e-04, 3.26e-06, 1.66e+00, 1.66e+00]    [1.12e-03, 7.74e-04, 3.26e-06, 1.66e+00, 1.66e+00]    []  
16000     [1.20e-04, 1.05e-03, 1.18e-27, 2.09e+03, 2.09e+03]    [1.20e-04, 1.05e-03, 1.18e-27, 2.09e+03, 2.09e+03]    []  
13000     [2.81e-06, 1.05e-03, 3.26e-25, 2.09e+03, 2.09e+03]    [2.81e-06, 1.05e-03, 3.26e-25, 2.09e+03, 2.09e+03]    []  
10000     [1.90e-09, 1.02e-03, 3.80e-17, 2.26e+03, 2.26e+03]    [1.90e-09, 1.02e-03, 3.80e-17, 2.26e+03, 2.26e+03]    []  
10000     [4.46e-07, 8.35e-04, 1.69e-14, 2.55e+03, 2.55e+03]    [4.46e-07, 8.35e-04, 1.69e-14, 2.55e+03, 2.55e+03]    []  
16000     [7.96e-10, 1.07e-03, 2.13e-20, 2.10e+03, 2.10e+03]    [7.96e-10, 1.07e-03, 2.13e-20, 2.10e+03, 2.10e+03]    []  
15000     [7.67e-03, 1.04e-03, 2.65e-32, 2.09e+03, 2.09e+03]    [7.67e-03, 1.04e-03, 2.65e-32, 2.09e+03, 2.09e+03]    []  
17000     [5.77e-10, 1.05e-03, 1.21e-28, 2.09e+03, 2.09e+03]    [5.77e-10, 1.05e-03, 1.21e-28, 2.09e+03, 2.09e+03]    []  
12000     [1.12e-03, 5.48e-04, 2.29e-05, 6.83e-01, 6.83e-01]    [1.12e-03, 5.48e-04, 2.29e-05, 6.83e-01, 6.83e-01]    []  
14000     [3.18e-01, 9.93e-04, 3.68e-25, 2.09e+03, 2.09e+03]    [3.18e-01, 9.93e-04, 3.68e-25, 2.09e+03, 2.09e+03]    []  
11000     [9.59e-04, 1.13e-03, 2.05e-06, 1.22e+00, 1.22e+00]    [9.59e-04, 1.13e-03, 2.05e-06, 1.22e+00, 1.22e+00]    []  
11000     [1.80e-09, 1.04e-03, 6.89e-18, 2.23e+03, 2.23e+03]    [1.80e-09, 1.04e-03, 6.89e-18, 2.23e+03, 2.23e+03]    []  
17000     [3.50e-10, 1.07e-03, 7.51e-21, 2.10e+03, 2.10e+03]    [3.50e-10, 1.07e-03, 7.51e-21, 2.10e+03, 2.10e+03]    []  
11000     [7.35e-08, 8.47e-04, 1.83e-15, 2.53e+03, 2.53e+03]    [7.35e-08, 8.47e-04, 1.83e-15, 2.53e+03, 2.53e+03]    []  
18000     [1.29e-03, 1.05e-03, 9.22e-28, 2.09e+03, 2.09e+03]    [1.29e-03, 1.05e-03, 9.22e-28, 2.09e+03, 2.09e+03]    []  
16000     [4.98e-01, 9.66e-04, 1.71e-33, 2.09e+03, 2.09e+03]    [4.98e-01, 9.66e-04, 1.71e-33, 2.09e+03, 2.09e+03]    []  
15000     [1.63e-04, 1.05e-03, 5.07e-26, 2.09e+03, 2.09e+03]    [1.63e-04, 1.05e-03, 5.07e-26, 2.09e+03, 2.09e+03]    []  
13000     [1.25e-03, 4.76e-04, 1.30e-04, 6.39e-01, 6.39e-01]    [1.25e-03, 4.76e-04, 1.30e-04, 6.39e-01, 6.39e-01]    []  
18000     [7.10e-11, 1.06e-03, 8.88e-21, 2.09e+03, 2.09e+03]    [7.10e-11, 1.06e-03, 8.88e-21, 2.09e+03, 2.09e+03]    []  
19000     [1.21e-06, 1.05e-03, 2.27e-26, 2.09e+03, 2.09e+03]    [1.21e-06, 1.05e-03, 2.27e-26, 2.09e+03, 2.09e+03]    []  
12000     [8.11e-04, 1.09e-03, 4.45e-06, 1.06e+00, 1.06e+00]    [8.11e-04, 1.09e-03, 4.45e-06, 1.06e+00, 1.06e+00]    []  
17000     [7.07e-05, 1.05e-03, 4.03e-30, 2.09e+03, 2.09e+03]    [7.07e-05, 1.05e-03, 4.03e-30, 2.09e+03, 2.09e+03]    []  
12000     [2.01e-09, 1.06e-03, 2.68e-18, 2.21e+03, 2.21e+03]    [2.01e-09, 1.06e-03, 2.68e-18, 2.21e+03, 2.21e+03]    []  
12000     [1.67e-08, 8.58e-04, 5.19e-16, 2.51e+03, 2.51e+03]    [1.67e-08, 8.58e-04, 5.19e-16, 2.51e+03, 2.51e+03]    []  
16000     [3.68e-04, 1.05e-03, 2.51e-26, 2.09e+03, 2.09e+03]    [3.68e-04, 1.05e-03, 2.51e-26, 2.09e+03, 2.09e+03]    []  
19000     [3.61e-11, 1.05e-03, 3.00e-21, 2.09e+03, 2.09e+03]    [3.61e-11, 1.05e-03, 3.00e-21, 2.09e+03, 2.09e+03]    []  
20000     [6.03e-09, 1.05e-03, 8.04e-32, 2.09e+03, 2.09e+03]    [6.03e-09, 1.05e-03, 8.04e-32, 2.09e+03, 2.09e+03]    []  
14000     [1.15e-03, 4.25e-04, 1.33e-04, 7.35e-01, 7.35e-01]    [1.15e-03, 4.25e-04, 1.33e-04, 7.35e-01, 7.35e-01]    []  
18000     [2.05e-07, 1.05e-03, 3.20e-30, 2.09e+03, 2.09e+03]    [2.05e-07, 1.05e-03, 3.20e-30, 2.09e+03, 2.09e+03]    []  
13000     [6.22e-04, 4.51e-04, 5.32e-06, 8.02e-01, 8.02e-01]    [6.22e-04, 4.51e-04, 5.32e-06, 8.02e-01, 8.02e-01]    []  
20000     [2.79e-09, 1.05e-03, 1.41e-21, 2.09e+03, 2.09e+03]    [2.79e-09, 1.05e-03, 1.41e-21, 2.09e+03, 2.09e+03]    []  
21000     [7.02e-02, 1.04e-03, 1.24e-32, 2.09e+03, 2.09e+03]    [7.02e-02, 1.04e-03, 1.24e-32, 2.09e+03, 2.09e+03]    []  
13000     [1.27e-09, 1.07e-03, 4.46e-19, 2.18e+03, 2.18e+03]    [1.27e-09, 1.07e-03, 4.46e-19, 2.18e+03, 2.18e+03]    []  
17000     [8.99e-06, 1.05e-03, 1.22e-26, 2.09e+03, 2.09e+03]    [8.99e-06, 1.05e-03, 1.22e-26, 2.09e+03, 2.09e+03]    []  
13000     [3.41e-09, 8.69e-04, 2.16e-16, 2.49e+03, 2.49e+03]    [3.41e-09, 8.69e-04, 2.16e-16, 2.49e+03, 2.49e+03]    []  
19000     [3.59e-05, 1.05e-03, 1.08e-30, 2.09e+03, 2.09e+03]    [3.59e-05, 1.05e-03, 1.08e-30, 2.09e+03, 2.09e+03]    []  
15000     [8.68e-04, 3.54e-04, 1.54e-05, 5.00e-01, 5.00e-01]    [8.68e-04, 3.54e-04, 1.54e-05, 5.00e-01, 5.00e-01]    []  
22000     [6.46e-08, 1.05e-03, 2.15e-33, 2.09e+03, 2.09e+03]    [6.46e-08, 1.05e-03, 2.15e-33, 2.09e+03, 2.09e+03]    []  
21000     [1.44e-10, 1.05e-03, 7.54e-22, 2.09e+03, 2.09e+03]    [1.44e-10, 1.05e-03, 7.54e-22, 2.09e+03, 2.09e+03]    []  
18000     [3.74e-06, 1.05e-03, 9.41e-26, 2.09e+03, 2.09e+03]    [3.74e-06, 1.05e-03, 9.41e-26, 2.09e+03, 2.09e+03]    []  
14000     [3.29e-04, 1.41e-04, 6.97e-06, 1.03e+00, 1.03e+00]    [3.29e-04, 1.41e-04, 6.97e-06, 1.03e+00, 1.03e+00]    []  
20000     [5.12e-01, 9.64e-04, 3.02e-31, 2.09e+03, 2.09e+03]    [5.12e-01, 9.64e-04, 3.02e-31, 2.09e+03, 2.09e+03]    []  
14000     [1.31e-09, 1.08e-03, 3.41e-19, 2.16e+03, 2.16e+03]    [1.31e-09, 1.08e-03, 3.41e-19, 2.16e+03, 2.16e+03]    []  
14000     [2.92e-10, 8.80e-04, 4.96e-17, 2.48e+03, 2.48e+03]    [2.92e-10, 8.80e-04, 4.96e-17, 2.48e+03, 2.48e+03]    []  
23000     [8.72e-02, 1.04e-03, 1.93e-32, 2.09e+03, 2.09e+03]    [8.72e-02, 1.04e-03, 1.93e-32, 2.09e+03, 2.09e+03]    []  
22000     [3.08e-10, 1.05e-03, 3.40e-22, 2.09e+03, 2.09e+03]    [3.08e-10, 1.05e-03, 3.40e-22, 2.09e+03, 2.09e+03]    []  
16000     [1.09e-03, 3.08e-04, 1.78e-05, 8.29e-01, 8.29e-01]    [1.09e-03, 3.08e-04, 1.78e-05, 8.29e-01, 8.29e-01]    []  
19000     [3.76e-05, 1.05e-03, 5.18e-26, 2.09e+03, 2.09e+03]    [3.76e-05, 1.05e-03, 5.18e-26, 2.09e+03, 2.09e+03]    []  
21000     [4.24e-03, 1.05e-03, 9.85e-32, 2.09e+03, 2.09e+03]    [4.24e-03, 1.05e-03, 9.85e-32, 2.09e+03, 2.09e+03]    []  
15000     [2.63e-04, 1.09e-04, 7.63e-06, 6.39e-01, 6.39e-01]    [2.63e-04, 1.09e-04, 7.63e-06, 6.39e-01, 6.39e-01]    []  
24000     [4.64e+00, 4.62e-04, 4.59e-29, 2.09e+03, 2.09e+03]    [4.64e+00, 4.62e-04, 4.59e-29, 2.09e+03, 2.09e+03]    []  
23000     [1.14e-09, 1.05e-03, 7.08e-23, 2.09e+03, 2.09e+03]    [1.14e-09, 1.05e-03, 7.08e-23, 2.09e+03, 2.09e+03]    []  
15000     [2.70e-09, 1.08e-03, 1.10e-19, 2.15e+03, 2.15e+03]    [2.70e-09, 1.08e-03, 1.10e-19, 2.15e+03, 2.15e+03]    []  
17000     [9.10e-04, 2.82e-04, 6.17e-05, 3.12e-01, 3.12e-01]    [9.10e-04, 2.82e-04, 6.17e-05, 3.12e-01, 3.12e-01]    []  
15000     [1.91e-11, 8.91e-04, 1.19e-17, 2.46e+03, 2.46e+03]    [1.91e-11, 8.91e-04, 1.19e-17, 2.46e+03, 2.46e+03]    []  
20000     [2.06e-07, 1.05e-03, 1.84e-26, 2.09e+03, 2.09e+03]    [2.06e-07, 1.05e-03, 1.84e-26, 2.09e+03, 2.09e+03]    []  
22000     [9.26e-05, 1.05e-03, 3.93e-32, 2.09e+03, 2.09e+03]    [9.26e-05, 1.05e-03, 3.93e-32, 2.09e+03, 2.09e+03]    []  
25000     [5.29e-08, 1.05e-03, 1.25e-31, 2.09e+03, 2.09e+03]    [5.29e-08, 1.05e-03, 1.25e-31, 2.09e+03, 2.09e+03]    []  
24000     [8.47e-10, 1.05e-03, 6.59e-23, 2.09e+03, 2.09e+03]    [8.47e-10, 1.05e-03, 6.59e-23, 2.09e+03, 2.09e+03]    []  
16000     [2.65e-04, 9.08e-05, 1.63e-05, 1.88e+00, 1.88e+00]    [2.65e-04, 9.08e-05, 1.63e-05, 1.88e+00, 1.88e+00]    []  
16000     [1.41e-08, 1.09e-03, 2.66e-20, 2.13e+03, 2.13e+03]    [1.41e-08, 1.09e-03, 2.66e-20, 2.13e+03, 2.13e+03]    []  
23000     [1.59e-04, 1.05e-03, 1.27e-32, 2.09e+03, 2.09e+03]    [1.59e-04, 1.05e-03, 1.27e-32, 2.09e+03, 2.09e+03]    []  
18000     [1.16e-03, 2.63e-04, 1.03e-04, 4.91e-01, 4.91e-01]    [1.16e-03, 2.63e-04, 1.03e-04, 4.91e-01, 4.91e-01]    []  
26000     [2.47e-01, 1.03e-03, 7.78e-33, 2.09e+03, 2.09e+03]    [2.47e-01, 1.03e-03, 7.78e-33, 2.09e+03, 2.09e+03]    []  
21000     [1.34e-01, 1.04e-03, 5.75e-27, 2.09e+03, 2.09e+03]    [1.34e-01, 1.04e-03, 5.75e-27, 2.09e+03, 2.09e+03]    []  
16000     [2.67e-10, 9.01e-04, 2.19e-18, 2.45e+03, 2.45e+03]    [2.67e-10, 9.01e-04, 2.19e-18, 2.45e+03, 2.45e+03]    []  
25000     [9.59e-10, 1.05e-03, 1.27e-23, 2.09e+03, 2.09e+03]    [9.59e-10, 1.05e-03, 1.27e-23, 2.09e+03, 2.09e+03]    []  
27000     [1.56e-05, 1.05e-03, 6.19e-34, 2.09e+03, 2.09e+03]    [1.56e-05, 1.05e-03, 6.19e-34, 2.09e+03, 2.09e+03]    []  
24000     [3.38e+00, 6.81e-04, 7.14e-33, 2.09e+03, 2.09e+03]    [3.38e+00, 6.81e-04, 7.14e-33, 2.09e+03, 2.09e+03]    []  
17000     [2.22e-04, 8.12e-05, 7.13e-06, 6.98e-01, 6.98e-01]    [2.22e-04, 8.12e-05, 7.13e-06, 6.98e-01, 6.98e-01]    []  
26000     [1.73e-11, 1.05e-03, 8.94e-24, 2.09e+03, 2.09e+03]    [1.73e-11, 1.05e-03, 8.94e-24, 2.09e+03, 2.09e+03]    []  
22000     [2.02e-04, 1.05e-03, 1.76e-27, 2.09e+03, 2.09e+03]    [2.02e-04, 1.05e-03, 1.76e-27, 2.09e+03, 2.09e+03]    []  
19000     [5.80e-04, 2.53e-04, 2.71e-05, 2.66e-01, 2.66e-01]    [5.80e-04, 2.53e-04, 2.71e-05, 2.66e-01, 2.66e-01]    []  
17000     [1.17e-09, 1.09e-03, 1.74e-20, 2.12e+03, 2.12e+03]    [1.17e-09, 1.09e-03, 1.74e-20, 2.12e+03, 2.12e+03]    []  
17000     [6.24e-10, 9.11e-04, 1.05e-18, 2.43e+03, 2.43e+03]    [6.24e-10, 9.11e-04, 1.05e-18, 2.43e+03, 2.43e+03]    []  
28000     [1.10e-04, 1.05e-03, 2.06e-34, 2.09e+03, 2.09e+03]    [1.10e-04, 1.05e-03, 2.06e-34, 2.09e+03, 2.09e+03]    []  
25000     [2.06e-05, 1.05e-03, 1.19e-33, 2.09e+03, 2.09e+03]    [2.06e-05, 1.05e-03, 1.19e-33, 2.09e+03, 2.09e+03]    []  
27000     [1.43e-08, 1.05e-03, 5.24e-24, 2.09e+03, 2.09e+03]    [1.43e-08, 1.05e-03, 5.24e-24, 2.09e+03, 2.09e+03]    []  
23000     [1.93e-02, 1.04e-03, 5.55e-28, 2.09e+03, 2.09e+03]    [1.93e-02, 1.04e-03, 5.55e-28, 2.09e+03, 2.09e+03]    []  
18000     [1.85e-04, 6.98e-05, 7.87e-06, 7.31e-01, 7.31e-01]    [1.85e-04, 6.98e-05, 7.87e-06, 7.31e-01, 7.31e-01]    []  
20000     [7.01e-04, 2.44e-04, 3.41e-05, 3.52e-01, 3.52e-01]    [7.01e-04, 2.44e-04, 3.41e-05, 3.52e-01, 3.52e-01]    []  
29000     [6.00e-10, 1.05e-03, 6.02e-35, 2.09e+03, 2.09e+03]    [6.00e-10, 1.05e-03, 6.02e-35, 2.09e+03, 2.09e+03]    []  
18000     [3.95e-10, 1.09e-03, 5.81e-21, 2.11e+03, 2.11e+03]    [3.95e-10, 1.09e-03, 5.81e-21, 2.11e+03, 2.11e+03]    []  
28000     [8.21e-10, 1.05e-03, 2.72e-24, 2.09e+03, 2.09e+03]    [8.21e-10, 1.05e-03, 2.72e-24, 2.09e+03, 2.09e+03]    []  
26000     [1.55e+01, 1.89e-05, 2.24e-34, 2.10e+03, 2.10e+03]    [1.55e+01, 1.89e-05, 2.24e-34, 2.10e+03, 2.10e+03]    []  
18000     [5.54e-10, 9.21e-04, 2.85e-19, 2.42e+03, 2.42e+03]    [5.54e-10, 9.21e-04, 2.85e-19, 2.42e+03, 2.42e+03]    []  
24000     [1.04e-04, 1.05e-03, 4.70e-27, 2.09e+03, 2.09e+03]    [1.04e-04, 1.05e-03, 4.70e-27, 2.09e+03, 2.09e+03]    []  
30000     [6.26e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [6.26e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
19000     [1.62e-04, 6.29e-05, 3.25e-06, 4.41e-01, 4.41e-01]    [1.62e-04, 6.29e-05, 3.25e-06, 4.41e-01, 4.41e-01]    []  
21000     [5.77e-04, 2.37e-04, 3.82e-06, 2.53e-01, 2.53e-01]    [5.77e-04, 2.37e-04, 3.82e-06, 2.53e-01, 2.53e-01]    []  
29000     [8.37e-09, 1.05e-03, 1.48e-23, 2.09e+03, 2.09e+03]    [8.37e-09, 1.05e-03, 1.48e-23, 2.09e+03, 2.09e+03]    []  
27000     [2.19e-01, 1.01e-03, 3.31e-35, 2.09e+03, 2.09e+03]    [2.19e-01, 1.01e-03, 3.31e-35, 2.09e+03, 2.09e+03]    []  
19000     [2.92e-08, 1.08e-03, 1.59e-21, 2.11e+03, 2.11e+03]    [2.92e-08, 1.08e-03, 1.59e-21, 2.11e+03, 2.11e+03]    []  
31000     [1.94e+00, 7.69e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [1.94e+00, 7.69e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  
25000     [2.37e+00, 7.85e-04, 1.55e-28, 2.09e+03, 2.09e+03]    [2.37e+00, 7.85e-04, 1.55e-28, 2.09e+03, 2.09e+03]    []  
19000     [9.81e-10, 9.30e-04, 3.43e-19, 2.40e+03, 2.40e+03]    [9.81e-10, 9.30e-04, 3.43e-19, 2.40e+03, 2.40e+03]    []  
30000     [6.16e-07, 1.05e-03, 4.14e-11, 2.09e+03, 2.09e+03]    [6.16e-07, 1.05e-03, 4.14e-11, 2.09e+03, 2.09e+03]    []  
28000     [1.68e-07, 1.05e-03, 3.40e-35, 2.09e+03, 2.09e+03]    [1.68e-07, 1.05e-03, 3.40e-35, 2.09e+03, 2.09e+03]    []  
22000     [5.23e-04, 2.34e-04, 1.54e-05, 2.70e-01, 2.70e-01]    [5.23e-04, 2.34e-04, 1.54e-05, 2.70e-01, 2.70e-01]    []  
20000     [1.43e-04, 6.24e-05, 5.53e-06, 4.72e-01, 4.72e-01]    [1.43e-04, 6.24e-05, 5.53e-06, 4.72e-01, 4.72e-01]    []  
32000     [2.32e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.32e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
26000     [7.23e-09, 1.05e-03, 6.18e-29, 2.09e+03, 2.09e+03]    [7.23e-09, 1.05e-03, 6.18e-29, 2.09e+03, 2.09e+03]    []  
20000     [6.00e-10, 1.08e-03, 1.44e-21, 2.10e+03, 2.10e+03]    [6.00e-10, 1.08e-03, 1.44e-21, 2.10e+03, 2.10e+03]    []  
31000     [1.53e-04, 9.64e-04, 1.80e-08, 9.73e+02, 9.73e+02]    [1.53e-04, 9.64e-04, 1.80e-08, 9.73e+02, 9.73e+02]    []  
29000     [8.54e-13, 1.05e-03, 9.45e-36, 2.09e+03, 2.09e+03]    [8.54e-13, 1.05e-03, 9.45e-36, 2.09e+03, 2.09e+03]    []  
20000     [1.13e-09, 9.40e-04, 2.41e-20, 2.39e+03, 2.39e+03]    [1.13e-09, 9.40e-04, 2.41e-20, 2.39e+03, 2.39e+03]    []  
33000     [3.23e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.23e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
23000     [8.19e-04, 2.29e-04, 7.80e-06, 2.72e-01, 2.72e-01]    [8.19e-04, 2.29e-04, 7.80e-06, 2.72e-01, 2.72e-01]    []  
21000     [1.37e-04, 7.05e-05, 4.51e-06, 3.02e-01, 3.02e-01]    [1.37e-04, 7.05e-05, 4.51e-06, 3.02e-01, 3.02e-01]    []  
32000     [5.23e-02, 8.22e-04, 1.62e-03, 7.65e+02, 7.65e+02]    [5.23e-02, 8.22e-04, 1.62e-03, 7.65e+02, 7.65e+02]    []  
27000     [2.23e-08, 1.05e-03, 1.78e-29, 2.09e+03, 2.09e+03]    [2.23e-08, 1.05e-03, 1.78e-29, 2.09e+03, 2.09e+03]    []  
30000     [7.38e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [7.38e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
34000     [3.21e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.21e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
21000     [1.73e-08, 1.07e-03, 6.55e-22, 2.10e+03, 2.10e+03]    [1.73e-08, 1.07e-03, 6.55e-22, 2.10e+03, 2.10e+03]    []  
21000     [7.44e-10, 9.49e-04, 4.36e-20, 2.37e+03, 2.37e+03]    [7.44e-10, 9.49e-04, 4.36e-20, 2.37e+03, 2.37e+03]    []  
33000     [1.50e-02, 6.26e-04, 7.78e-04, 6.50e+02, 6.50e+02]    [1.50e-02, 6.26e-04, 7.78e-04, 6.50e+02, 6.50e+02]    []  
24000     [1.59e-03, 2.24e-04, 5.99e-06, 2.06e-01, 2.06e-01]    [1.59e-03, 2.24e-04, 5.99e-06, 2.06e-01, 2.06e-01]    []  
28000     [5.55e-10, 1.05e-03, 5.51e-30, 2.09e+03, 2.09e+03]    [5.55e-10, 1.05e-03, 5.51e-30, 2.09e+03, 2.09e+03]    []  
35000     [4.62e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [4.62e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
31000     [5.75e-01, 9.88e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [5.75e-01, 9.88e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  
22000     [1.39e-04, 6.63e-05, 5.38e-06, 3.66e-01, 3.66e-01]    [1.39e-04, 6.63e-05, 5.38e-06, 3.66e-01, 3.66e-01]    []  
34000     [1.43e-02, 5.64e-04, 1.19e-03, 5.74e+02, 5.74e+02]    [1.43e-02, 5.64e-04, 1.19e-03, 5.74e+02, 5.74e+02]    []  
22000     [1.18e-07, 1.06e-03, 2.34e-22, 2.09e+03, 2.09e+03]    [1.18e-07, 1.06e-03, 2.34e-22, 2.09e+03, 2.09e+03]    []  
36000     [3.36e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.36e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
29000     [3.07e-05, 1.05e-03, 1.08e-30, 2.09e+03, 2.09e+03]    [3.07e-05, 1.05e-03, 1.08e-30, 2.09e+03, 2.09e+03]    []  
22000     [9.71e-10, 9.58e-04, 3.17e-21, 2.36e+03, 2.36e+03]    [9.71e-10, 9.58e-04, 3.17e-21, 2.36e+03, 2.36e+03]    []  
32000     [2.62e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.62e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
25000     [4.93e-04, 2.16e-04, 4.49e-06, 1.87e-01, 1.87e-01]    [4.93e-04, 2.16e-04, 4.49e-06, 1.87e-01, 1.87e-01]    []  
35000     [1.24e-02, 5.02e-04, 3.73e-04, 5.06e+02, 5.06e+02]    [1.24e-02, 5.02e-04, 3.73e-04, 5.06e+02, 5.06e+02]    []  
23000     [1.47e-04, 7.02e-05, 3.53e-06, 2.46e-01, 2.46e-01]    [1.47e-04, 7.02e-05, 3.53e-06, 2.46e-01, 2.46e-01]    []  
37000     [4.72e-01, 9.70e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [4.72e-01, 9.70e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  
23000     [2.99e-08, 1.05e-03, 1.08e-22, 2.09e+03, 2.09e+03]    [2.99e-08, 1.05e-03, 1.08e-22, 2.09e+03, 2.09e+03]    []  
30000     [3.97e-04, 1.05e-03, 1.54e-31, 2.09e+03, 2.09e+03]    [3.97e-04, 1.05e-03, 1.54e-31, 2.09e+03, 2.09e+03]    []  
33000     [9.38e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.38e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
26000     [4.59e-04, 2.15e-04, 3.19e-05, 1.48e-01, 1.48e-01]    [4.59e-04, 2.15e-04, 3.19e-05, 1.48e-01, 1.48e-01]    []  
23000     [8.47e-10, 9.67e-04, 1.77e-21, 2.35e+03, 2.35e+03]    [8.47e-10, 9.67e-04, 1.77e-21, 2.35e+03, 2.35e+03]    []  
36000     [1.26e-02, 4.46e-04, 3.26e-04, 4.49e+02, 4.49e+02]    [1.26e-02, 4.46e-04, 3.26e-04, 4.49e+02, 4.49e+02]    []  
38000     [3.50e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.50e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
24000     [1.51e-04, 7.19e-05, 3.33e-06, 3.93e-01, 3.93e-01]    [1.51e-04, 7.19e-05, 3.33e-06, 3.93e-01, 3.93e-01]    []  
34000     [2.10e-01, 1.03e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.10e-01, 1.03e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
31000     [9.90e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.90e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
37000     [1.49e-02, 3.88e-04, 8.18e-04, 3.93e+02, 3.93e+02]    [1.49e-02, 3.88e-04, 8.18e-04, 3.93e+02, 3.93e+02]    []  
24000     [1.27e-08, 1.05e-03, 6.79e-23, 2.09e+03, 2.09e+03]    [1.27e-08, 1.05e-03, 6.79e-23, 2.09e+03, 2.09e+03]    []  
39000     [1.78e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.78e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
27000     [1.38e-03, 2.07e-04, 1.07e-05, 4.36e-01, 4.36e-01]    [1.38e-03, 2.07e-04, 1.07e-05, 4.36e-01, 4.36e-01]    []  
24000     [6.46e-10, 9.75e-04, 3.19e-22, 2.33e+03, 2.33e+03]    [6.46e-10, 9.75e-04, 3.19e-22, 2.33e+03, 2.33e+03]    []  
35000     [9.93e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.93e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
32000     [3.61e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.61e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
38000     [2.67e-02, 3.79e-04, 9.01e-04, 3.34e+02, 3.34e+02]    [2.67e-02, 3.79e-04, 9.01e-04, 3.34e+02, 3.34e+02]    []  
25000     [1.77e-04, 8.82e-05, 3.31e-05, 3.44e+00, 3.44e+00]    [1.77e-04, 8.82e-05, 3.31e-05, 3.44e+00, 3.44e+00]    []  
40000     [2.84e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.84e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
                                                                                                                           25000     [4.20e-23, 1.05e-03, 6.17e-23, 2.09e+03, 2.09e+03]    [4.20e-23, 1.05e-03, 6.17e-23, 2.09e+03, 2.09e+03]    []  
28000     [6.26e-04, 2.05e-04, 1.03e-05, 8.17e-01, 8.17e-01]    [6.26e-04, 2.05e-04, 1.03e-05, 8.17e-01, 8.17e-01]    []  
36000     [9.22e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.22e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
39000     [2.11e-02, 3.39e-04, 1.05e-03, 2.85e+02, 2.85e+02]    [2.11e-02, 3.39e-04, 1.05e-03, 2.85e+02, 2.85e+02]    []  
41000     [3.34e-01, 1.02e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.34e-01, 1.02e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
25000     [3.42e-10, 9.83e-04, 2.10e-22, 2.32e+03, 2.32e+03]    [3.42e-10, 9.83e-04, 2.10e-22, 2.32e+03, 2.32e+03]    []  
33000     [1.15e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.15e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
                                                                                                                           26000     [1.56e-04, 7.23e-05, 6.94e-06, 1.79e-01, 1.79e-01]    [1.56e-04, 7.23e-05, 6.94e-06, 1.79e-01, 1.79e-01]    []  
37000     [3.48e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.48e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
40000     [8.59e-03, 2.62e-04, 9.13e-04, 2.44e+02, 2.44e+02]    [8.59e-03, 2.62e-04, 9.13e-04, 2.44e+02, 2.44e+02]    []  
42000     [2.46e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.46e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
29000     [4.65e-04, 1.98e-04, 2.22e-06, 1.04e-01, 1.04e-01]    [4.65e-04, 1.98e-04, 2.22e-06, 1.04e-01, 1.04e-01]    []  
26000     [2.54e-09, 1.01e-03, 5.66e-14, 1.78e+03, 1.78e+03]    [2.54e-09, 1.01e-03, 5.66e-14, 1.78e+03, 1.78e+03]    []  
34000     [6.51e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [6.51e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
26000     [2.58e-11, 9.91e-04, 1.67e-22, 2.31e+03, 2.31e+03]    [2.58e-11, 9.91e-04, 1.67e-22, 2.31e+03, 2.31e+03]    []  
38000     [5.24e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [5.24e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
41000     [4.65e-03, 2.21e-04, 5.40e-04, 2.08e+02, 2.08e+02]    [4.65e-03, 2.21e-04, 5.40e-04, 2.08e+02, 2.08e+02]    []  
43000     [2.33e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.33e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
27000     [1.60e-04, 7.21e-05, 4.30e-06, 1.84e-01, 1.84e-01]    [1.60e-04, 7.21e-05, 4.30e-06, 1.84e-01, 1.84e-01]    []  
30000     [3.43e-03, 1.86e-04, 3.17e-06, 2.15e-01, 2.15e-01]    [3.43e-03, 1.86e-04, 3.17e-06, 2.15e-01, 2.15e-01]    []  
35000     [5.13e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [5.13e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
27000     [2.82e-09, 9.74e-04, 1.36e-13, 1.74e+03, 1.74e+03]    [2.82e-09, 9.74e-04, 1.36e-13, 1.74e+03, 1.74e+03]    []  
44000     [2.85e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.85e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
42000     [3.40e-03, 2.12e-04, 4.43e-04, 1.75e+02, 1.75e+02]    [3.40e-03, 2.12e-04, 4.43e-04, 1.75e+02, 1.75e+02]    []  
39000     [3.92e-01, 9.82e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [3.92e-01, 9.82e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  
27000     [3.07e-11, 9.99e-04, 1.86e-23, 2.30e+03, 2.30e+03]    [3.07e-11, 9.99e-04, 1.86e-23, 2.30e+03, 2.30e+03]    []  
                                                                                                                           28000     [1.53e-04, 7.16e-05, 5.40e-06, 1.60e-01, 1.60e-01]    [1.53e-04, 7.16e-05, 5.40e-06, 1.60e-01, 1.60e-01]    []  
36000     [9.02e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.02e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
31000     [5.94e-04, 1.73e-04, 4.46e-06, 1.81e-01, 1.81e-01]    [5.94e-04, 1.73e-04, 4.46e-06, 1.81e-01, 1.81e-01]    []  
45000     [6.62e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [6.62e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
43000     [2.99e-03, 2.23e-04, 4.14e-04, 1.46e+02, 1.46e+02]    [2.99e-03, 2.23e-04, 4.14e-04, 1.46e+02, 1.46e+02]    []  
40000     [1.10e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.10e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
28000     [9.67e-02, 9.36e-04, 6.97e-05, 1.59e+03, 1.59e+03]    [9.67e-02, 9.36e-04, 6.97e-05, 1.59e+03, 1.59e+03]    []  
                                                                                                                           28000     [3.18e-09, 1.01e-03, 1.11e-23, 2.29e+03, 2.29e+03]    [3.18e-09, 1.01e-03, 1.11e-23, 2.29e+03, 2.29e+03]    []  
46000     [5.34e-01, 9.61e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [5.34e-01, 9.61e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  
37000     [6.67e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [6.67e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
44000     [2.67e-03, 2.08e-04, 4.00e-04, 1.21e+02, 1.21e+02]    [2.67e-03, 2.08e-04, 4.00e-04, 1.21e+02, 1.21e+02]    []  
29000     [1.57e-04, 6.99e-05, 4.27e-06, 3.10e-01, 3.10e-01]    [1.57e-04, 6.99e-05, 4.27e-06, 3.10e-01, 3.10e-01]    []  
41000     [3.10e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.10e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
32000     [9.79e-04, 1.62e-04, 1.27e-05, 8.83e-02, 8.83e-02]    [9.79e-04, 1.62e-04, 1.27e-05, 8.83e-02, 8.83e-02]    []  
47000     [1.47e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.47e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
29000     [2.33e-01, 8.12e-04, 1.37e-03, 8.31e+02, 8.31e+02]    [2.33e-01, 8.12e-04, 1.37e-03, 8.31e+02, 8.31e+02]    []  
45000     [2.14e-03, 1.99e-04, 3.58e-04, 9.75e+01, 9.75e+01]    [2.14e-03, 1.99e-04, 3.58e-04, 9.75e+01, 9.75e+01]    []  
38000     [9.19e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.19e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
42000     [1.44e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.44e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
29000     [1.37e-11, 1.01e-03, 2.80e-24, 2.27e+03, 2.27e+03]    [1.37e-11, 1.01e-03, 2.80e-24, 2.27e+03, 2.27e+03]    []  
33000     [1.01e-03, 1.82e-04, 2.98e-05, 4.59e-01, 4.59e-01]    [1.01e-03, 1.82e-04, 2.98e-05, 4.59e-01, 4.59e-01]    []  
30000     [1.55e-04, 6.71e-05, 4.77e-06, 1.36e-01, 1.36e-01]    [1.55e-04, 6.71e-05, 4.77e-06, 1.36e-01, 1.36e-01]    []  
48000     [9.41e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.41e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
46000     [3.16e-03, 2.17e-04, 6.46e-04, 7.74e+01, 7.74e+01]    [3.16e-03, 2.17e-04, 6.46e-04, 7.74e+01, 7.74e+01]    []  
39000     [4.91e-02, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [4.91e-02, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
43000     [1.86e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.86e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
30000     [1.52e-01, 7.64e-04, 1.06e-03, 6.95e+02, 6.95e+02]    [1.52e-01, 7.64e-04, 1.06e-03, 6.95e+02, 6.95e+02]    []  
49000     [4.58e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [4.58e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
47000     [1.55e-02, 2.55e-04, 6.74e-03, 5.99e+01, 5.99e+01]    [1.55e-02, 2.55e-04, 6.74e-03, 5.99e+01, 5.99e+01]    []  
30000     [7.63e-09, 1.02e-03, 8.69e-25, 2.26e+03, 2.26e+03]    [7.63e-09, 1.02e-03, 8.69e-25, 2.26e+03, 2.26e+03]    []  
34000     [6.05e-04, 1.77e-04, 2.17e-05, 1.99e-01, 1.99e-01]    [6.05e-04, 1.77e-04, 2.17e-05, 1.99e-01, 1.99e-01]    []  
31000     [1.51e-04, 6.68e-05, 3.48e-06, 1.91e-01, 1.91e-01]    [1.51e-04, 6.68e-05, 3.48e-06, 1.91e-01, 1.91e-01]    []  
44000     [1.09e-02, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.09e-02, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
40000     [8.75e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [8.75e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
50000     [5.75e-01, 9.87e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [5.75e-01, 9.87e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  

Best model at step 36000:
  train loss: 4.19e+03
  test loss: 4.19e+03
  test metric: []

'train' took 7732.626174 s

[I 2023-10-08 19:49:50,583] Trial 7 finished with value: 45.664963209016946 and parameters: {'num_domain': 12334, 'num_boundary': 4121, 'resampling_period': 17830, 'lr': 0.03374066820740554}. Best is trial 7 with value: 45.664963209016946.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000115 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [7.44e+01, 4.71e-04, 4.81e-07, 3.62e+03, 3.62e+03]    [7.44e+01, 4.71e-04, 4.81e-07, 3.62e+03, 3.62e+03]    []  
31000     [1.86e-01, 7.36e-04, 1.53e-03, 6.14e+02, 6.14e+02]    [1.86e-01, 7.36e-04, 1.53e-03, 6.14e+02, 6.14e+02]    []  
48000     [9.63e-03, 2.94e-04, 2.37e-03, 4.52e+01, 4.52e+01]    [9.63e-03, 2.94e-04, 2.37e-03, 4.52e+01, 4.52e+01]    []  
                                                                                                                           35000     [6.22e-04, 1.83e-04, 1.27e-05, 8.44e-02, 8.44e-02]    [6.22e-04, 1.83e-04, 1.27e-05, 8.44e-02, 8.44e-02]    []  
45000     [2.35e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.35e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
31000     [3.42e-10, 1.03e-03, 4.26e-25, 2.25e+03, 2.25e+03]    [3.42e-10, 1.03e-03, 4.26e-25, 2.25e+03, 2.25e+03]    []  
1000      [1.93e-02, 3.38e-04, 3.96e-04, 3.87e+01, 3.87e+01]    [1.93e-02, 3.38e-04, 3.96e-04, 3.87e+01, 3.87e+01]    []  
41000     [3.03e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.03e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
32000     [1.48e-04, 6.45e-05, 3.81e-06, 1.63e-01, 1.63e-01]    [1.48e-04, 6.45e-05, 3.81e-06, 1.63e-01, 1.63e-01]    []  
49000     [8.92e-03, 3.52e-04, 1.85e-03, 3.30e+01, 3.30e+01]    [8.92e-03, 3.52e-04, 1.85e-03, 3.30e+01, 3.30e+01]    []  
                                                                                                                           2000      [2.52e-03, 2.62e-04, 3.84e-04, 5.58e+00, 5.58e+00]    [2.52e-03, 2.62e-04, 3.84e-04, 5.58e+00, 5.58e+00]    []  
32000     [1.27e-01, 6.94e-04, 1.75e-03, 5.59e+02, 5.59e+02]    [1.27e-01, 6.94e-04, 1.75e-03, 5.59e+02, 5.59e+02]    []  
46000     [1.48e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.48e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
36000     [5.47e-04, 1.84e-04, 1.22e-05, 1.85e-01, 1.85e-01]    [5.47e-04, 1.84e-04, 1.22e-05, 1.85e-01, 1.85e-01]    []  
42000     [7.39e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [7.39e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
50000     [9.52e-03, 4.43e-04, 1.52e-03, 2.32e+01, 2.32e+01]    [9.52e-03, 4.43e-04, 1.52e-03, 2.32e+01, 2.32e+01]    []  

Best model at step 50000:
  train loss: 4.65e+01
  test loss: 4.65e+01
  test metric: []

'train' took 8114.762934 s

[I 2023-10-08 19:56:08,589] Trial 6 finished with value: 4.855526086724602 and parameters: {'num_domain': 8903, 'num_boundary': 8721, 'resampling_period': 27168, 'lr': 1.3268839470415566e-05}. Best is trial 6 with value: 4.855526086724602.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000098 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.91e+01, 1.17e-04, 2.84e-07, 3.61e+03, 3.61e+03]    [1.91e+01, 1.17e-04, 2.84e-07, 3.61e+03, 3.61e+03]    []  
32000     [1.14e-08, 1.03e-03, 1.53e-25, 2.24e+03, 2.24e+03]    [1.14e-08, 1.03e-03, 1.53e-25, 2.24e+03, 2.24e+03]    []  
33000     [1.37e-04, 6.40e-05, 4.15e-06, 1.51e-01, 1.51e-01]    [1.37e-04, 6.40e-05, 4.15e-06, 1.51e-01, 1.51e-01]    []  
3000      [1.19e-02, 1.57e-04, 1.66e-04, 5.37e+00, 5.37e+00]    [1.19e-02, 1.57e-04, 1.66e-04, 5.37e+00, 5.37e+00]    []  
47000     [2.54e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.54e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
                                                                                                                           1000      [1.47e-10, 1.05e-03, 6.57e-17, 2.09e+03, 2.09e+03]    [1.47e-10, 1.05e-03, 6.57e-17, 2.09e+03, 2.09e+03]    []  
43000     [1.38e-02, 1.04e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.38e-02, 1.04e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
33000     [2.76e-02, 6.34e-04, 2.67e-03, 5.10e+02, 5.10e+02]    [2.76e-02, 6.34e-04, 2.67e-03, 5.10e+02, 5.10e+02]    []  
37000     [3.91e-04, 1.87e-04, 2.13e-06, 6.59e-02, 6.59e-02]    [3.91e-04, 1.87e-04, 2.13e-06, 6.59e-02, 6.59e-02]    []  
4000      [2.07e-03, 1.41e-04, 7.91e-05, 2.14e+00, 2.14e+00]    [2.07e-03, 1.41e-04, 7.91e-05, 2.14e+00, 2.14e+00]    []  
33000     [1.24e-08, 1.04e-03, 6.88e-26, 2.23e+03, 2.23e+03]    [1.24e-08, 1.04e-03, 6.88e-26, 2.23e+03, 2.23e+03]    []  
48000     [3.06e+00, 6.33e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [3.06e+00, 6.33e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  
34000     [1.33e-04, 6.67e-05, 2.97e-06, 1.79e-01, 1.79e-01]    [1.33e-04, 6.67e-05, 2.97e-06, 1.79e-01, 1.79e-01]    []  
2000      [8.46e-11, 1.05e-03, 6.16e-17, 2.09e+03, 2.09e+03]    [8.46e-11, 1.05e-03, 6.16e-17, 2.09e+03, 2.09e+03]    []  
5000      [9.08e-04, 1.79e-04, 1.35e-04, 2.96e+00, 2.96e+00]    [9.08e-04, 1.79e-04, 1.35e-04, 2.96e+00, 2.96e+00]    []  
44000     [2.73e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.73e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
34000     [1.27e-02, 5.73e-04, 8.36e-04, 4.64e+02, 4.64e+02]    [1.27e-02, 5.73e-04, 8.36e-04, 4.64e+02, 4.64e+02]    []  
38000     [9.53e-04, 1.90e-04, 2.36e-06, 7.86e-02, 7.86e-02]    [9.53e-04, 1.90e-04, 2.36e-06, 7.86e-02, 7.86e-02]    []  
3000      [7.59e-11, 1.05e-03, 3.26e-16, 2.09e+03, 2.09e+03]    [7.59e-11, 1.05e-03, 3.26e-16, 2.09e+03, 2.09e+03]    []  
49000     [7.66e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [7.66e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
6000      [6.91e-04, 3.17e-04, 2.89e-05, 5.81e-01, 5.81e-01]    [6.91e-04, 3.17e-04, 2.89e-05, 5.81e-01, 5.81e-01]    []  
34000     [1.13e-10, 1.04e-03, 8.14e-27, 2.22e+03, 2.22e+03]    [1.13e-10, 1.04e-03, 8.14e-27, 2.22e+03, 2.22e+03]    []  
35000     [1.20e-04, 6.27e-05, 4.01e-06, 2.23e-01, 2.23e-01]    [1.20e-04, 6.27e-05, 4.01e-06, 2.23e-01, 2.23e-01]    []  
45000     [2.23e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.23e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
4000      [2.98e-02, 6.68e-05, 6.58e-05, 1.83e+01, 1.83e+01]    [2.98e-02, 6.68e-05, 6.58e-05, 1.83e+01, 1.83e+01]    []  
50000     [8.59e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [8.59e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  

Best model at step 1000:
  train loss: 4.19e+03
  test loss: 4.19e+03
  test metric: []

'train' took 8758.478624 s

[I 2023-10-08 20:06:46,864] Trial 5 finished with value: 45.662423301783065 and parameters: {'num_domain': 25310, 'num_boundary': 5615, 'resampling_period': 40927, 'lr': 0.052819097604981456}. Best is trial 6 with value: 4.855526086724602.
7000      [1.06e-03, 3.51e-04, 1.10e-04, 1.77e+00, 1.77e+00]    [1.06e-03, 3.51e-04, 1.10e-04, 1.77e+00, 1.77e+00]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000131 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.61e+01, 1.77e-04, 5.21e-08, 3.61e+03, 3.61e+03]    [4.61e+01, 1.77e-04, 5.21e-08, 3.61e+03, 3.61e+03]    []  
39000     [6.53e-04, 1.96e-04, 7.38e-05, 3.14e-01, 3.14e-01]    [6.53e-04, 1.96e-04, 7.38e-05, 3.14e-01, 3.14e-01]    []  
35000     [3.55e-03, 5.33e-04, 8.11e-05, 4.18e+02, 4.18e+02]    [3.55e-03, 5.33e-04, 8.11e-05, 4.18e+02, 4.18e+02]    []  
                                                                                                                           46000     [7.63e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [7.63e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
5000      [1.71e-03, 7.58e-04, 2.58e-05, 9.18e+00, 9.18e+00]    [1.71e-03, 7.58e-04, 2.58e-05, 9.18e+00, 9.18e+00]    []  
8000      [3.17e-03, 3.82e-04, 4.29e-05, 3.57e-01, 3.57e-01]    [3.17e-03, 3.82e-04, 4.29e-05, 3.57e-01, 3.57e-01]    []  
35000     [4.80e-09, 1.05e-03, 6.32e-27, 2.21e+03, 2.21e+03]    [4.80e-09, 1.05e-03, 6.32e-27, 2.21e+03, 2.21e+03]    []  
36000     [1.22e-04, 6.49e-05, 1.55e-05, 1.54e+00, 1.54e+00]    [1.22e-04, 6.49e-05, 1.55e-05, 1.54e+00, 1.54e+00]    []  
1000      [1.18e-08, 9.31e-04, 2.70e-14, 2.40e+03, 2.40e+03]    [1.18e-08, 9.31e-04, 2.70e-14, 2.40e+03, 2.40e+03]    []  
40000     [4.36e-04, 1.96e-04, 4.69e-06, 6.12e-02, 6.12e-02]    [4.36e-04, 1.96e-04, 4.69e-06, 6.12e-02, 6.12e-02]    []  
6000      [1.23e-03, 8.62e-04, 3.33e-05, 7.41e+00, 7.41e+00]    [1.23e-03, 8.62e-04, 3.33e-05, 7.41e+00, 7.41e+00]    []  
36000     [5.16e-03, 4.69e-04, 5.81e-05, 3.71e+02, 3.71e+02]    [5.16e-03, 4.69e-04, 5.81e-05, 3.71e+02, 3.71e+02]    []  
9000      [8.42e-04, 4.00e-04, 1.11e-04, 3.73e-01, 3.73e-01]    [8.42e-04, 4.00e-04, 1.11e-04, 3.73e-01, 3.73e-01]    []  
47000     [5.52e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [5.52e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                      37000     [1.10e-04, 5.73e-05, 3.37e-06, 1.01e-01, 1.01e-01]    [1.10e-04, 5.73e-05, 3.37e-06, 1.01e-01, 1.01e-01]    []  
36000     [2.62e-10, 1.05e-03, 2.06e-27, 2.21e+03, 2.21e+03]    [2.62e-10, 1.05e-03, 2.06e-27, 2.21e+03, 2.21e+03]    []  
7000      [4.24e-04, 3.76e-04, 9.81e-06, 5.84e+00, 5.84e+00]    [4.24e-04, 3.76e-04, 9.81e-06, 5.84e+00, 5.84e+00]    []  
10000     [5.77e-04, 4.12e-04, 4.28e-05, 2.52e-01, 2.52e-01]    [5.77e-04, 4.12e-04, 4.28e-05, 2.52e-01, 2.52e-01]    []  
2000      [2.79e-07, 1.05e-03, 5.19e-16, 2.21e+03, 2.21e+03]    [2.79e-07, 1.05e-03, 5.19e-16, 2.21e+03, 2.21e+03]    []  
41000     [4.08e-04, 2.02e-04, 4.18e-06, 5.43e-02, 5.43e-02]    [4.08e-04, 2.02e-04, 4.18e-06, 5.43e-02, 5.43e-02]    []  
48000     [6.75e-02, 1.04e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [6.75e-02, 1.04e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
37000     [2.01e-03, 4.00e-04, 1.89e-05, 3.33e+02, 3.33e+02]    [2.01e-03, 4.00e-04, 1.89e-05, 3.33e+02, 3.33e+02]    []  
8000      [4.12e-04, 1.72e-04, 7.79e-05, 3.57e+00, 3.57e+00]    [4.12e-04, 1.72e-04, 7.79e-05, 3.57e+00, 3.57e+00]    []  
11000     [1.99e-03, 4.25e-04, 7.53e-05, 3.22e-01, 3.22e-01]    [1.99e-03, 4.25e-04, 7.53e-05, 3.22e-01, 3.22e-01]    []  
38000     [1.04e-04, 5.38e-05, 4.41e-06, 8.80e-02, 8.80e-02]    [1.04e-04, 5.38e-05, 4.41e-06, 8.80e-02, 8.80e-02]    []  
3000      [2.24e-07, 1.09e-03, 1.45e-16, 2.13e+03, 2.13e+03]    [2.24e-07, 1.09e-03, 1.45e-16, 2.13e+03, 2.13e+03]    []  
37000     [1.77e-09, 1.06e-03, 3.65e-28, 2.20e+03, 2.20e+03]    [1.77e-09, 1.06e-03, 3.65e-28, 2.20e+03, 2.20e+03]    []  
42000     [3.73e-04, 2.05e-04, 7.54e-06, 5.12e-02, 5.12e-02]    [3.73e-04, 2.05e-04, 7.54e-06, 5.12e-02, 5.12e-02]    []  
49000     [5.33e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [5.33e-04, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
9000      [4.33e-04, 2.89e-04, 1.15e-04, 5.29e+00, 5.29e+00]    [4.33e-04, 2.89e-04, 1.15e-04, 5.29e+00, 5.29e+00]    []  
12000     [3.61e-04, 4.33e-04, 1.12e-05, 1.37e-01, 1.37e-01]    [3.61e-04, 4.33e-04, 1.12e-05, 1.37e-01, 1.37e-01]    []  
                                                                                                                                                                                                                                                      38000     [6.71e-04, 3.31e-04, 1.19e-05, 3.00e+02, 3.00e+02]    [6.71e-04, 3.31e-04, 1.19e-05, 3.00e+02, 3.00e+02]    []  
4000      [5.94e-08, 1.07e-03, 5.17e-17, 2.10e+03, 2.10e+03]    [5.94e-08, 1.07e-03, 5.17e-17, 2.10e+03, 2.10e+03]    []  
13000     [5.49e-04, 4.43e-04, 1.03e-04, 1.22e-01, 1.22e-01]    [5.49e-04, 4.43e-04, 1.03e-04, 1.22e-01, 1.22e-01]    []  
10000     [3.84e-04, 2.40e-04, 7.15e-05, 2.16e+00, 2.16e+00]    [3.84e-04, 2.40e-04, 7.15e-05, 2.16e+00, 2.16e+00]    []  
50000     [5.07e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [5.07e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  

Best model at step 1000:
  train loss: 4.19e+03
  test loss: 4.19e+03
  test metric: []

'train' took 9663.524907 s

[I 2023-10-08 20:21:30,946] Trial 2 finished with value: 45.66239575301855 and parameters: {'num_domain': 35879, 'num_boundary': 8594, 'resampling_period': 14246, 'lr': 0.028776332239458746}. Best is trial 6 with value: 4.855526086724602.
39000     [1.00e-04, 5.17e-05, 8.66e-06, 9.72e-02, 9.72e-02]    [1.00e-04, 5.17e-05, 8.66e-06, 9.72e-02, 9.72e-02]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000103 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.45e+01, 6.94e-05, 1.57e-07, 3.60e+03, 3.60e+03]    [4.45e+01, 6.94e-05, 1.57e-07, 3.60e+03, 3.60e+03]    []  
43000     [4.25e-04, 2.11e-04, 1.36e-06, 4.97e-02, 4.97e-02]    [4.25e-04, 2.11e-04, 1.36e-06, 4.97e-02, 4.97e-02]    []  
38000     [1.54e-09, 1.06e-03, 9.96e-29, 2.19e+03, 2.19e+03]    [1.54e-09, 1.06e-03, 9.96e-29, 2.19e+03, 2.19e+03]    []  
44000     [5.29e-05, 3.59e-05, 2.16e-06, 4.64e-02, 4.64e-02]    [5.29e-05, 3.59e-05, 2.16e-06, 4.64e-02, 4.64e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            23000     [1.12e-02, 2.42e-04, 3.01e-05, 3.22e-01, 3.22e-01]    [1.12e-02, 2.42e-04, 3.01e-05, 3.22e-01, 3.22e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       45000     [8.04e-05, 3.67e-05, 2.50e-06, 3.55e-02, 3.55e-02]    [8.04e-05, 3.67e-05, 2.50e-06, 3.55e-02, 3.55e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        46000     [5.82e-05, 2.48e-05, 2.41e-06, 2.35e-02, 2.35e-02]    [5.82e-05, 2.48e-05, 2.41e-06, 2.35e-02, 2.35e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                 24000     [5.05e-04, 2.29e-04, 2.92e-05, 1.39e-01, 1.39e-01]    [5.05e-04, 2.29e-04, 2.92e-05, 1.39e-01, 1.39e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            41000     [2.13e-13, 1.07e-03, 1.95e-30, 2.17e+03, 2.17e+03]    [2.13e-13, 1.07e-03, 1.95e-30, 2.17e+03, 2.17e+03]    []  
8000      [6.62e-11, 1.05e-03, 3.82e-18, 2.09e+03, 2.09e+03]    [6.62e-11, 1.05e-03, 3.82e-18, 2.09e+03, 2.09e+03]    []  
19000     [3.35e-03, 6.22e-04, 1.50e-04, 3.02e+00, 3.02e+00]    [3.35e-03, 6.22e-04, 1.50e-04, 3.02e+00, 3.02e+00]    []  
16000     [4.76e-04, 2.34e-04, 1.30e-06, 1.16e+00, 1.16e+00]    [4.76e-04, 2.34e-04, 1.30e-06, 1.16e+00, 1.16e+00]    []  
42000     [1.17e-04, 9.28e-05, 7.57e-06, 1.93e+02, 1.93e+02]    [1.17e-04, 9.28e-05, 7.57e-06, 1.93e+02, 1.93e+02]    []  
47000     [2.07e-03, 1.97e-04, 9.33e-06, 4.14e-02, 4.14e-02]    [2.07e-03, 1.97e-04, 9.33e-06, 4.14e-02, 4.14e-02]    []  
20000     [2.39e-04, 6.57e-04, 1.05e-05, 5.45e-02, 5.45e-02]    [2.39e-04, 6.57e-04, 1.05e-05, 5.45e-02, 5.45e-02]    []  
43000     [8.00e-05, 6.30e-05, 2.71e-06, 2.56e-01, 2.56e-01]    [8.00e-05, 6.30e-05, 2.71e-06, 2.56e-01, 2.56e-01]    []  
4000      [9.44e-02, 7.46e-04, 1.98e-11, 2.72e+03, 2.72e+03]    [9.44e-02, 7.46e-04, 1.98e-11, 2.72e+03, 2.72e+03]    []  
17000     [6.47e-04, 4.08e-04, 1.48e-06, 1.83e+00, 1.83e+00]    [6.47e-04, 4.08e-04, 1.48e-06, 1.83e+00, 1.83e+00]    []  
9000      [6.86e-11, 1.05e-03, 1.46e-18, 2.09e+03, 2.09e+03]    [6.86e-11, 1.05e-03, 1.46e-18, 2.09e+03, 2.09e+03]    []  
42000     [1.92e-12, 1.08e-03, 1.92e-30, 2.16e+03, 2.16e+03]    [1.92e-12, 1.08e-03, 1.92e-30, 2.16e+03, 2.16e+03]    []  
                                                                                                                           21000     [2.05e-04, 6.27e-04, 5.13e-05, 5.63e-02, 5.63e-02]    [2.05e-04, 6.27e-04, 5.13e-05, 5.63e-02, 5.63e-02]    []  
48000     [6.42e-04, 1.98e-04, 1.45e-05, 2.08e-01, 2.08e-01]    [6.42e-04, 1.98e-04, 1.45e-05, 2.08e-01, 2.08e-01]    []  
43000     [2.29e-04, 5.26e-05, 1.82e-06, 1.71e+02, 1.71e+02]    [2.29e-04, 5.26e-05, 1.82e-06, 1.71e+02, 1.71e+02]    []  
18000     [5.90e-04, 3.92e-04, 2.43e-06, 2.61e+00, 2.61e+00]    [5.90e-04, 3.92e-04, 2.43e-06, 2.61e+00, 2.61e+00]    []  
                                                                                                                           44000     [8.44e-05, 4.34e-05, 1.56e-05, 2.86e-01, 2.86e-01]    [8.44e-05, 4.34e-05, 1.56e-05, 2.86e-01, 2.86e-01]    []  
10000     [7.62e-11, 1.05e-03, 1.98e-18, 2.09e+03, 2.09e+03]    [7.62e-11, 1.05e-03, 1.98e-18, 2.09e+03, 2.09e+03]    []  
5000      [2.75e-02, 7.68e-04, 5.50e-12, 2.67e+03, 2.67e+03]    [2.75e-02, 7.68e-04, 5.50e-12, 2.67e+03, 2.67e+03]    []  
22000     [1.22e-03, 2.88e-04, 3.65e-04, 2.45e+00, 2.45e+00]    [1.22e-03, 2.88e-04, 3.65e-04, 2.45e+00, 2.45e+00]    []  
43000     [1.40e-09, 1.08e-03, 2.35e-31, 2.15e+03, 2.15e+03]    [1.40e-09, 1.08e-03, 2.35e-31, 2.15e+03, 2.15e+03]    []  
19000     [4.96e-04, 3.23e-04, 1.56e-06, 3.95e-01, 3.95e-01]    [4.96e-04, 3.23e-04, 1.56e-06, 3.95e-01, 3.95e-01]    []  
49000     [7.04e-04, 1.93e-04, 4.19e-05, 1.30e-01, 1.30e-01]    [7.04e-04, 1.93e-04, 4.19e-05, 1.30e-01, 1.30e-01]    []  
44000     [1.40e-04, 2.04e-05, 4.10e-06, 1.50e+02, 1.50e+02]    [1.40e-04, 2.04e-05, 4.10e-06, 1.50e+02, 1.50e+02]    []  
23000     [4.63e-04, 3.19e-04, 1.44e-04, 8.42e-01, 8.42e-01]    [4.63e-04, 3.19e-04, 1.44e-04, 8.42e-01, 8.42e-01]    []  
11000     [7.28e-11, 1.05e-03, 3.64e-19, 2.09e+03, 2.09e+03]    [7.28e-11, 1.05e-03, 3.64e-19, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               20000     [3.45e-04, 2.06e-04, 1.01e-06, 4.77e-01, 4.77e-01]    [3.45e-04, 2.06e-04, 1.01e-06, 4.77e-01, 4.77e-01]    []  
45000     [7.99e-05, 4.60e-05, 1.43e-06, 9.63e-02, 9.63e-02]    [7.99e-05, 4.60e-05, 1.43e-06, 9.63e-02, 9.63e-02]    []  
6000      [6.94e-03, 7.84e-04, 6.10e-13, 2.64e+03, 2.64e+03]    [6.94e-03, 7.84e-04, 6.10e-13, 2.64e+03, 2.64e+03]    []  
44000     [7.69e-12, 1.08e-03, 3.95e-32, 2.15e+03, 2.15e+03]    [7.69e-12, 1.08e-03, 3.95e-32, 2.15e+03, 2.15e+03]    []  
24000     [1.40e-02, 3.72e-04, 1.66e-04, 1.35e-01, 1.35e-01]    [1.40e-02, 3.72e-04, 1.66e-04, 1.35e-01, 1.35e-01]    []  
50000     [4.05e-04, 1.91e-04, 1.26e-05, 3.79e-02, 3.79e-02]    [4.05e-04, 1.91e-04, 1.26e-05, 3.79e-02, 3.79e-02]    []  

Best model at step 50000:
  train loss: 7.64e-02
  test loss: 7.64e-02
  test metric: []

'train' took 11284.025021 s

[I 2023-10-08 20:48:20,772] Trial 0 finished with value: 0.4913720930752669 and parameters: {'num_domain': 72691, 'num_boundary': 2649, 'resampling_period': 22246, 'lr': 0.0007777338906153725}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000111 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.92e+01, 4.75e-05, 1.98e-07, 3.59e+03, 3.59e+03]    [3.92e+01, 4.75e-05, 1.98e-07, 3.59e+03, 3.59e+03]    []  
[W 2023-10-08 20:48:34,215] Trial 15 failed with parameters: {'num_domain': 96521, 'num_boundary': 4675, 'resampling_period': 3380, 'lr': 4.316879576008258e-05} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 10.75 GiB total capacity; 9.77 GiB already allocated; 73.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 10.75 GiB total capacity; 9.77 GiB already allocated; 73.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-08 20:48:34,244] Trial 15 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 80, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 10.75 GiB total capacity; 9.77 GiB already allocated; 73.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
21000     [2.47e-04, 1.49e-04, 8.59e-07, 4.10e-01, 4.10e-01]    [2.47e-04, 1.49e-04, 8.59e-07, 4.10e-01, 4.10e-01]    []  
45000     [1.87e-04, 3.78e-06, 8.69e-06, 1.30e+02, 1.30e+02]    [1.87e-04, 3.78e-06, 8.69e-06, 1.30e+02, 1.30e+02]    []  
12000     [2.34e-08, 1.05e-03, 8.50e-19, 2.09e+03, 2.09e+03]    [2.34e-08, 1.05e-03, 8.50e-19, 2.09e+03, 2.09e+03]    []  
25000     [1.94e-04, 3.87e-04, 9.59e-05, 1.18e-01, 1.18e-01]    [1.94e-04, 3.87e-04, 9.59e-05, 1.18e-01, 1.18e-01]    []  
46000     [6.60e-05, 4.21e-05, 2.88e-06, 6.11e-02, 6.11e-02]    [6.60e-05, 4.21e-05, 2.88e-06, 6.11e-02, 6.11e-02]    []  
7000      [3.62e-04, 7.98e-04, 2.06e-13, 2.61e+03, 2.61e+03]    [3.62e-04, 7.98e-04, 2.06e-13, 2.61e+03, 2.61e+03]    []  
22000     [2.65e-04, 8.64e-05, 3.42e-06, 3.02e+00, 3.02e+00]    [2.65e-04, 8.64e-05, 3.42e-06, 3.02e+00, 3.02e+00]    []  
45000     [3.77e-10, 1.08e-03, 2.09e-33, 2.14e+03, 2.14e+03]    [3.77e-10, 1.08e-03, 2.09e-33, 2.14e+03, 2.14e+03]    []  
26000     [1.54e-03, 4.18e-04, 1.47e-05, 5.59e-01, 5.59e-01]    [1.54e-03, 4.18e-04, 1.47e-05, 5.59e-01, 5.59e-01]    []  
13000     [1.59e-04, 6.69e-04, 1.17e-08, 1.49e+03, 1.49e+03]    [1.59e-04, 6.69e-04, 1.17e-08, 1.49e+03, 1.49e+03]    []  
46000     [1.10e-04, 5.76e-06, 5.45e-06, 1.12e+02, 1.12e+02]    [1.10e-04, 5.76e-06, 5.45e-06, 1.12e+02, 1.12e+02]    []  
23000     [2.16e-04, 8.91e-05, 3.52e-07, 3.15e-01, 3.15e-01]    [2.16e-04, 8.91e-05, 3.52e-07, 3.15e-01, 3.15e-01]    []  
47000     [5.89e-05, 3.94e-05, 1.17e-06, 5.85e-02, 5.85e-02]    [5.89e-05, 3.94e-05, 1.17e-06, 5.85e-02, 5.85e-02]    []  
27000     [2.06e-02, 4.54e-04, 8.75e-06, 7.88e-02, 7.88e-02]    [2.06e-02, 4.54e-04, 8.75e-06, 7.88e-02, 7.88e-02]    []  
8000      [1.99e-05, 8.11e-04, 2.96e-14, 2.59e+03, 2.59e+03]    [1.99e-05, 8.11e-04, 2.96e-14, 2.59e+03, 2.59e+03]    []  
46000     [1.22e-08, 1.09e-03, 8.07e-34, 2.14e+03, 2.14e+03]    [1.22e-08, 1.09e-03, 8.07e-34, 2.14e+03, 2.14e+03]    []  
24000     [1.68e-04, 1.18e-04, 1.05e-06, 3.37e-01, 3.37e-01]    [1.68e-04, 1.18e-04, 1.05e-06, 3.37e-01, 3.37e-01]    []  
14000     [2.99e-02, 1.71e-04, 2.34e-04, 3.00e+02, 3.00e+02]    [2.99e-02, 1.71e-04, 2.34e-04, 3.00e+02, 3.00e+02]    []  
28000     [3.69e-04, 4.68e-04, 3.25e-05, 1.10e-01, 1.10e-01]    [3.69e-04, 4.68e-04, 3.25e-05, 1.10e-01, 1.10e-01]    []  
47000     [1.55e-05, 3.16e-05, 1.18e-06, 9.60e+01, 9.60e+01]    [1.55e-05, 3.16e-05, 1.18e-06, 9.60e+01, 9.60e+01]    []  
25000     [1.79e-04, 1.07e-04, 1.09e-06, 2.84e-01, 2.84e-01]    [1.79e-04, 1.07e-04, 1.09e-06, 2.84e-01, 2.84e-01]    []  
48000     [6.08e-05, 4.04e-05, 2.58e-06, 5.67e-02, 5.67e-02]    [6.08e-05, 4.04e-05, 2.58e-06, 5.67e-02, 5.67e-02]    []  
9000      [1.84e-06, 8.24e-04, 1.78e-14, 2.56e+03, 2.56e+03]    [1.84e-06, 8.24e-04, 1.78e-14, 2.56e+03, 2.56e+03]    []  
29000     [6.10e-04, 4.61e-04, 5.03e-05, 7.98e-02, 7.98e-02]    [6.10e-04, 4.61e-04, 5.03e-05, 7.98e-02, 7.98e-02]    []  
15000     [3.47e-02, 1.12e-04, 1.75e-04, 1.27e+02, 1.27e+02]    [3.47e-02, 1.12e-04, 1.75e-04, 1.27e+02, 1.27e+02]    []  
47000     [1.13e-10, 1.09e-03, 0.00e+00, 2.13e+03, 2.13e+03]    [1.13e-10, 1.09e-03, 0.00e+00, 2.13e+03, 2.13e+03]    []  
26000     [1.71e-04, 9.91e-05, 5.19e-07, 2.45e-01, 2.45e-01]    [1.71e-04, 9.91e-05, 5.19e-07, 2.45e-01, 2.45e-01]    []  
48000     [4.43e-06, 8.76e-05, 1.26e-06, 8.11e+01, 8.11e+01]    [4.43e-06, 8.76e-05, 1.26e-06, 8.11e+01, 8.11e+01]    []  
30000     [2.30e-03, 2.33e-04, 6.49e-06, 4.72e-02, 4.72e-02]    [2.30e-03, 2.33e-04, 6.49e-06, 4.72e-02, 4.72e-02]    []  
49000     [5.76e-05, 5.23e-05, 6.42e-07, 1.04e-01, 1.04e-01]    [5.76e-05, 5.23e-05, 6.42e-07, 1.04e-01, 1.04e-01]    []  
10000     [2.40e-07, 8.37e-04, 3.72e-15, 2.54e+03, 2.54e+03]    [2.40e-07, 8.37e-04, 3.72e-15, 2.54e+03, 2.54e+03]    []  
16000     [1.46e-02, 3.21e-04, 5.06e-04, 4.95e+01, 4.95e+01]    [1.46e-02, 3.21e-04, 5.06e-04, 4.95e+01, 4.95e+01]    []  
27000     [1.71e-04, 8.70e-05, 7.02e-07, 3.65e-01, 3.65e-01]    [1.71e-04, 8.70e-05, 7.02e-07, 3.65e-01, 3.65e-01]    []  
48000     [3.25e-10, 1.09e-03, 0.00e+00, 2.13e+03, 2.13e+03]    [3.25e-10, 1.09e-03, 0.00e+00, 2.13e+03, 2.13e+03]    []  
31000     [7.12e-05, 2.02e-04, 8.16e-07, 4.92e-02, 4.92e-02]    [7.12e-05, 2.02e-04, 8.16e-07, 4.92e-02, 4.92e-02]    []  
49000     [2.76e-05, 1.77e-04, 7.47e-06, 6.77e+01, 6.77e+01]    [2.76e-05, 1.77e-04, 7.47e-06, 6.77e+01, 6.77e+01]    []  
28000     [1.73e-04, 8.74e-05, 1.31e-06, 2.91e-01, 2.91e-01]    [1.73e-04, 8.74e-05, 1.31e-06, 2.91e-01, 2.91e-01]    []  
32000     [7.75e-03, 3.26e-04, 1.16e-05, 1.14e+00, 1.14e+00]    [7.75e-03, 3.26e-04, 1.16e-05, 1.14e+00, 1.14e+00]    []  
50000     [5.60e-05, 4.24e-05, 5.99e-06, 1.10e-01, 1.10e-01]    [5.60e-05, 4.24e-05, 5.99e-06, 1.10e-01, 1.10e-01]    []  

Best model at step 48000:
  train loss: 1.14e-01
  test loss: 1.14e-01
  test metric: []

'train' took 12381.361184 s

[I 2023-10-08 21:07:06,068] Trial 4 finished with value: 0.5697639438641833 and parameters: {'num_domain': 77503, 'num_boundary': 8786, 'resampling_period': 12411, 'lr': 0.0007172081368525014}. Best is trial 0 with value: 0.4913720930752669.
17000     [8.91e-03, 5.32e-04, 2.78e-04, 1.86e+01, 1.86e+01]    [8.91e-03, 5.32e-04, 2.78e-04, 1.86e+01, 1.86e+01]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000104 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [2.40e+01, 6.57e-05, 1.29e-07, 3.61e+03, 3.61e+03]    [2.40e+01, 6.57e-05, 1.29e-07, 3.61e+03, 3.61e+03]    []  
11000     [3.74e-08, 8.49e-04, 2.45e-15, 2.53e+03, 2.53e+03]    [3.74e-08, 8.49e-04, 2.45e-15, 2.53e+03, 2.53e+03]    []  
                                                                                                                           29000     [1.40e-04, 8.42e-05, 6.63e-07, 2.05e-01, 2.05e-01]    [1.40e-04, 8.42e-05, 6.63e-07, 2.05e-01, 2.05e-01]    []  
49000     [1.62e-09, 1.09e-03, 0.00e+00, 2.12e+03, 2.12e+03]    [1.62e-09, 1.09e-03, 0.00e+00, 2.12e+03, 2.12e+03]    []  
33000     [2.31e-04, 3.07e-04, 4.50e-06, 4.14e-01, 4.14e-01]    [2.31e-04, 3.07e-04, 4.50e-06, 4.14e-01, 4.14e-01]    []  
50000     [7.54e-05, 3.04e-04, 2.09e-05, 5.58e+01, 5.58e+01]    [7.54e-05, 3.04e-04, 2.09e-05, 5.58e+01, 5.58e+01]    []  

Best model at step 50000:
  train loss: 1.12e+02
  test loss: 1.12e+02
  test metric: []

'train' took 12584.482394 s

[I 2023-10-08 21:10:20,157] Trial 3 finished with value: 7.5126421268648595 and parameters: {'num_domain': 87324, 'num_boundary': 5848, 'resampling_period': 23863, 'lr': 9.415201172429395e-06}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000110 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [7.82e+00, 2.13e-04, 1.59e-07, 3.60e+03, 3.60e+03]    [7.82e+00, 2.13e-04, 1.59e-07, 3.60e+03, 3.60e+03]    []  
18000     [1.03e-02, 9.07e-04, 4.71e-05, 9.08e+00, 9.08e+00]    [1.03e-02, 9.07e-04, 4.71e-05, 9.08e+00, 9.08e+00]    []  
1000      [1.96e-04, 1.78e-04, 9.95e-07, 1.58e+01, 1.58e+01]    [1.96e-04, 1.78e-04, 9.95e-07, 1.58e+01, 1.58e+01]    []  
30000     [1.26e-04, 8.80e-05, 1.53e-06, 2.90e-01, 2.90e-01]    [1.26e-04, 8.80e-05, 1.53e-06, 2.90e-01, 2.90e-01]    []  
34000     [3.55e-04, 1.85e-04, 1.80e-06, 2.94e-01, 2.94e-01]    [3.55e-04, 1.85e-04, 1.80e-06, 2.94e-01, 2.94e-01]    []  
12000     [5.11e-09, 8.61e-04, 6.69e-16, 2.51e+03, 2.51e+03]    [5.11e-09, 8.61e-04, 6.69e-16, 2.51e+03, 2.51e+03]    []  
50000     [1.37e-11, 1.09e-03, 0.00e+00, 2.12e+03, 2.12e+03]    [1.37e-11, 1.09e-03, 0.00e+00, 2.12e+03, 2.12e+03]    []  

Best model at step 50000:
  train loss: 4.24e+03
  test loss: 4.24e+03
  test metric: []

'train' took 12756.325507 s

[I 2023-10-08 21:13:03,950] Trial 1 finished with value: 45.981804550351015 and parameters: {'num_domain': 83171, 'num_boundary': 9212, 'resampling_period': 16876, 'lr': 3.249725642833982e-06}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000114 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [9.28e+00, 1.80e-04, 3.84e-07, 3.62e+03, 3.62e+03]    [9.28e+00, 1.80e-04, 3.84e-07, 3.62e+03, 3.62e+03]    []  
31000     [1.38e-04, 8.81e-05, 6.35e-07, 1.96e-01, 1.96e-01]    [1.38e-04, 8.81e-05, 6.35e-07, 1.96e-01, 1.96e-01]    []  
1000      [5.77e-09, 1.05e-03, 7.17e-14, 2.09e+03, 2.09e+03]    [5.77e-09, 1.05e-03, 7.17e-14, 2.09e+03, 2.09e+03]    []  
19000     [5.77e-03, 4.65e-04, 2.10e-05, 6.71e+00, 6.71e+00]    [5.77e-03, 4.65e-04, 2.10e-05, 6.71e+00, 6.71e+00]    []  
35000     [2.35e-04, 6.97e-05, 2.69e-08, 1.30e-01, 1.30e-01]    [2.35e-04, 6.97e-05, 2.69e-08, 1.30e-01, 1.30e-01]    []  
2000      [2.19e-04, 3.88e-05, 1.16e-09, 9.33e+00, 9.33e+00]    [2.19e-04, 3.88e-05, 1.16e-09, 9.33e+00, 9.33e+00]    []  
13000     [4.73e-10, 8.72e-04, 9.65e-17, 2.49e+03, 2.49e+03]    [4.73e-10, 8.72e-04, 9.65e-17, 2.49e+03, 2.49e+03]    []  
32000     [1.50e-04, 1.14e-04, 2.28e-06, 4.67e-01, 4.67e-01]    [1.50e-04, 1.14e-04, 2.28e-06, 4.67e-01, 4.67e-01]    []  
36000     [1.84e-04, 4.18e-05, 1.44e-07, 8.79e-02, 8.79e-02]    [1.84e-04, 4.18e-05, 1.44e-07, 8.79e-02, 8.79e-02]    []  
1000      [1.57e-07, 1.06e-03, 2.39e-15, 2.21e+03, 2.21e+03]    [1.57e-07, 1.06e-03, 2.39e-15, 2.21e+03, 2.21e+03]    []  
2000      [1.23e-10, 1.05e-03, 3.94e-23, 2.09e+03, 2.09e+03]    [1.23e-10, 1.05e-03, 3.94e-23, 2.09e+03, 2.09e+03]    []  
20000     [1.11e-02, 2.10e-04, 7.84e-05, 5.72e+00, 5.72e+00]    [1.11e-02, 2.10e-04, 7.84e-05, 5.72e+00, 5.72e+00]    []  
3000      [6.06e-05, 2.50e-05, 4.86e-09, 2.14e+00, 2.14e+00]    [6.06e-05, 2.50e-05, 4.86e-09, 2.14e+00, 2.14e+00]    []  
33000     [1.29e-04, 1.11e-04, 3.84e-07, 1.49e-01, 1.49e-01]    [1.29e-04, 1.11e-04, 3.84e-07, 1.49e-01, 1.49e-01]    []  
37000     [7.57e-05, 1.14e-05, 1.67e-08, 6.08e-02, 6.08e-02]    [7.57e-05, 1.14e-05, 1.67e-08, 6.08e-02, 6.08e-02]    []  
                                                                                                                           14000     [2.82e-11, 8.83e-04, 7.70e-17, 2.47e+03, 2.47e+03]    [2.82e-11, 8.83e-04, 7.70e-17, 2.47e+03, 2.47e+03]    []  
3000      [1.23e-10, 1.05e-03, 2.71e-23, 2.09e+03, 2.09e+03]    [1.23e-10, 1.05e-03, 2.71e-23, 2.09e+03, 2.09e+03]    []  
2000      [5.27e-08, 1.08e-03, 2.81e-16, 2.10e+03, 2.10e+03]    [5.27e-08, 1.08e-03, 2.81e-16, 2.10e+03, 2.10e+03]    []  
21000     [8.72e-03, 1.67e-04, 2.03e-04, 4.92e+00, 4.92e+00]    [8.72e-03, 1.67e-04, 2.03e-04, 4.92e+00, 4.92e+00]    []  
34000     [1.21e-04, 6.92e-05, 4.08e-07, 1.51e-01, 1.51e-01]    [1.21e-04, 6.92e-05, 4.08e-07, 1.51e-01, 1.51e-01]    []  
38000     [5.02e-05, 5.86e-06, 3.03e-08, 5.23e-02, 5.23e-02]    [5.02e-05, 5.86e-06, 3.03e-08, 5.23e-02, 5.23e-02]    []  
4000      [4.03e-05, 7.34e-05, 2.89e-08, 2.94e+00, 2.94e+00]    [4.03e-05, 7.34e-05, 2.89e-08, 2.94e+00, 2.94e+00]    []  
35000     [1.22e-04, 6.64e-05, 2.04e-06, 3.06e-01, 3.06e-01]    [1.22e-04, 6.64e-05, 2.04e-06, 3.06e-01, 3.06e-01]    []  
39000     [3.52e-05, 4.07e-06, 7.80e-09, 4.58e-02, 4.58e-02]    [3.52e-05, 4.07e-06, 7.80e-09, 4.58e-02, 4.58e-02]    []  
15000     [2.85e-10, 8.94e-04, 7.53e-18, 2.46e+03, 2.46e+03]    [2.85e-10, 8.94e-04, 7.53e-18, 2.46e+03, 2.46e+03]    []  
4000      [2.32e-10, 1.05e-03, 2.00e-23, 2.09e+03, 2.09e+03]    [2.32e-10, 1.05e-03, 2.00e-23, 2.09e+03, 2.09e+03]    []  
22000     [1.07e-02, 8.75e-05, 6.99e-06, 4.14e+00, 4.14e+00]    [1.07e-02, 8.75e-05, 6.99e-06, 4.14e+00, 4.14e+00]    []  
3000      [1.02e-09, 1.05e-03, 8.54e-17, 2.09e+03, 2.09e+03]    [1.02e-09, 1.05e-03, 8.54e-17, 2.09e+03, 2.09e+03]    []  
                                                                                                                           40000     [2.69e-05, 3.06e-06, 2.60e-08, 3.80e-02, 3.80e-02]    [2.69e-05, 3.06e-06, 2.60e-08, 3.80e-02, 3.80e-02]    []  
36000     [1.07e-04, 5.65e-05, 4.13e-07, 1.47e-01, 1.47e-01]    [1.07e-04, 5.65e-05, 4.13e-07, 1.47e-01, 1.47e-01]    []  
5000      [2.88e-04, 4.57e-04, 2.02e-23, 5.03e+00, 5.03e+00]    [2.88e-04, 4.57e-04, 2.02e-23, 5.03e+00, 5.03e+00]    []  
5000      [3.44e-02, 1.05e-03, 2.54e-25, 2.09e+03, 2.09e+03]    [3.44e-02, 1.05e-03, 2.54e-25, 2.09e+03, 2.09e+03]    []  
23000     [6.05e-03, 6.16e-05, 1.25e-05, 3.52e+00, 3.52e+00]    [6.05e-03, 6.16e-05, 1.25e-05, 3.52e+00, 3.52e+00]    []  
16000     [4.47e-10, 9.04e-04, 3.08e-18, 2.44e+03, 2.44e+03]    [4.47e-10, 9.04e-04, 3.08e-18, 2.44e+03, 2.44e+03]    []  
41000     [2.20e-04, 7.35e-06, 2.71e-07, 4.24e+00, 4.24e+00]    [2.20e-04, 7.35e-06, 2.71e-07, 4.24e+00, 4.24e+00]    []  
37000     [1.27e-04, 4.43e-05, 9.67e-07, 1.10e+00, 1.10e+00]    [1.27e-04, 4.43e-05, 9.67e-07, 1.10e+00, 1.10e+00]    []  
4000      [6.99e-11, 1.05e-03, 2.30e-17, 2.09e+03, 2.09e+03]    [6.99e-11, 1.05e-03, 2.30e-17, 2.09e+03, 2.09e+03]    []  
6000      [5.87e-04, 8.49e-04, 2.20e-22, 2.31e+01, 2.31e+01]    [5.87e-04, 8.49e-04, 2.20e-22, 2.31e+01, 2.31e+01]    []  
42000     [3.08e-04, 1.13e-04, 7.92e-08, 3.13e-01, 3.13e-01]    [3.08e-04, 1.13e-04, 7.92e-08, 3.13e-01, 3.13e-01]    []  
38000     [1.11e-04, 6.09e-05, 6.69e-06, 6.43e-01, 6.43e-01]    [1.11e-04, 6.09e-05, 6.69e-06, 6.43e-01, 6.43e-01]    []  
6000      [1.75e-08, 1.05e-03, 4.85e-24, 2.09e+03, 2.09e+03]    [1.75e-08, 1.05e-03, 4.85e-24, 2.09e+03, 2.09e+03]    []  
24000     [9.15e-03, 5.55e-05, 1.46e-03, 2.98e+00, 2.98e+00]    [9.15e-03, 5.55e-05, 1.46e-03, 2.98e+00, 2.98e+00]    []  
17000     [6.35e-10, 9.15e-04, 1.00e-18, 2.42e+03, 2.42e+03]    [6.35e-10, 9.15e-04, 1.00e-18, 2.42e+03, 2.42e+03]    []  
5000      [6.00e-11, 1.05e-03, 2.24e-17, 2.09e+03, 2.09e+03]    [6.00e-11, 1.05e-03, 2.24e-17, 2.09e+03, 2.09e+03]    []  
43000     [8.51e-05, 1.39e-05, 2.86e-07, 6.41e-01, 6.41e-01]    [8.51e-05, 1.39e-05, 2.86e-07, 6.41e-01, 6.41e-01]    []  
39000     [9.76e-05, 4.66e-05, 2.27e-07, 1.03e-01, 1.03e-01]    [9.76e-05, 4.66e-05, 2.27e-07, 1.03e-01, 1.03e-01]    []  
7000      [3.03e-04, 1.36e-04, 2.30e-16, 4.35e+00, 4.35e+00]    [3.03e-04, 1.36e-04, 2.30e-16, 4.35e+00, 4.35e+00]    []  
7000      [4.01e-06, 1.05e-03, 5.57e-23, 2.09e+03, 2.09e+03]    [4.01e-06, 1.05e-03, 5.57e-23, 2.09e+03, 2.09e+03]    []  
25000     [7.05e-03, 5.78e-05, 4.04e-04, 2.59e+00, 2.59e+00]    [7.05e-03, 5.78e-05, 4.04e-04, 2.59e+00, 2.59e+00]    []  
44000     [1.03e-04, 1.07e-05, 3.25e-07, 6.37e-02, 6.37e-02]    [1.03e-04, 1.07e-05, 3.25e-07, 6.37e-02, 6.37e-02]    []  
40000     [1.01e-04, 4.23e-05, 2.01e-07, 1.77e-01, 1.77e-01]    [1.01e-04, 4.23e-05, 2.01e-07, 1.77e-01, 1.77e-01]    []  
18000     [8.60e-10, 9.25e-04, 4.64e-19, 2.41e+03, 2.41e+03]    [8.60e-10, 9.25e-04, 4.64e-19, 2.41e+03, 2.41e+03]    []  
6000      [5.91e-11, 1.05e-03, 9.33e-18, 2.09e+03, 2.09e+03]    [5.91e-11, 1.05e-03, 9.33e-18, 2.09e+03, 2.09e+03]    []  
8000      [5.47e-03, 6.18e-04, 1.37e-17, 1.02e+03, 1.02e+03]    [5.47e-03, 6.18e-04, 1.37e-17, 1.02e+03, 1.02e+03]    []  
45000     [5.45e-05, 7.91e-06, 2.11e-08, 6.28e-02, 6.28e-02]    [5.45e-05, 7.91e-06, 2.11e-08, 6.28e-02, 6.28e-02]    []  
41000     [7.60e-05, 1.40e-04, 2.13e-07, 2.40e-01, 2.40e-01]    [7.60e-05, 1.40e-04, 2.13e-07, 2.40e-01, 2.40e-01]    []  
8000      [1.12e+00, 8.80e-04, 3.21e-31, 2.09e+03, 2.09e+03]    [1.12e+00, 8.80e-04, 3.21e-31, 2.09e+03, 2.09e+03]    []  
26000     [6.66e-03, 8.51e-05, 3.63e-05, 2.32e+00, 2.32e+00]    [6.66e-03, 8.51e-05, 3.63e-05, 2.32e+00, 2.32e+00]    []  
46000     [3.55e-05, 3.77e-06, 2.40e-08, 3.63e-02, 3.63e-02]    [3.55e-05, 3.77e-06, 2.40e-08, 3.63e-02, 3.63e-02]    []  
7000      [5.32e-11, 1.05e-03, 2.97e-18, 2.09e+03, 2.09e+03]    [5.32e-11, 1.05e-03, 2.97e-18, 2.09e+03, 2.09e+03]    []  
19000     [8.38e-10, 9.35e-04, 3.46e-20, 2.39e+03, 2.39e+03]    [8.38e-10, 9.35e-04, 3.46e-20, 2.39e+03, 2.39e+03]    []  
42000     [8.69e-05, 3.86e-05, 1.23e-07, 9.76e-02, 9.76e-02]    [8.69e-05, 3.86e-05, 1.23e-07, 9.76e-02, 9.76e-02]    []  
9000      [8.30e-04, 6.03e-04, 7.38e-24, 9.35e+02, 9.35e+02]    [8.30e-04, 6.03e-04, 7.38e-24, 9.35e+02, 9.35e+02]    []  
9000      [4.10e-08, 1.05e-03, 1.62e-30, 2.09e+03, 2.09e+03]    [4.10e-08, 1.05e-03, 1.62e-30, 2.09e+03, 2.09e+03]    []  
27000     [3.22e-03, 2.08e-04, 1.21e-05, 2.11e+00, 2.11e+00]    [3.22e-03, 2.08e-04, 1.21e-05, 2.11e+00, 2.11e+00]    []  
47000     [1.97e-05, 1.55e-06, 2.31e-09, 3.16e-02, 3.16e-02]    [1.97e-05, 1.55e-06, 2.31e-09, 3.16e-02, 3.16e-02]    []  
43000     [8.39e-05, 3.80e-05, 4.91e-07, 8.91e-02, 8.91e-02]    [8.39e-05, 3.80e-05, 4.91e-07, 8.91e-02, 8.91e-02]    []  
8000      [5.27e-11, 1.05e-03, 1.48e-18, 2.09e+03, 2.09e+03]    [5.27e-11, 1.05e-03, 1.48e-18, 2.09e+03, 2.09e+03]    []  
20000     [8.47e-10, 9.44e-04, 3.33e-20, 2.38e+03, 2.38e+03]    [8.47e-10, 9.44e-04, 3.33e-20, 2.38e+03, 2.38e+03]    []  
48000     [1.67e-05, 1.26e-06, 1.45e-07, 2.90e-02, 2.90e-02]    [1.67e-05, 1.26e-06, 1.45e-07, 2.90e-02, 2.90e-02]    []  
10000     [4.80e-05, 5.70e-04, 1.63e-17, 8.81e+02, 8.81e+02]    [4.80e-05, 5.70e-04, 1.63e-17, 8.81e+02, 8.81e+02]    []  
10000     [3.24e-06, 1.05e-03, 4.26e-32, 2.09e+03, 2.09e+03]    [3.24e-06, 1.05e-03, 4.26e-32, 2.09e+03, 2.09e+03]    []  
28000     [5.43e-03, 2.98e-04, 2.77e-04, 1.91e+00, 1.91e+00]    [5.43e-03, 2.98e-04, 2.77e-04, 1.91e+00, 1.91e+00]    []  
44000     [9.00e-05, 2.97e-05, 1.07e-07, 1.11e-01, 1.11e-01]    [9.00e-05, 2.97e-05, 1.07e-07, 1.11e-01, 1.11e-01]    []  
49000     [1.15e-05, 6.88e-07, 2.92e-09, 2.25e-02, 2.25e-02]    [1.15e-05, 6.88e-07, 2.92e-09, 2.25e-02, 2.25e-02]    []  
45000     [7.90e-05, 6.80e-05, 1.29e-07, 8.41e-02, 8.41e-02]    [7.90e-05, 6.80e-05, 1.29e-07, 8.41e-02, 8.41e-02]    []  
9000      [8.00e-11, 1.05e-03, 9.30e-19, 2.09e+03, 2.09e+03]    [8.00e-11, 1.05e-03, 9.30e-19, 2.09e+03, 2.09e+03]    []  
11000     [2.18e-04, 5.66e-04, 7.19e-18, 8.74e+02, 8.74e+02]    [2.18e-04, 5.66e-04, 7.19e-18, 8.74e+02, 8.74e+02]    []  
21000     [9.32e-10, 9.54e-04, 4.14e-21, 2.37e+03, 2.37e+03]    [9.32e-10, 9.54e-04, 4.14e-21, 2.37e+03, 2.37e+03]    []  
11000     [2.44e-08, 1.05e-03, 2.58e-32, 2.09e+03, 2.09e+03]    [2.44e-08, 1.05e-03, 2.58e-32, 2.09e+03, 2.09e+03]    []  
29000     [7.26e-03, 4.99e-04, 2.70e-05, 1.68e+00, 1.68e+00]    [7.26e-03, 4.99e-04, 2.70e-05, 1.68e+00, 1.68e+00]    []  
50000     [1.87e-05, 1.35e-06, 9.54e-08, 2.58e-02, 2.58e-02]    [1.87e-05, 1.35e-06, 9.54e-08, 2.58e-02, 2.58e-02]    []  

Best model at step 49000:
  train loss: 4.50e-02
  test loss: 4.50e-02
  test metric: []

'train' took 7221.708521 s

[I 2023-10-08 21:50:19,350] Trial 10 finished with value: 0.6647077296633317 and parameters: {'num_domain': 5414, 'num_boundary': 3624, 'resampling_period': 46620, 'lr': 0.003523041123726689}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000127 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.22e+01, 1.81e-04, 2.13e-07, 3.61e+03, 3.61e+03]    [4.22e+01, 1.81e-04, 2.13e-07, 3.61e+03, 3.61e+03]    []  
46000     [7.38e-05, 5.38e-05, 1.13e-06, 7.42e-02, 7.42e-02]    [7.38e-05, 5.38e-05, 1.13e-06, 7.42e-02, 7.42e-02]    []  
10000     [1.66e-02, 7.43e-04, 4.80e-04, 1.47e+03, 1.47e+03]    [1.66e-02, 7.43e-04, 4.80e-04, 1.47e+03, 1.47e+03]    []  
12000     [2.16e-04, 1.05e-03, 6.06e-32, 2.09e+03, 2.09e+03]    [2.16e-04, 1.05e-03, 6.06e-32, 2.09e+03, 2.09e+03]    []  
30000     [4.46e-03, 6.65e-04, 7.13e-06, 1.52e+00, 1.52e+00]    [4.46e-03, 6.65e-04, 7.13e-06, 1.52e+00, 1.52e+00]    []  
12000     [2.38e-04, 5.68e-04, 1.09e-16, 8.73e+02, 8.73e+02]    [2.38e-04, 5.68e-04, 1.09e-16, 8.73e+02, 8.73e+02]    []  
22000     [8.72e-10, 9.63e-04, 5.05e-21, 2.35e+03, 2.35e+03]    [8.72e-10, 9.63e-04, 5.05e-21, 2.35e+03, 2.35e+03]    []  
47000     [6.78e-05, 2.59e-05, 5.73e-08, 1.24e-01, 1.24e-01]    [6.78e-05, 2.59e-05, 5.73e-08, 1.24e-01, 1.24e-01]    []  
1000      [5.82e-07, 1.05e-03, 8.37e-17, 2.21e+03, 2.21e+03]    [5.82e-07, 1.05e-03, 8.37e-17, 2.21e+03, 2.21e+03]    []  
48000     [1.26e-04, 6.90e-05, 1.76e-05, 1.67e-01, 1.67e-01]    [1.26e-04, 6.90e-05, 1.76e-05, 1.67e-01, 1.67e-01]    []  
13000     [8.54e-13, 1.05e-03, 2.01e-31, 2.09e+03, 2.09e+03]    [8.54e-13, 1.05e-03, 2.01e-31, 2.09e+03, 2.09e+03]    []  
31000     [4.82e-03, 5.56e-04, 1.99e-05, 1.36e+00, 1.36e+00]    [4.82e-03, 5.56e-04, 1.99e-05, 1.36e+00, 1.36e+00]    []  
11000     [3.24e-02, 6.33e-04, 3.21e-03, 5.20e+01, 5.20e+01]    [3.24e-02, 6.33e-04, 3.21e-03, 5.20e+01, 5.20e+01]    []  
13000     [1.81e-04, 5.62e-04, 5.10e-14, 9.89e+02, 9.89e+02]    [1.81e-04, 5.62e-04, 5.10e-14, 9.89e+02, 9.89e+02]    []  
2000      [2.54e-07, 1.08e-03, 1.19e-16, 2.10e+03, 2.10e+03]    [2.54e-07, 1.08e-03, 1.19e-16, 2.10e+03, 2.10e+03]    []  
23000     [3.60e-09, 9.72e-04, 1.71e-21, 2.34e+03, 2.34e+03]    [3.60e-09, 9.72e-04, 1.71e-21, 2.34e+03, 2.34e+03]    []  
49000     [5.61e-05, 1.75e-05, 7.72e-08, 6.48e-02, 6.48e-02]    [5.61e-05, 1.75e-05, 7.72e-08, 6.48e-02, 6.48e-02]    []  
                                                                                                                           14000     [3.00e-08, 1.05e-03, 1.20e-31, 2.09e+03, 2.09e+03]    [3.00e-08, 1.05e-03, 1.20e-31, 2.09e+03, 2.09e+03]    []  
32000     [4.45e-03, 4.66e-04, 4.49e-05, 1.23e+00, 1.23e+00]    [4.45e-03, 4.66e-04, 4.49e-05, 1.23e+00, 1.23e+00]    []  
12000     [4.15e-03, 6.72e-05, 2.45e-04, 1.73e+01, 1.73e+01]    [4.15e-03, 6.72e-05, 2.45e-04, 1.73e+01, 1.73e+01]    []  
14000     [6.84e-01, 5.94e-04, 0.00e+00, 9.07e+02, 9.07e+02]    [6.84e-01, 5.94e-04, 0.00e+00, 9.07e+02, 9.07e+02]    []  
3000      [6.96e-09, 1.05e-03, 2.51e-17, 2.09e+03, 2.09e+03]    [6.96e-09, 1.05e-03, 2.51e-17, 2.09e+03, 2.09e+03]    []  
50000     [6.48e-05, 2.52e-05, 6.02e-07, 6.18e-02, 6.18e-02]    [6.48e-05, 2.52e-05, 6.02e-07, 6.18e-02, 6.18e-02]    []  

Best model at step 50000:
  train loss: 1.24e-01
  test loss: 1.24e-01
  test metric: []

'train' took 7505.615915 s

[I 2023-10-08 22:01:20,682] Trial 11 finished with value: 0.5257384435614475 and parameters: {'num_domain': 5606, 'num_boundary': 5636, 'resampling_period': 4626, 'lr': 0.0006980238610231373}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000113 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [7.38e+01, 3.25e-04, 7.92e-08, 3.60e+03, 3.60e+03]    [7.38e+01, 3.25e-04, 7.92e-08, 3.60e+03, 3.60e+03]    []  
24000     [4.61e-09, 9.80e-04, 1.19e-22, 2.33e+03, 2.33e+03]    [4.61e-09, 9.80e-04, 1.19e-22, 2.33e+03, 2.33e+03]    []  
15000     [2.47e-10, 1.05e-03, 7.53e-32, 2.09e+03, 2.09e+03]    [2.47e-10, 1.05e-03, 7.53e-32, 2.09e+03, 2.09e+03]    []  
33000     [3.02e-03, 4.07e-04, 7.08e-05, 1.09e+00, 1.09e+00]    [3.02e-03, 4.07e-04, 7.08e-05, 1.09e+00, 1.09e+00]    []  
4000      [7.58e-11, 1.05e-03, 1.52e-17, 2.09e+03, 2.09e+03]    [7.58e-11, 1.05e-03, 1.52e-17, 2.09e+03, 2.09e+03]    []  
15000     [8.12e-05, 5.84e-04, 0.00e+00, 9.08e+02, 9.08e+02]    [8.12e-05, 5.84e-04, 0.00e+00, 9.08e+02, 9.08e+02]    []  
13000     [2.44e-03, 3.73e-04, 2.30e-03, 1.29e+01, 1.29e+01]    [2.44e-03, 3.73e-04, 2.30e-03, 1.29e+01, 1.29e+01]    []  
1000      [3.07e-07, 1.00e-03, 4.39e-14, 2.29e+03, 2.29e+03]    [3.07e-07, 1.00e-03, 4.39e-14, 2.29e+03, 2.29e+03]    []  
25000     [1.47e-09, 9.89e-04, 4.37e-23, 2.31e+03, 2.31e+03]    [1.47e-09, 9.89e-04, 4.37e-23, 2.31e+03, 2.31e+03]    []  
16000     [1.56e-02, 1.05e-03, 1.23e-32, 2.09e+03, 2.09e+03]    [1.56e-02, 1.05e-03, 1.23e-32, 2.09e+03, 2.09e+03]    []  
34000     [3.51e-03, 3.67e-04, 6.49e-05, 9.85e-01, 9.85e-01]    [3.51e-03, 3.67e-04, 6.49e-05, 9.85e-01, 9.85e-01]    []  
5000      [7.75e-11, 1.05e-03, 3.67e-18, 2.09e+03, 2.09e+03]    [7.75e-11, 1.05e-03, 3.67e-18, 2.09e+03, 2.09e+03]    []  
2000      [6.47e-07, 1.09e-03, 1.63e-15, 2.12e+03, 2.12e+03]    [6.47e-07, 1.09e-03, 1.63e-15, 2.12e+03, 2.12e+03]    []  
16000     [2.25e-04, 5.77e-04, 0.00e+00, 8.94e+02, 8.94e+02]    [2.25e-04, 5.77e-04, 0.00e+00, 8.94e+02, 8.94e+02]    []  
14000     [3.54e-03, 3.17e-04, 8.63e-04, 9.86e+00, 9.86e+00]    [3.54e-03, 3.17e-04, 8.63e-04, 9.86e+00, 9.86e+00]    []  
26000     [2.63e-09, 9.97e-04, 9.55e-24, 2.30e+03, 2.30e+03]    [2.63e-09, 9.97e-04, 9.55e-24, 2.30e+03, 2.30e+03]    []  
17000     [5.15e-03, 1.05e-03, 2.71e-32, 2.09e+03, 2.09e+03]    [5.15e-03, 1.05e-03, 2.71e-32, 2.09e+03, 2.09e+03]    []  
35000     [8.53e-03, 3.21e-04, 6.74e-04, 8.94e-01, 8.94e-01]    [8.53e-03, 3.21e-04, 6.74e-04, 8.94e-01, 8.94e-01]    []  
6000      [8.01e-11, 1.05e-03, 6.71e-18, 2.09e+03, 2.09e+03]    [8.01e-11, 1.05e-03, 6.71e-18, 2.09e+03, 2.09e+03]    []  
3000      [7.21e-08, 1.06e-03, 4.95e-15, 2.10e+03, 2.10e+03]    [7.21e-08, 1.06e-03, 4.95e-15, 2.10e+03, 2.10e+03]    []  
17000     [2.45e-06, 6.54e-04, 4.00e-28, 9.50e+02, 9.50e+02]    [2.45e-06, 6.54e-04, 4.00e-28, 9.50e+02, 9.50e+02]    []  
15000     [2.56e-03, 3.44e-04, 4.09e-04, 8.08e+00, 8.08e+00]    [2.56e-03, 3.44e-04, 4.09e-04, 8.08e+00, 8.08e+00]    []  
18000     [1.02e-03, 1.05e-03, 2.05e-32, 2.09e+03, 2.09e+03]    [1.02e-03, 1.05e-03, 2.05e-32, 2.09e+03, 2.09e+03]    []  
36000     [7.43e-03, 3.25e-04, 9.90e-05, 8.08e-01, 8.08e-01]    [7.43e-03, 3.25e-04, 9.90e-05, 8.08e-01, 8.08e-01]    []  
27000     [3.44e-09, 1.00e-03, 4.22e-24, 2.29e+03, 2.29e+03]    [3.44e-09, 1.00e-03, 4.22e-24, 2.29e+03, 2.29e+03]    []  
7000      [6.25e-11, 1.05e-03, 5.66e-18, 2.09e+03, 2.09e+03]    [6.25e-11, 1.05e-03, 5.66e-18, 2.09e+03, 2.09e+03]    []  
4000      [9.62e-10, 1.05e-03, 5.64e-16, 2.09e+03, 2.09e+03]    [9.62e-10, 1.05e-03, 5.64e-16, 2.09e+03, 2.09e+03]    []  
18000     [2.43e-05, 5.73e-04, 0.00e+00, 8.96e+02, 8.96e+02]    [2.43e-05, 5.73e-04, 0.00e+00, 8.96e+02, 8.96e+02]    []  
16000     [1.80e-03, 2.83e-04, 1.59e-04, 6.75e+00, 6.75e+00]    [1.80e-03, 2.83e-04, 1.59e-04, 6.75e+00, 6.75e+00]    []  
19000     [1.91e-06, 1.05e-03, 2.45e-31, 2.09e+03, 2.09e+03]    [1.91e-06, 1.05e-03, 2.45e-31, 2.09e+03, 2.09e+03]    []  
37000     [3.62e-03, 4.05e-04, 1.03e-04, 7.38e-01, 7.38e-01]    [3.62e-03, 4.05e-04, 1.03e-04, 7.38e-01, 7.38e-01]    []  
8000      [5.95e-11, 1.05e-03, 3.16e-15, 2.09e+03, 2.09e+03]    [5.95e-11, 1.05e-03, 3.16e-15, 2.09e+03, 2.09e+03]    []  
28000     [2.58e-09, 1.01e-03, 1.68e-24, 2.28e+03, 2.28e+03]    [2.58e-09, 1.01e-03, 1.68e-24, 2.28e+03, 2.28e+03]    []  
5000      [9.41e-11, 1.05e-03, 2.18e-16, 2.09e+03, 2.09e+03]    [9.41e-11, 1.05e-03, 2.18e-16, 2.09e+03, 2.09e+03]    []  
19000     [2.47e-02, 5.88e-04, 0.00e+00, 9.01e+02, 9.01e+02]    [2.47e-02, 5.88e-04, 0.00e+00, 9.01e+02, 9.01e+02]    []  
17000     [5.64e-03, 5.82e-04, 8.02e-04, 5.46e+00, 5.46e+00]    [5.64e-03, 5.82e-04, 8.02e-04, 5.46e+00, 5.46e+00]    []  
20000     [4.13e-08, 1.05e-03, 4.10e-33, 2.09e+03, 2.09e+03]    [4.13e-08, 1.05e-03, 4.10e-33, 2.09e+03, 2.09e+03]    []  
38000     [3.12e-03, 4.57e-04, 9.51e-05, 6.77e-01, 6.77e-01]    [3.12e-03, 4.57e-04, 9.51e-05, 6.77e-01, 6.77e-01]    []  
9000      [8.73e-03, 3.01e-06, 1.22e-05, 1.61e+02, 1.61e+02]    [8.73e-03, 3.01e-06, 1.22e-05, 1.61e+02, 1.61e+02]    []  
6000      [6.94e-11, 1.05e-03, 1.82e-16, 2.09e+03, 2.09e+03]    [6.94e-11, 1.05e-03, 1.82e-16, 2.09e+03, 2.09e+03]    []  
29000     [1.40e-08, 1.02e-03, 4.35e-25, 2.27e+03, 2.27e+03]    [1.40e-08, 1.02e-03, 4.35e-25, 2.27e+03, 2.27e+03]    []  
20000     [1.17e+00, 5.49e-04, 0.00e+00, 9.14e+02, 9.14e+02]    [1.17e+00, 5.49e-04, 0.00e+00, 9.14e+02, 9.14e+02]    []  
18000     [3.96e-03, 1.20e-03, 1.14e-03, 4.26e+00, 4.26e+00]    [3.96e-03, 1.20e-03, 1.14e-03, 4.26e+00, 4.26e+00]    []  
21000     [2.76e-04, 1.05e-03, 1.71e-33, 2.09e+03, 2.09e+03]    [2.76e-04, 1.05e-03, 1.71e-33, 2.09e+03, 2.09e+03]    []  
39000     [2.56e-03, 4.93e-04, 1.09e-04, 6.21e-01, 6.21e-01]    [2.56e-03, 4.93e-04, 1.09e-04, 6.21e-01, 6.21e-01]    []  
10000     [1.70e-02, 1.21e-04, 5.84e-05, 2.56e+01, 2.56e+01]    [1.70e-02, 1.21e-04, 5.84e-05, 2.56e+01, 2.56e+01]    []  
7000      [3.22e-03, 1.07e-03, 2.86e-10, 1.81e+03, 1.81e+03]    [3.22e-03, 1.07e-03, 2.86e-10, 1.81e+03, 1.81e+03]    []  
30000     [7.94e-10, 1.03e-03, 7.14e-25, 2.25e+03, 2.25e+03]    [7.94e-10, 1.03e-03, 7.14e-25, 2.25e+03, 2.25e+03]    []  
21000     [1.70e-04, 5.65e-04, 0.00e+00, 9.04e+02, 9.04e+02]    [1.70e-04, 5.65e-04, 0.00e+00, 9.04e+02, 9.04e+02]    []  
22000     [8.54e-11, 1.05e-03, 3.79e-34, 2.09e+03, 2.09e+03]    [8.54e-11, 1.05e-03, 3.79e-34, 2.09e+03, 2.09e+03]    []  
40000     [6.65e-03, 5.51e-04, 1.37e-04, 5.77e-01, 5.77e-01]    [6.65e-03, 5.51e-04, 1.37e-04, 5.77e-01, 5.77e-01]    []  
19000     [2.22e-03, 1.28e-03, 1.44e-04, 3.27e+00, 3.27e+00]    [2.22e-03, 1.28e-03, 1.44e-04, 3.27e+00, 3.27e+00]    []  
11000     [2.64e-03, 8.33e-04, 1.34e-05, 9.90e+00, 9.90e+00]    [2.64e-03, 8.33e-04, 1.34e-05, 9.90e+00, 9.90e+00]    []  
8000      [2.96e-02, 9.36e-05, 1.07e-04, 1.60e+02, 1.60e+02]    [2.96e-02, 9.36e-05, 1.07e-04, 1.60e+02, 1.60e+02]    []  
                                                                                                                           31000     [1.80e-10, 1.03e-03, 1.08e-25, 2.24e+03, 2.24e+03]    [1.80e-10, 1.03e-03, 1.08e-25, 2.24e+03, 2.24e+03]    []  
22000     [2.66e-03, 7.93e-04, 0.00e+00, 3.44e+02, 3.44e+02]    [2.66e-03, 7.93e-04, 0.00e+00, 3.44e+02, 3.44e+02]    []  
23000     [2.57e-08, 1.05e-03, 4.36e-34, 2.09e+03, 2.09e+03]    [2.57e-08, 1.05e-03, 4.36e-34, 2.09e+03, 2.09e+03]    []  
41000     [2.46e-03, 5.56e-04, 8.41e-05, 5.91e-01, 5.91e-01]    [2.46e-03, 5.56e-04, 8.41e-05, 5.91e-01, 5.91e-01]    []  
20000     [1.61e-03, 1.24e-03, 8.41e-05, 2.55e+00, 2.55e+00]    [1.61e-03, 1.24e-03, 8.41e-05, 2.55e+00, 2.55e+00]    []  
12000     [1.27e-03, 9.56e-04, 6.14e-06, 7.86e+00, 7.86e+00]    [1.27e-03, 9.56e-04, 6.14e-06, 7.86e+00, 7.86e+00]    []  
9000      [7.83e-03, 6.49e-04, 1.40e-05, 3.18e+01, 3.18e+01]    [7.83e-03, 6.49e-04, 1.40e-05, 3.18e+01, 3.18e+01]    []  
                                                                                                                           23000     [3.55e-04, 7.08e-04, 0.00e+00, 9.34e+02, 9.34e+02]    [3.55e-04, 7.08e-04, 0.00e+00, 9.34e+02, 9.34e+02]    []  
32000     [1.57e-08, 1.04e-03, 6.19e-26, 2.23e+03, 2.23e+03]    [1.57e-08, 1.04e-03, 6.19e-26, 2.23e+03, 2.23e+03]    []  
24000     [2.49e-02, 1.05e-03, 1.93e-34, 2.09e+03, 2.09e+03]    [2.49e-02, 1.05e-03, 1.93e-34, 2.09e+03, 2.09e+03]    []  
42000     [3.12e-03, 5.61e-04, 6.94e-05, 4.99e-01, 4.99e-01]    [3.12e-03, 5.61e-04, 6.94e-05, 4.99e-01, 4.99e-01]    []  
13000     [9.29e-04, 1.10e-03, 7.23e-06, 6.90e+00, 6.90e+00]    [9.29e-04, 1.10e-03, 7.23e-06, 6.90e+00, 6.90e+00]    []  
10000     [4.66e-03, 1.21e-03, 1.22e-05, 1.04e+01, 1.04e+01]    [4.66e-03, 1.21e-03, 1.22e-05, 1.04e+01, 1.04e+01]    []  
21000     [2.48e-03, 1.04e-03, 2.82e-04, 2.01e+00, 2.01e+00]    [2.48e-03, 1.04e-03, 2.82e-04, 2.01e+00, 2.01e+00]    []  
                                                                                                                                                                                                                                                      25000     [1.51e-06, 1.05e-03, 8.07e-35, 2.09e+03, 2.09e+03]    [1.51e-06, 1.05e-03, 8.07e-35, 2.09e+03, 2.09e+03]    []  
43000     [2.20e-03, 5.61e-04, 5.80e-05, 4.66e-01, 4.66e-01]    [2.20e-03, 5.61e-04, 5.80e-05, 4.66e-01, 4.66e-01]    []  
24000     [6.30e-05, 6.55e-04, 0.00e+00, 1.01e+03, 1.01e+03]    [6.30e-05, 6.55e-04, 0.00e+00, 1.01e+03, 1.01e+03]    []  
33000     [7.43e-10, 1.04e-03, 4.73e-27, 2.22e+03, 2.22e+03]    [7.43e-10, 1.04e-03, 4.73e-27, 2.22e+03, 2.22e+03]    []  
14000     [7.90e-04, 1.06e-03, 4.46e-06, 5.63e+00, 5.63e+00]    [7.90e-04, 1.06e-03, 4.46e-06, 5.63e+00, 5.63e+00]    []  
11000     [2.16e-03, 6.18e-04, 3.13e-06, 7.56e+00, 7.56e+00]    [2.16e-03, 6.18e-04, 3.13e-06, 7.56e+00, 7.56e+00]    []  
22000     [6.33e-03, 1.03e-03, 2.18e-04, 1.61e+00, 1.61e+00]    [6.33e-03, 1.03e-03, 2.18e-04, 1.61e+00, 1.61e+00]    []  
26000     [1.13e-05, 1.05e-03, 3.31e-35, 2.09e+03, 2.09e+03]    [1.13e-05, 1.05e-03, 3.31e-35, 2.09e+03, 2.09e+03]    []  
44000     [2.32e-03, 5.31e-04, 1.16e-04, 4.39e-01, 4.39e-01]    [2.32e-03, 5.31e-04, 1.16e-04, 4.39e-01, 4.39e-01]    []  
25000     [9.21e-03, 6.11e-04, 0.00e+00, 9.94e+02, 9.94e+02]    [9.21e-03, 6.11e-04, 0.00e+00, 9.94e+02, 9.94e+02]    []  
15000     [8.60e-04, 7.83e-04, 3.43e-06, 4.72e+00, 4.72e+00]    [8.60e-04, 7.83e-04, 3.43e-06, 4.72e+00, 4.72e+00]    []  
12000     [2.00e-03, 3.27e-04, 3.98e-05, 6.35e+00, 6.35e+00]    [2.00e-03, 3.27e-04, 3.98e-05, 6.35e+00, 6.35e+00]    []  
34000     [3.39e-09, 1.05e-03, 4.90e-27, 2.21e+03, 2.21e+03]    [3.39e-09, 1.05e-03, 4.90e-27, 2.21e+03, 2.21e+03]    []  
23000     [1.76e-03, 1.01e-03, 2.83e-04, 1.32e+00, 1.32e+00]    [1.76e-03, 1.01e-03, 2.83e-04, 1.32e+00, 1.32e+00]    []  
                                                                                                                           45000     [3.92e-03, 5.02e-04, 1.41e-03, 4.19e-01, 4.19e-01]    [3.92e-03, 5.02e-04, 1.41e-03, 4.19e-01, 4.19e-01]    []  
27000     [3.27e-02, 1.04e-03, 1.26e-35, 2.09e+03, 2.09e+03]    [3.27e-02, 1.04e-03, 1.26e-35, 2.09e+03, 2.09e+03]    []  
16000     [8.52e-04, 6.91e-04, 2.90e-06, 3.91e+00, 3.91e+00]    [8.52e-04, 6.91e-04, 2.90e-06, 3.91e+00, 3.91e+00]    []  
26000     [3.50e-05, 6.59e-04, 8.85e-16, 1.00e+03, 1.00e+03]    [3.50e-05, 6.59e-04, 8.85e-16, 1.00e+03, 1.00e+03]    []  
13000     [2.11e-03, 2.75e-04, 5.34e-06, 4.93e+00, 4.93e+00]    [2.11e-03, 2.75e-04, 5.34e-06, 4.93e+00, 4.93e+00]    []  
35000     [1.58e-09, 1.06e-03, 1.47e-27, 2.21e+03, 2.21e+03]    [1.58e-09, 1.06e-03, 1.47e-27, 2.21e+03, 2.21e+03]    []  
24000     [1.75e-03, 9.62e-04, 1.86e-04, 1.18e+00, 1.18e+00]    [1.75e-03, 9.62e-04, 1.86e-04, 1.18e+00, 1.18e+00]    []  
46000     [3.13e-03, 5.14e-04, 4.28e-05, 4.12e-01, 4.12e-01]    [3.13e-03, 5.14e-04, 4.28e-05, 4.12e-01, 4.12e-01]    []  
28000     [5.49e-02, 1.05e-03, 7.29e-36, 2.09e+03, 2.09e+03]    [5.49e-02, 1.05e-03, 7.29e-36, 2.09e+03, 2.09e+03]    []  
17000     [7.45e-04, 1.07e-03, 2.48e-06, 3.29e+00, 3.29e+00]    [7.45e-04, 1.07e-03, 2.48e-06, 3.29e+00, 3.29e+00]    []  
14000     [1.23e-02, 5.34e-04, 1.11e-06, 3.65e+00, 3.65e+00]    [1.23e-02, 5.34e-04, 1.11e-06, 3.65e+00, 3.65e+00]    []  
27000     [2.20e-05, 5.70e-04, 0.00e+00, 1.14e+03, 1.14e+03]    [2.20e-05, 5.70e-04, 0.00e+00, 1.14e+03, 1.14e+03]    []  
36000     [9.87e-10, 1.06e-03, 1.91e-28, 2.20e+03, 2.20e+03]    [9.87e-10, 1.06e-03, 1.91e-28, 2.20e+03, 2.20e+03]    []  
25000     [1.79e-03, 8.59e-04, 9.73e-05, 1.01e+00, 1.01e+00]    [1.79e-03, 8.59e-04, 9.73e-05, 1.01e+00, 1.01e+00]    []  
47000     [2.64e-03, 5.08e-04, 3.45e-05, 3.74e-01, 3.74e-01]    [2.64e-03, 5.08e-04, 3.45e-05, 3.74e-01, 3.74e-01]    []  
29000     [1.12e-04, 1.05e-03, 1.74e-36, 2.09e+03, 2.09e+03]    [1.12e-04, 1.05e-03, 1.74e-36, 2.09e+03, 2.09e+03]    []  
18000     [4.90e-04, 1.19e-03, 7.39e-06, 3.04e+00, 3.04e+00]    [4.90e-04, 1.19e-03, 7.39e-06, 3.04e+00, 3.04e+00]    []  
15000     [6.39e-03, 1.13e-03, 2.43e-06, 2.90e+00, 2.90e+00]    [6.39e-03, 1.13e-03, 2.43e-06, 2.90e+00, 2.90e+00]    []  
28000     [4.80e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [4.80e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
                                                                                                                           26000     [1.58e-03, 7.52e-04, 7.89e-05, 9.07e-01, 9.07e-01]    [1.58e-03, 7.52e-04, 7.89e-05, 9.07e-01, 9.07e-01]    []  
37000     [6.32e-09, 1.06e-03, 8.84e-28, 2.19e+03, 2.19e+03]    [6.32e-09, 1.06e-03, 8.84e-28, 2.19e+03, 2.19e+03]    []  
48000     [7.92e-03, 5.01e-04, 3.52e-05, 3.62e-01, 3.62e-01]    [7.92e-03, 5.01e-04, 3.52e-05, 3.62e-01, 3.62e-01]    []  
30000     [1.69e-07, 1.05e-03, 8.92e-37, 2.09e+03, 2.09e+03]    [1.69e-07, 1.05e-03, 8.92e-37, 2.09e+03, 2.09e+03]    []  
19000     [7.26e-04, 9.27e-04, 5.25e-06, 2.54e+00, 2.54e+00]    [7.26e-04, 9.27e-04, 5.25e-06, 2.54e+00, 2.54e+00]    []  
16000     [2.38e-03, 1.01e-03, 1.61e-06, 2.20e+00, 2.20e+00]    [2.38e-03, 1.01e-03, 1.61e-06, 2.20e+00, 2.20e+00]    []  
29000     [7.71e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [7.71e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
27000     [1.66e-03, 7.06e-04, 2.07e-04, 8.54e-01, 8.54e-01]    [1.66e-03, 7.06e-04, 2.07e-04, 8.54e-01, 8.54e-01]    []  
24000     [2.79e-02, 1.04e-03, 4.28e-04, 3.43e-01, 3.43e-01]    [5.52e-03, 5.05e-04, 3.00e-04, 3.43e-01, 3.43e-01]    []  
31000     [3.32e-06, 1.05e-03, 4.39e-37, 2.09e+03, 2.09e+03]    [3.32e-06, 1.05e-03, 4.39e-37, 2.09e+03, 2.09e+03]    []  
38000     [1.56e-10, 1.07e-03, 3.41e-29, 2.18e+03, 2.18e+03]    [1.56e-10, 1.07e-03, 3.41e-29, 2.18e+03, 2.18e+03]    []  
20000     [7.28e-04, 9.47e-04, 6.23e-06, 2.25e+00, 2.25e+00]    [7.28e-04, 9.47e-04, 6.23e-06, 2.25e+00, 2.25e+00]    []  
17000     [1.97e-03, 9.11e-04, 5.25e-06, 1.93e+00, 1.93e+00]    [1.97e-03, 9.11e-04, 5.25e-06, 1.93e+00, 1.93e+00]    []  
30000     [7.71e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [7.71e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
50000     [4.57e-03, 4.75e-04, 8.42e-04, 3.27e-01, 3.27e-01]    [4.57e-03, 4.75e-04, 8.42e-04, 3.27e-01, 3.27e-01]    []  

Best model at step 50000:
  train loss: 6.59e-01
  test loss: 6.59e-01
  test metric: []

'train' took 10612.767851 s

[I 2023-10-08 23:03:50,526] Trial 12 finished with value: 0.7717641741096306 and parameters: {'num_domain': 63604, 'num_boundary': 1818, 'resampling_period': 36185, 'lr': 6.4408430384347e-05}. Best is trial 0 with value: 0.4913720930752669.
32000     [7.57e-01, 9.28e-04, 1.17e-37, 2.09e+03, 2.09e+03]    [7.57e-01, 9.28e-04, 1.17e-37, 2.09e+03, 2.09e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000112 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.79e+01, 2.71e-05, 6.78e-09, 3.61e+03, 3.61e+03]    [3.79e+01, 2.71e-05, 6.78e-09, 3.61e+03, 3.61e+03]    []  
28000     [1.59e-03, 6.87e-04, 8.44e-05, 7.48e-01, 7.48e-01]    [1.59e-03, 6.87e-04, 8.44e-05, 7.48e-01, 7.48e-01]    []  
21000     [7.63e-04, 8.77e-04, 8.49e-06, 1.99e+00, 1.99e+00]    [7.63e-04, 8.77e-04, 8.49e-06, 1.99e+00, 1.99e+00]    []  
18000     [1.81e-03, 1.03e-03, 1.72e-06, 1.45e+00, 1.45e+00]    [1.81e-03, 1.03e-03, 1.72e-06, 1.45e+00, 1.45e+00]    []  
39000     [4.72e-10, 1.07e-03, 1.97e-29, 2.17e+03, 2.17e+03]    [4.72e-10, 1.07e-03, 1.97e-29, 2.17e+03, 2.17e+03]    []  
31000     [6.92e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [6.92e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
                                                                                                                           33000     [2.47e-03, 1.05e-03, 5.05e-38, 2.09e+03, 2.09e+03]    [2.47e-03, 1.05e-03, 5.05e-38, 2.09e+03, 2.09e+03]    []  
1000      [5.10e-07, 1.08e-03, 5.40e-15, 2.15e+03, 2.15e+03]    [5.10e-07, 1.08e-03, 5.40e-15, 2.15e+03, 2.15e+03]    []  
22000     [6.88e-04, 7.99e-04, 7.45e-06, 1.70e+00, 1.70e+00]    [6.88e-04, 7.99e-04, 7.45e-06, 1.70e+00, 1.70e+00]    []  
19000     [1.37e-03, 9.88e-04, 2.51e-06, 1.21e+00, 1.21e+00]    [1.37e-03, 9.88e-04, 2.51e-06, 1.21e+00, 1.21e+00]    []  
29000     [1.15e-03, 6.62e-04, 7.82e-05, 6.66e-01, 6.66e-01]    [1.15e-03, 6.62e-04, 7.82e-05, 6.66e-01, 6.66e-01]    []  
40000     [3.23e-09, 1.08e-03, 2.17e-30, 2.17e+03, 2.17e+03]    [3.23e-09, 1.08e-03, 2.17e-30, 2.17e+03, 2.17e+03]    []  
32000     [6.17e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [6.17e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
2000      [3.07e-08, 1.06e-03, 2.71e-16, 2.10e+03, 2.10e+03]    [3.07e-08, 1.06e-03, 2.71e-16, 2.10e+03, 2.10e+03]    []  
34000     [2.46e-07, 1.05e-03, 5.08e-38, 2.09e+03, 2.09e+03]    [2.46e-07, 1.05e-03, 5.08e-38, 2.09e+03, 2.09e+03]    []  
20000     [1.42e-03, 9.96e-04, 3.29e-06, 1.04e+00, 1.04e+00]    [1.42e-03, 9.96e-04, 3.29e-06, 1.04e+00, 1.04e+00]    []  
23000     [5.84e-04, 8.17e-04, 7.38e-06, 1.48e+00, 1.48e+00]    [5.84e-04, 8.17e-04, 7.38e-06, 1.48e+00, 1.48e+00]    []  
30000     [1.42e-03, 6.40e-04, 1.10e-04, 5.98e-01, 5.98e-01]    [1.42e-03, 6.40e-04, 1.10e-04, 5.98e-01, 5.98e-01]    []  
41000     [4.43e-09, 1.08e-03, 4.85e-30, 2.16e+03, 2.16e+03]    [4.43e-09, 1.08e-03, 4.85e-30, 2.16e+03, 2.16e+03]    []  
33000     [1.37e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.37e-11, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
3000      [2.28e-10, 1.05e-03, 1.96e-16, 2.09e+03, 2.09e+03]    [2.28e-10, 1.05e-03, 1.96e-16, 2.09e+03, 2.09e+03]    []  
35000     [3.55e-02, 1.04e-03, 1.21e-38, 2.09e+03, 2.09e+03]    [3.55e-02, 1.04e-03, 1.21e-38, 2.09e+03, 2.09e+03]    []  
21000     [1.42e-03, 9.09e-04, 7.57e-06, 9.98e-01, 9.98e-01]    [1.42e-03, 9.09e-04, 7.57e-06, 9.98e-01, 9.98e-01]    []  
24000     [4.67e-04, 8.26e-04, 1.41e-05, 1.31e+00, 1.31e+00]    [4.67e-04, 8.26e-04, 1.41e-05, 1.31e+00, 1.31e+00]    []  
                                                                                                                           31000     [1.90e-03, 6.20e-04, 1.14e-04, 5.68e-01, 5.68e-01]    [1.90e-03, 6.20e-04, 1.14e-04, 5.68e-01, 5.68e-01]    []  
42000     [7.47e-09, 1.08e-03, 3.58e-31, 2.15e+03, 2.15e+03]    [7.47e-09, 1.08e-03, 3.58e-31, 2.15e+03, 2.15e+03]    []  
34000     [4.43e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [4.43e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
4000      [8.09e-11, 1.05e-03, 4.31e-18, 2.09e+03, 2.09e+03]    [8.09e-11, 1.05e-03, 4.31e-18, 2.09e+03, 2.09e+03]    []  
36000     [4.54e-03, 1.05e-03, 2.57e-39, 2.09e+03, 2.09e+03]    [4.54e-03, 1.05e-03, 2.57e-39, 2.09e+03, 2.09e+03]    []  
22000     [9.44e-04, 8.36e-04, 1.27e-05, 8.12e-01, 8.12e-01]    [9.44e-04, 8.36e-04, 1.27e-05, 8.12e-01, 8.12e-01]    []  
25000     [4.77e-04, 7.51e-04, 8.67e-06, 1.16e+00, 1.16e+00]    [4.77e-04, 7.51e-04, 8.67e-06, 1.16e+00, 1.16e+00]    []  
32000     [1.16e-03, 6.03e-04, 8.38e-05, 4.59e-01, 4.59e-01]    [1.16e-03, 6.03e-04, 8.38e-05, 4.59e-01, 4.59e-01]    []  
5000      [7.01e-11, 1.05e-03, 5.49e-17, 2.09e+03, 2.09e+03]    [7.01e-11, 1.05e-03, 5.49e-17, 2.09e+03, 2.09e+03]    []  
35000     [4.16e+00, 5.14e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [4.16e+00, 5.14e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  
43000     [4.87e-09, 1.08e-03, 1.29e-31, 2.15e+03, 2.15e+03]    [4.87e-09, 1.08e-03, 1.29e-31, 2.15e+03, 2.15e+03]    []  
37000     [5.27e-08, 1.05e-03, 8.66e-39, 2.09e+03, 2.09e+03]    [5.27e-08, 1.05e-03, 8.66e-39, 2.09e+03, 2.09e+03]    []  
23000     [7.02e-04, 5.00e-04, 1.23e-05, 7.30e-01, 7.30e-01]    [7.02e-04, 5.00e-04, 1.23e-05, 7.30e-01, 7.30e-01]    []  
26000     [4.15e-04, 6.67e-04, 1.34e-05, 1.04e+00, 1.04e+00]    [4.15e-04, 6.67e-04, 1.34e-05, 1.04e+00, 1.04e+00]    []  
                                                                                                                                                                                                                                                      33000     [1.09e-03, 5.97e-04, 1.10e-04, 4.90e-01, 4.90e-01]    [1.09e-03, 5.97e-04, 1.10e-04, 4.90e-01, 4.90e-01]    []  
6000      [1.11e-10, 1.05e-03, 3.77e-16, 2.09e+03, 2.09e+03]    [1.11e-10, 1.05e-03, 3.77e-16, 2.09e+03, 2.09e+03]    []  
36000     [1.33e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.33e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
24000     [6.41e-04, 3.81e-04, 9.22e-06, 6.61e-01, 6.61e-01]    [6.41e-04, 3.81e-04, 9.22e-06, 6.61e-01, 6.61e-01]    []  
38000     [2.96e-02, 1.05e-03, 1.19e-39, 2.09e+03, 2.09e+03]    [2.96e-02, 1.05e-03, 1.19e-39, 2.09e+03, 2.09e+03]    []  
27000     [3.81e-04, 5.91e-04, 1.03e-05, 9.50e-01, 9.50e-01]    [3.81e-04, 5.91e-04, 1.03e-05, 9.50e-01, 9.50e-01]    []  
44000     [9.87e-10, 1.08e-03, 4.71e-33, 2.14e+03, 2.14e+03]    [9.87e-10, 1.08e-03, 4.71e-33, 2.14e+03, 2.14e+03]    []  
7000      [1.39e-02, 8.56e-05, 2.54e-05, 1.01e+02, 1.01e+02]    [1.39e-02, 8.56e-05, 2.54e-05, 1.01e+02, 1.01e+02]    []  
34000     [1.08e-03, 5.74e-04, 1.01e-04, 3.89e-01, 3.89e-01]    [1.08e-03, 5.74e-04, 1.01e-04, 3.89e-01, 3.89e-01]    []  
25000     [8.34e-04, 3.26e-04, 6.73e-06, 7.99e-01, 7.99e-01]    [8.34e-04, 3.26e-04, 6.73e-06, 7.99e-01, 7.99e-01]    []  
39000     [1.73e-11, 1.05e-03, 1.00e-39, 2.09e+03, 2.09e+03]    [1.73e-11, 1.05e-03, 1.00e-39, 2.09e+03, 2.09e+03]    []  
28000     [5.31e-04, 5.39e-04, 6.46e-06, 8.60e-01, 8.60e-01]    [5.31e-04, 5.39e-04, 6.46e-06, 8.60e-01, 8.60e-01]    []  
37000     [3.33e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.33e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
45000     [5.26e-09, 1.09e-03, 2.31e-33, 2.14e+03, 2.14e+03]    [5.26e-09, 1.09e-03, 2.31e-33, 2.14e+03, 2.14e+03]    []  
                                                                                                                           8000      [3.80e-03, 9.34e-04, 6.28e-06, 1.38e+01, 1.38e+01]    [3.80e-03, 9.34e-04, 6.28e-06, 1.38e+01, 1.38e+01]    []  
35000     [1.23e-03, 5.60e-04, 1.08e-04, 3.98e-01, 3.98e-01]    [1.23e-03, 5.60e-04, 1.08e-04, 3.98e-01, 3.98e-01]    []  
26000     [6.15e-04, 2.59e-04, 5.30e-06, 5.54e-01, 5.54e-01]    [6.15e-04, 2.59e-04, 5.30e-06, 5.54e-01, 5.54e-01]    []  
40000     [4.68e-06, 1.05e-03, 4.57e-41, 2.09e+03, 2.09e+03]    [4.68e-06, 1.05e-03, 4.57e-41, 2.09e+03, 2.09e+03]    []  
29000     [3.79e-04, 5.05e-04, 9.28e-06, 7.89e-01, 7.89e-01]    [3.79e-04, 5.05e-04, 9.28e-06, 7.89e-01, 7.89e-01]    []  
38000     [6.00e-10, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [6.00e-10, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
46000     [4.24e-09, 1.09e-03, 0.00e+00, 2.13e+03, 2.13e+03]    [4.24e-09, 1.09e-03, 0.00e+00, 2.13e+03, 2.13e+03]    []  
9000      [1.56e-03, 9.72e-04, 7.03e-06, 9.80e+00, 9.80e+00]    [1.56e-03, 9.72e-04, 7.03e-06, 9.80e+00, 9.80e+00]    []  
27000     [6.36e-04, 2.49e-04, 5.15e-06, 5.37e-01, 5.37e-01]    [6.36e-04, 2.49e-04, 5.15e-06, 5.37e-01, 5.37e-01]    []  
30000     [3.80e-04, 4.86e-04, 6.36e-06, 6.76e-01, 6.76e-01]    [3.80e-04, 4.86e-04, 6.36e-06, 6.76e-01, 6.76e-01]    []  
41000     [2.04e-03, 1.05e-03, 1.85e-41, 2.09e+03, 2.09e+03]    [2.04e-03, 1.05e-03, 1.85e-41, 2.09e+03, 2.09e+03]    []  
36000     [1.59e-03, 5.50e-04, 1.20e-04, 3.86e-01, 3.86e-01]    [1.59e-03, 5.50e-04, 1.20e-04, 3.86e-01, 3.86e-01]    []  
39000     [2.80e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.80e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
47000     [3.95e-09, 1.09e-03, 0.00e+00, 2.13e+03, 2.13e+03]    [3.95e-09, 1.09e-03, 0.00e+00, 2.13e+03, 2.13e+03]    []  
10000     [7.97e-04, 9.41e-04, 4.76e-06, 8.73e+00, 8.73e+00]    [7.97e-04, 9.41e-04, 4.76e-06, 8.73e+00, 8.73e+00]    []  
28000     [4.45e-04, 2.22e-04, 4.14e-06, 4.81e-01, 4.81e-01]    [4.45e-04, 2.22e-04, 4.14e-06, 4.81e-01, 4.81e-01]    []  
31000     [3.56e-04, 4.90e-04, 9.12e-06, 6.10e-01, 6.10e-01]    [3.56e-04, 4.90e-04, 9.12e-06, 6.10e-01, 6.10e-01]    []  
42000     [5.57e-08, 1.05e-03, 1.61e-39, 2.09e+03, 2.09e+03]    [5.57e-08, 1.05e-03, 1.61e-39, 2.09e+03, 2.09e+03]    []  
37000     [8.66e-04, 5.31e-04, 1.35e-04, 3.06e-01, 3.06e-01]    [8.66e-04, 5.31e-04, 1.35e-04, 3.06e-01, 3.06e-01]    []  
40000     [6.63e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [6.63e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
11000     [2.49e-04, 1.99e-04, 1.76e-06, 7.86e+00, 7.86e+00]    [2.49e-04, 1.99e-04, 1.76e-06, 7.86e+00, 7.86e+00]    []  
48000     [2.44e-09, 1.09e-03, 0.00e+00, 2.12e+03, 2.12e+03]    [2.44e-09, 1.09e-03, 0.00e+00, 2.12e+03, 2.12e+03]    []  
29000     [3.91e-04, 2.15e-04, 4.15e-06, 4.42e-01, 4.42e-01]    [3.91e-04, 2.15e-04, 4.15e-06, 4.42e-01, 4.42e-01]    []  
32000     [3.47e-04, 4.91e-04, 6.63e-06, 5.66e-01, 5.66e-01]    [3.47e-04, 4.91e-04, 6.63e-06, 5.66e-01, 5.66e-01]    []  
43000     [1.27e-03, 1.05e-03, 2.50e-39, 2.09e+03, 2.09e+03]    [1.27e-03, 1.05e-03, 2.50e-39, 2.09e+03, 2.09e+03]    []  
38000     [1.03e-03, 5.21e-04, 2.36e-04, 3.94e-01, 3.94e-01]    [1.03e-03, 5.21e-04, 2.36e-04, 3.94e-01, 3.94e-01]    []  

Best model at step 48000:
  train loss: 1.03e-01
  test loss: 1.03e-01
  test metric: []

'train' took 21684.724345 s

[I 2023-10-08 23:43:21,660] Trial 8 finished with value: 0.6312587190952846 and parameters: {'num_domain': 44540, 'num_boundary': 8682, 'resampling_period': 47185, 'lr': 0.0028995505499266063}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000209 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [7.00e+01, 2.25e-04, 2.83e-07, 3.62e+03, 3.62e+03]    [7.00e+01, 2.25e-04, 2.83e-07, 3.62e+03, 3.62e+03]    []  
                                                                                                                                                                                                                                                      32000     [9.98e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.98e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          33000     [3.85e-01, 9.83e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [3.85e-01, 9.83e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  
1000      [2.29e-06, 1.07e-03, 2.00e-16, 2.17e+03, 2.17e+03]    [2.29e-06, 1.07e-03, 2.00e-16, 2.17e+03, 2.17e+03]    []  
43000     [3.69e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.69e-07, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
32000     [6.60e-04, 2.52e-04, 2.52e-06, 3.67e-01, 3.67e-01]    [6.60e-04, 2.52e-04, 2.52e-06, 3.67e-01, 3.67e-01]    []  
35000     [2.83e-04, 4.21e-04, 1.17e-05, 4.63e-01, 4.63e-01]    [2.83e-04, 4.21e-04, 1.17e-05, 4.63e-01, 4.63e-01]    []  
46000     [3.23e-07, 1.05e-03, 5.28e-41, 2.09e+03, 2.09e+03]    [3.23e-07, 1.05e-03, 5.28e-41, 2.09e+03, 2.09e+03]    []  
1000      [2.72e-07, 1.09e-03, 7.81e-15, 2.13e+03, 2.13e+03]    [2.72e-07, 1.09e-03, 7.81e-15, 2.13e+03, 2.13e+03]    []  
15000     [9.40e-04, 4.30e-04, 2.46e-06, 5.12e+00, 5.12e+00]    [9.40e-04, 4.30e-04, 2.46e-06, 5.12e+00, 5.12e+00]    []  
41000     [7.26e-04, 4.87e-04, 9.39e-05, 2.52e-01, 2.52e-01]    [7.26e-04, 4.87e-04, 9.39e-05, 2.52e-01, 2.52e-01]    []  
44000     [8.69e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [8.69e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
33000     [3.50e-04, 2.42e-04, 2.40e-06, 3.54e-01, 3.54e-01]    [3.50e-04, 2.42e-04, 2.40e-06, 3.54e-01, 3.54e-01]    []  
36000     [2.93e-04, 4.03e-04, 7.64e-06, 4.08e-01, 4.08e-01]    [2.93e-04, 4.03e-04, 7.64e-06, 4.08e-01, 4.08e-01]    []  
47000     [1.14e-05, 1.05e-03, 1.15e-40, 2.09e+03, 2.09e+03]    [1.14e-05, 1.05e-03, 1.15e-40, 2.09e+03, 2.09e+03]    []  
2000      [6.32e-09, 1.06e-03, 3.51e-19, 2.09e+03, 2.09e+03]    [6.32e-09, 1.06e-03, 3.51e-19, 2.09e+03, 2.09e+03]    []  
16000     [1.45e-03, 6.65e-04, 6.88e-07, 4.54e+00, 4.54e+00]    [1.45e-03, 6.65e-04, 6.88e-07, 4.54e+00, 4.54e+00]    []  
45000     [1.07e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.07e-06, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
42000     [2.27e-03, 4.69e-04, 1.45e-04, 2.65e-01, 2.65e-01]    [2.27e-03, 4.69e-04, 1.45e-04, 2.65e-01, 2.65e-01]    []  
34000     [3.37e-04, 2.33e-04, 2.16e-06, 3.29e-01, 3.29e-01]    [3.37e-04, 2.33e-04, 2.16e-06, 3.29e-01, 3.29e-01]    []  
37000     [2.78e-04, 3.82e-04, 7.32e-06, 3.81e-01, 3.81e-01]    [2.78e-04, 3.82e-04, 7.32e-06, 3.81e-01, 3.81e-01]    []  
48000     [6.17e-11, 1.05e-03, 4.22e-40, 2.09e+03, 2.09e+03]    [6.17e-11, 1.05e-03, 4.22e-40, 2.09e+03, 2.09e+03]    []  
3000      [5.10e-11, 1.05e-03, 4.18e-18, 2.09e+03, 2.09e+03]    [5.10e-11, 1.05e-03, 4.18e-18, 2.09e+03, 2.09e+03]    []  
17000     [1.37e-03, 9.30e-04, 4.13e-07, 3.93e+00, 3.93e+00]    [1.37e-03, 9.30e-04, 4.13e-07, 3.93e+00, 3.93e+00]    []  
                                                                                                                           46000     [4.43e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [4.43e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
35000     [4.81e-04, 2.34e-04, 3.83e-06, 5.68e-01, 5.68e-01]    [4.81e-04, 2.34e-04, 3.83e-06, 5.68e-01, 5.68e-01]    []  
43000     [9.88e-04, 4.65e-04, 8.58e-05, 2.97e-01, 2.97e-01]    [9.88e-04, 4.65e-04, 8.58e-05, 2.97e-01, 2.97e-01]    []  
38000     [2.94e-04, 3.67e-04, 7.05e-06, 3.84e-01, 3.84e-01]    [2.94e-04, 3.67e-04, 7.05e-06, 3.84e-01, 3.84e-01]    []  
49000     [1.85e-06, 1.05e-03, 3.12e-40, 2.09e+03, 2.09e+03]    [1.85e-06, 1.05e-03, 3.12e-40, 2.09e+03, 2.09e+03]    []  
4000      [7.51e-11, 1.05e-03, 5.09e-18, 2.09e+03, 2.09e+03]    [7.51e-11, 1.05e-03, 5.09e-18, 2.09e+03, 2.09e+03]    []  
18000     [1.88e-03, 8.25e-04, 2.76e-06, 3.28e+00, 3.28e+00]    [1.88e-03, 8.25e-04, 2.76e-06, 3.28e+00, 3.28e+00]    []  
36000     [4.03e-04, 2.72e-04, 1.66e-06, 2.98e-01, 2.98e-01]    [4.03e-04, 2.72e-04, 1.66e-06, 2.98e-01, 2.98e-01]    []  
47000     [9.78e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.78e-09, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
39000     [3.51e-04, 3.53e-04, 8.34e-06, 3.68e-01, 3.68e-01]    [3.51e-04, 3.53e-04, 8.34e-06, 3.68e-01, 3.68e-01]    []  
44000     [6.14e-04, 4.63e-04, 1.51e-04, 2.37e-01, 2.37e-01]    [6.14e-04, 4.63e-04, 1.51e-04, 2.37e-01, 2.37e-01]    []  
50000     [7.39e-09, 1.05e-03, 3.26e-40, 2.09e+03, 2.09e+03]    [7.39e-09, 1.05e-03, 3.26e-40, 2.09e+03, 2.09e+03]    []  

Best model at step 1000:
  train loss: 4.19e+03
  test loss: 4.19e+03
  test metric: []

'train' took 10626.258362 s

[I 2023-10-09 00:07:37,204] Trial 17 finished with value: 45.66239824271556 and parameters: {'num_domain': 66399, 'num_boundary': 913, 'resampling_period': 2197, 'lr': 0.01453941195588142}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000103 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [2.38e+01, 8.90e-05, 8.50e-08, 3.61e+03, 3.61e+03]    [2.38e+01, 8.90e-05, 8.50e-08, 3.61e+03, 3.61e+03]    []  
5000      [7.14e-11, 1.05e-03, 1.44e-17, 2.09e+03, 2.09e+03]    [7.14e-11, 1.05e-03, 1.44e-17, 2.09e+03, 2.09e+03]    []  
19000     [1.34e-03, 7.85e-04, 4.11e-06, 2.81e+00, 2.81e+00]    [1.34e-03, 7.85e-04, 4.11e-06, 2.81e+00, 2.81e+00]    []  
37000     [3.56e-04, 2.71e-04, 1.35e-06, 2.85e-01, 2.85e-01]    [3.56e-04, 2.71e-04, 1.35e-06, 2.85e-01, 2.85e-01]    []  
40000     [3.59e-04, 3.41e-04, 5.28e-05, 5.38e-01, 5.38e-01]    [3.59e-04, 3.41e-04, 5.28e-05, 5.38e-01, 5.38e-01]    []  
48000     [9.50e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.50e-08, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
45000     [5.30e-04, 4.61e-04, 1.37e-04, 2.12e-01, 2.12e-01]    [5.30e-04, 4.61e-04, 1.37e-04, 2.12e-01, 2.12e-01]    []  
1000      [7.96e-07, 1.08e-03, 3.75e-16, 2.16e+03, 2.16e+03]    [7.96e-07, 1.08e-03, 3.75e-16, 2.16e+03, 2.16e+03]    []  
6000      [6.65e-11, 1.05e-03, 4.35e-22, 2.09e+03, 2.09e+03]    [6.65e-11, 1.05e-03, 4.35e-22, 2.09e+03, 2.09e+03]    []  
20000     [1.20e-03, 8.30e-04, 2.65e-07, 2.27e+00, 2.27e+00]    [1.20e-03, 8.30e-04, 2.65e-07, 2.27e+00, 2.27e+00]    []  
                                                                                                                           38000     [4.48e-04, 2.73e-04, 1.82e-06, 3.95e-01, 3.95e-01]    [4.48e-04, 2.73e-04, 1.82e-06, 3.95e-01, 3.95e-01]    []  
41000     [3.19e-04, 3.31e-04, 1.65e-05, 3.07e-01, 3.07e-01]    [3.19e-04, 3.31e-04, 1.65e-05, 3.07e-01, 3.07e-01]    []  
49000     [4.39e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [4.39e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
7000      [1.09e-10, 1.05e-03, 4.17e-21, 2.09e+03, 2.09e+03]    [1.09e-10, 1.05e-03, 4.17e-21, 2.09e+03, 2.09e+03]    []  
2000      [5.45e-08, 1.06e-03, 3.74e-17, 2.10e+03, 2.10e+03]    [5.45e-08, 1.06e-03, 3.74e-17, 2.10e+03, 2.10e+03]    []  
21000     [1.04e-03, 7.45e-04, 7.09e-07, 1.94e+00, 1.94e+00]    [1.04e-03, 7.45e-04, 7.09e-07, 1.94e+00, 1.94e+00]    []  
46000     [6.81e-04, 4.59e-04, 8.43e-05, 2.11e-01, 2.11e-01]    [6.81e-04, 4.59e-04, 8.43e-05, 2.11e-01, 2.11e-01]    []  
                                                                                                                           39000     [3.42e-04, 2.74e-04, 7.50e-07, 2.63e-01, 2.63e-01]    [3.42e-04, 2.74e-04, 7.50e-07, 2.63e-01, 2.63e-01]    []  
42000     [3.14e-04, 3.18e-04, 8.79e-06, 2.93e-01, 2.93e-01]    [3.14e-04, 3.18e-04, 8.79e-06, 2.93e-01, 2.93e-01]    []  
50000     [2.46e-02, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.46e-02, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  

Best model at step 3000:
  train loss: 4.28e+00
  test loss: 4.28e+00
  test metric: []

'train' took 11477.738220 s

[I 2023-10-09 00:18:35,746] Trial 16 finished with value: 45.662853870335816 and parameters: {'num_domain': 69531, 'num_boundary': 5612, 'resampling_period': 16390, 'lr': 0.030731961514242394}. Best is trial 0 with value: 0.4913720930752669.
8000      [2.05e-07, 1.05e-03, 1.18e-19, 2.09e+03, 2.09e+03]    [2.05e-07, 1.05e-03, 1.18e-19, 2.09e+03, 2.09e+03]    []  
22000     [7.44e-04, 6.12e-04, 1.12e-06, 1.71e+00, 1.71e+00]    [7.44e-04, 6.12e-04, 1.12e-06, 1.71e+00, 1.71e+00]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000095 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [7.90e+01, 3.08e-04, 1.71e-07, 3.63e+03, 3.63e+03]    [7.90e+01, 3.08e-04, 1.71e-07, 3.63e+03, 3.63e+03]    []  
3000      [1.65e-10, 1.05e-03, 8.63e-18, 2.09e+03, 2.09e+03]    [1.65e-10, 1.05e-03, 8.63e-18, 2.09e+03, 2.09e+03]    []  
47000     [1.24e-03, 4.59e-04, 8.83e-05, 2.06e-01, 2.06e-01]    [1.24e-03, 4.59e-04, 8.83e-05, 2.06e-01, 2.06e-01]    []  
                                                                                                                           40000     [3.80e-04, 2.61e-04, 1.33e-06, 2.53e-01, 2.53e-01]    [3.80e-04, 2.61e-04, 1.33e-06, 2.53e-01, 2.53e-01]    []  
43000     [2.91e-04, 3.18e-04, 8.97e-06, 2.81e-01, 2.81e-01]    [2.91e-04, 3.18e-04, 8.97e-06, 2.81e-01, 2.81e-01]    []  
23000     [9.00e-04, 5.45e-04, 1.51e-06, 1.54e+00, 1.54e+00]    [9.00e-04, 5.45e-04, 1.51e-06, 1.54e+00, 1.54e+00]    []  
9000      [3.19e-10, 1.05e-03, 4.66e-20, 2.09e+03, 2.09e+03]    [3.19e-10, 1.05e-03, 4.66e-20, 2.09e+03, 2.09e+03]    []  
1000      [1.36e-06, 1.09e-03, 2.04e-15, 2.13e+03, 2.13e+03]    [1.36e-06, 1.09e-03, 2.04e-15, 2.13e+03, 2.13e+03]    []  
4000      [6.95e-11, 1.05e-03, 1.88e-18, 2.09e+03, 2.09e+03]    [6.95e-11, 1.05e-03, 1.88e-18, 2.09e+03, 2.09e+03]    []  
48000     [1.18e-03, 4.60e-04, 1.12e-04, 1.91e-01, 1.91e-01]    [1.18e-03, 4.60e-04, 1.12e-04, 1.91e-01, 1.91e-01]    []  
41000     [4.60e-04, 2.38e-04, 1.30e-06, 2.42e-01, 2.42e-01]    [4.60e-04, 2.38e-04, 1.30e-06, 2.42e-01, 2.42e-01]    []  
44000     [2.91e-04, 3.14e-04, 7.71e-06, 2.68e-01, 2.68e-01]    [2.91e-04, 3.14e-04, 7.71e-06, 2.68e-01, 2.68e-01]    []  
24000     [9.34e-04, 4.68e-04, 6.07e-06, 1.65e+00, 1.65e+00]    [9.34e-04, 4.68e-04, 6.07e-06, 1.65e+00, 1.65e+00]    []  
10000     [8.00e-07, 1.05e-03, 5.12e-18, 2.09e+03, 2.09e+03]    [8.00e-07, 1.05e-03, 5.12e-18, 2.09e+03, 2.09e+03]    []  
2000      [1.37e-08, 1.05e-03, 5.33e-16, 2.09e+03, 2.09e+03]    [1.37e-08, 1.05e-03, 5.33e-16, 2.09e+03, 2.09e+03]    []  
5000      [5.28e-11, 1.05e-03, 1.44e-18, 2.09e+03, 2.09e+03]    [5.28e-11, 1.05e-03, 1.44e-18, 2.09e+03, 2.09e+03]    []  
49000     [1.63e-03, 4.56e-04, 2.43e-04, 2.15e-01, 2.15e-01]    [1.63e-03, 4.56e-04, 2.43e-04, 2.15e-01, 2.15e-01]    []  
42000     [4.36e-04, 2.30e-04, 4.87e-06, 4.04e-01, 4.04e-01]    [4.36e-04, 2.30e-04, 4.87e-06, 4.04e-01, 4.04e-01]    []  
45000     [3.55e-04, 3.15e-04, 7.22e-06, 2.58e-01, 2.58e-01]    [3.55e-04, 3.15e-04, 7.22e-06, 2.58e-01, 2.58e-01]    []  
25000     [1.09e-03, 4.24e-04, 1.20e-03, 1.22e+00, 1.22e+00]    [1.09e-03, 4.24e-04, 1.20e-03, 1.22e+00, 1.22e+00]    []  
11000     [1.55e-02, 6.58e-04, 2.80e-08, 6.05e+01, 6.05e+01]    [1.55e-02, 6.58e-04, 2.80e-08, 6.05e+01, 6.05e+01]    []  
3000      [1.14e-10, 1.05e-03, 7.53e-17, 2.09e+03, 2.09e+03]    [1.14e-10, 1.05e-03, 7.53e-17, 2.09e+03, 2.09e+03]    []  
6000      [9.54e-11, 1.05e-03, 5.33e-19, 2.09e+03, 2.09e+03]    [9.54e-11, 1.05e-03, 5.33e-19, 2.09e+03, 2.09e+03]    []  
                                                                                                                           50000     [1.13e-03, 4.57e-04, 8.96e-05, 2.04e-01, 2.04e-01]    [1.13e-03, 4.57e-04, 8.96e-05, 2.04e-01, 2.04e-01]    []  

Best model at step 48000:
  train loss: 3.83e-01
  test loss: 3.83e-01
  test metric: []

'train' took 11878.301238 s

[I 2023-10-09 00:31:13,805] Trial 18 finished with value: 0.6510920388673289 and parameters: {'num_domain': 69982, 'num_boundary': 8881, 'resampling_period': 28250, 'lr': 0.00013313413613476633}. Best is trial 0 with value: 0.4913720930752669.
43000     [2.79e-04, 2.18e-04, 9.15e-07, 2.25e-01, 2.25e-01]    [2.79e-04, 2.18e-04, 9.15e-07, 2.25e-01, 2.25e-01]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000117 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [6.29e+01, 2.42e-04, 2.82e-07, 3.61e+03, 3.61e+03]    [6.29e+01, 2.42e-04, 2.82e-07, 3.61e+03, 3.61e+03]    []  
46000     [2.35e-04, 3.02e-04, 7.00e-06, 2.48e-01, 2.48e-01]    [2.35e-04, 3.02e-04, 7.00e-06, 2.48e-01, 2.48e-01]    []  
26000     [6.01e-04, 3.47e-04, 8.21e-07, 1.08e+00, 1.08e+00]    [6.01e-04, 3.47e-04, 8.21e-07, 1.08e+00, 1.08e+00]    []  
12000     [5.20e-03, 1.35e-04, 2.27e-06, 1.39e+01, 1.39e+01]    [5.20e-03, 1.35e-04, 2.27e-06, 1.39e+01, 1.39e+01]    []  
4000      [1.11e-10, 1.05e-03, 1.33e-15, 2.09e+03, 2.09e+03]    [1.11e-10, 1.05e-03, 1.33e-15, 2.09e+03, 2.09e+03]    []  
7000      [4.71e-11, 1.05e-03, 2.65e-19, 2.09e+03, 2.09e+03]    [4.71e-11, 1.05e-03, 2.65e-19, 2.09e+03, 2.09e+03]    []  
44000     [3.29e-04, 2.18e-04, 1.11e-06, 2.26e-01, 2.26e-01]    [3.29e-04, 2.18e-04, 1.11e-06, 2.26e-01, 2.26e-01]    []  
1000      [5.34e-09, 1.05e-03, 3.06e-16, 2.09e+03, 2.09e+03]    [5.34e-09, 1.05e-03, 3.06e-16, 2.09e+03, 2.09e+03]    []  
47000     [2.45e-04, 2.88e-04, 1.16e-05, 2.72e-01, 2.72e-01]    [2.45e-04, 2.88e-04, 1.16e-05, 2.72e-01, 2.72e-01]    []  
27000     [6.04e-04, 3.06e-04, 6.87e-06, 1.03e+00, 1.03e+00]    [6.04e-04, 3.06e-04, 6.87e-06, 1.03e+00, 1.03e+00]    []  
13000     [1.13e-03, 2.38e-04, 1.56e-07, 1.03e+01, 1.03e+01]    [1.13e-03, 2.38e-04, 1.56e-07, 1.03e+01, 1.03e+01]    []  
5000      [1.28e-01, 8.74e-04, 2.32e-07, 1.53e+03, 1.53e+03]    [1.28e-01, 8.74e-04, 2.32e-07, 1.53e+03, 1.53e+03]    []  
8000      [3.85e-11, 1.05e-03, 1.54e-19, 2.09e+03, 2.09e+03]    [3.85e-11, 1.05e-03, 1.54e-19, 2.09e+03, 2.09e+03]    []  
45000     [3.06e-04, 2.13e-04, 1.59e-06, 2.08e-01, 2.08e-01]    [3.06e-04, 2.13e-04, 1.59e-06, 2.08e-01, 2.08e-01]    []  
2000      [1.52e-10, 1.05e-03, 9.62e-17, 2.09e+03, 2.09e+03]    [1.52e-10, 1.05e-03, 9.62e-17, 2.09e+03, 2.09e+03]    []  
48000     [1.97e-04, 2.70e-04, 7.51e-06, 2.33e-01, 2.33e-01]    [1.97e-04, 2.70e-04, 7.51e-06, 2.33e-01, 2.33e-01]    []  
28000     [7.40e-04, 2.65e-04, 1.12e-04, 1.31e+00, 1.31e+00]    [7.40e-04, 2.65e-04, 1.12e-04, 1.31e+00, 1.31e+00]    []  
14000     [6.05e-04, 2.67e-04, 1.86e-07, 8.82e+00, 8.82e+00]    [6.05e-04, 2.67e-04, 1.86e-07, 8.82e+00, 8.82e+00]    []  
6000      [6.36e-02, 2.67e-04, 1.83e-04, 3.92e+01, 3.92e+01]    [6.36e-02, 2.67e-04, 1.83e-04, 3.92e+01, 3.92e+01]    []  
9000      [1.44e-10, 1.05e-03, 1.12e-19, 2.09e+03, 2.09e+03]    [1.44e-10, 1.05e-03, 1.12e-19, 2.09e+03, 2.09e+03]    []  
46000     [2.72e-04, 2.08e-04, 1.87e-06, 2.00e-01, 2.00e-01]    [2.72e-04, 2.08e-04, 1.87e-06, 2.00e-01, 2.00e-01]    []  
3000      [9.42e-11, 1.05e-03, 2.24e-16, 2.09e+03, 2.09e+03]    [9.42e-11, 1.05e-03, 2.24e-16, 2.09e+03, 2.09e+03]    []  
49000     [2.26e-04, 2.57e-04, 1.41e-05, 3.63e-01, 3.63e-01]    [2.26e-04, 2.57e-04, 1.41e-05, 3.63e-01, 3.63e-01]    []  
29000     [4.38e-04, 2.22e-04, 1.75e-07, 7.52e-01, 7.52e-01]    [4.38e-04, 2.22e-04, 1.75e-07, 7.52e-01, 7.52e-01]    []  
15000     [2.74e-04, 2.61e-04, 2.96e-07, 7.71e+00, 7.71e+00]    [2.74e-04, 2.61e-04, 2.96e-07, 7.71e+00, 7.71e+00]    []  
7000      [1.65e-02, 3.58e-04, 3.98e-04, 1.09e+01, 1.09e+01]    [1.65e-02, 3.58e-04, 3.98e-04, 1.09e+01, 1.09e+01]    []  
10000     [2.27e-08, 8.16e-04, 1.43e-12, 1.70e+03, 1.70e+03]    [2.27e-08, 8.16e-04, 1.43e-12, 1.70e+03, 1.70e+03]    []  
47000     [6.04e-04, 2.11e-04, 2.33e-06, 2.26e-01, 2.26e-01]    [6.04e-04, 2.11e-04, 2.33e-06, 2.26e-01, 2.26e-01]    []  
4000      [2.03e-02, 1.03e-03, 2.01e-12, 1.57e+03, 1.57e+03]    [2.03e-02, 1.03e-03, 2.01e-12, 1.57e+03, 1.57e+03]    []  
50000     [2.16e-04, 2.46e-04, 7.07e-06, 3.38e-01, 3.38e-01]    [2.16e-04, 2.46e-04, 7.07e-06, 3.38e-01, 3.38e-01]    []  

Best model at step 48000:
  train loss: 4.67e-01
  test loss: 4.67e-01
  test metric: []

'train' took 10517.545161 s

[I 2023-10-09 00:45:48,427] Trial 19 finished with value: 0.7610404220800929 and parameters: {'num_domain': 62949, 'num_boundary': 1576, 'resampling_period': 6745, 'lr': 0.00012015071612275903}. Best is trial 0 with value: 0.4913720930752669.
30000     [9.14e-04, 1.73e-04, 2.28e-05, 6.76e-01, 6.76e-01]    [9.14e-04, 1.73e-04, 2.28e-05, 6.76e-01, 6.76e-01]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000243 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.15e+02, 9.15e-04, 1.56e-07, 3.60e+03, 3.60e+03]    [1.15e+02, 9.15e-04, 1.56e-07, 3.60e+03, 3.60e+03]    []  
16000     [3.24e-04, 1.66e-04, 1.73e-07, 6.94e+00, 6.94e+00]    [3.24e-04, 1.66e-04, 1.73e-07, 6.94e+00, 6.94e+00]    []  
8000      [6.30e-02, 5.10e-04, 1.06e-03, 8.46e+00, 8.46e+00]    [6.30e-02, 5.10e-04, 1.06e-03, 8.46e+00, 8.46e+00]    []  
                                                                                                                                                                                                                                                      48000     [4.54e-04, 2.15e-04, 1.44e-06, 1.89e-01, 1.89e-01]    [4.54e-04, 2.15e-04, 1.44e-06, 1.89e-01, 1.89e-01]    []  
11000     [2.31e-02, 1.08e-04, 3.58e-04, 3.99e+01, 3.99e+01]    [2.31e-02, 1.08e-04, 3.58e-04, 3.99e+01, 3.99e+01]    []  
5000      [1.30e-02, 9.82e-04, 3.73e-04, 1.39e+01, 1.39e+01]    [1.30e-02, 9.82e-04, 3.73e-04, 1.39e+01, 1.39e+01]    []  
31000     [3.86e-04, 1.59e-04, 3.99e-07, 6.00e-01, 6.00e-01]    [3.86e-04, 1.59e-04, 3.99e-07, 6.00e-01, 6.00e-01]    []  
1000      [2.31e-08, 1.05e-03, 5.16e-16, 2.09e+03, 2.09e+03]    [2.31e-08, 1.05e-03, 5.16e-16, 2.09e+03, 2.09e+03]    []  
17000     [6.96e-04, 2.99e-04, 7.25e-09, 6.31e+00, 6.31e+00]    [6.96e-04, 2.99e-04, 7.25e-09, 6.31e+00, 6.31e+00]    []  
9000      [7.83e-03, 4.43e-04, 3.17e-04, 7.08e+00, 7.08e+00]    [7.83e-03, 4.43e-04, 3.17e-04, 7.08e+00, 7.08e+00]    []  
49000     [2.73e-04, 2.04e-04, 1.52e-06, 1.81e-01, 1.81e-01]    [2.73e-04, 2.04e-04, 1.52e-06, 1.81e-01, 1.81e-01]    []  
12000     [1.91e-02, 1.10e-04, 2.74e-04, 1.65e+01, 1.65e+01]    [1.91e-02, 1.10e-04, 2.74e-04, 1.65e+01, 1.65e+01]    []  
6000      [7.89e-03, 1.12e-03, 8.61e-03, 9.03e+00, 9.03e+00]    [7.89e-03, 1.12e-03, 8.61e-03, 9.03e+00, 9.03e+00]    []  
32000     [3.61e-04, 1.59e-04, 1.15e-06, 5.41e-01, 5.41e-01]    [3.61e-04, 1.59e-04, 1.15e-06, 5.41e-01, 5.41e-01]    []  
2000      [1.35e-10, 1.05e-03, 1.05e-15, 2.09e+03, 2.09e+03]    [1.35e-10, 1.05e-03, 1.05e-15, 2.09e+03, 2.09e+03]    []  
18000     [7.58e-04, 3.77e-04, 1.15e-07, 5.72e+00, 5.72e+00]    [7.58e-04, 3.77e-04, 1.15e-07, 5.72e+00, 5.72e+00]    []  
10000     [5.44e-03, 2.33e-04, 1.63e-04, 6.02e+00, 6.02e+00]    [5.44e-03, 2.33e-04, 1.63e-04, 6.02e+00, 6.02e+00]    []  
50000     [2.50e-04, 2.01e-04, 1.88e-06, 1.74e-01, 1.74e-01]    [2.50e-04, 2.01e-04, 1.88e-06, 1.74e-01, 1.74e-01]    []  

Best model at step 50000:
  train loss: 3.49e-01
  test loss: 3.49e-01
  test metric: []

'train' took 10446.943496 s

[I 2023-10-09 00:55:38,110] Trial 20 finished with value: 0.6311078703716902 and parameters: {'num_domain': 62447, 'num_boundary': 991, 'resampling_period': 184, 'lr': 0.00010812640659497966}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000095 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.10e+01, 1.18e-05, 1.51e-07, 3.60e+03, 3.60e+03]    [4.10e+01, 1.18e-05, 1.51e-07, 3.60e+03, 3.60e+03]    []  
33000     [3.75e-04, 1.55e-04, 9.27e-06, 4.97e-01, 4.97e-01]    [3.75e-04, 1.55e-04, 9.27e-06, 4.97e-01, 4.97e-01]    []  
13000     [4.26e-03, 4.58e-05, 9.15e-05, 1.28e+01, 1.28e+01]    [4.26e-03, 4.58e-05, 9.15e-05, 1.28e+01, 1.28e+01]    []  
7000      [5.63e-04, 1.01e-03, 9.93e-06, 6.86e+00, 6.86e+00]    [5.63e-04, 1.01e-03, 9.93e-06, 6.86e+00, 6.86e+00]    []  
3000      [7.90e-01, 8.08e-04, 2.02e-07, 1.19e+03, 1.19e+03]    [7.90e-01, 8.08e-04, 2.02e-07, 1.19e+03, 1.19e+03]    []  
19000     [8.78e-04, 5.37e-04, 5.29e-07, 5.18e+00, 5.18e+00]    [8.78e-04, 5.37e-04, 5.29e-07, 5.18e+00, 5.18e+00]    []  
11000     [4.26e-03, 1.02e-04, 9.07e-05, 4.34e+00, 4.34e+00]    [4.26e-03, 1.02e-04, 9.07e-05, 4.34e+00, 4.34e+00]    []  
                                                                                                                           1000      [3.18e-09, 1.05e-03, 2.55e-17, 2.09e+03, 2.09e+03]    [3.18e-09, 1.05e-03, 2.55e-17, 2.09e+03, 2.09e+03]    []  
34000     [4.44e-04, 1.54e-04, 1.06e-06, 5.26e-01, 5.26e-01]    [4.44e-04, 1.54e-04, 1.06e-06, 5.26e-01, 5.26e-01]    []  
8000      [4.67e-04, 5.21e-04, 3.95e-06, 4.82e+00, 4.82e+00]    [4.67e-04, 5.21e-04, 3.95e-06, 4.82e+00, 4.82e+00]    []  
14000     [3.75e-03, 2.57e-05, 2.55e-04, 1.07e+01, 1.07e+01]    [3.75e-03, 2.57e-05, 2.55e-04, 1.07e+01, 1.07e+01]    []  
4000      [3.03e-02, 1.48e-04, 1.69e-03, 1.32e+01, 1.32e+01]    [3.03e-02, 1.48e-04, 1.69e-03, 1.32e+01, 1.32e+01]    []  
20000     [1.07e-03, 8.04e-04, 5.46e-07, 4.59e+00, 4.59e+00]    [1.07e-03, 8.04e-04, 5.46e-07, 4.59e+00, 4.59e+00]    []  
12000     [4.17e-03, 3.82e-05, 1.15e-04, 3.05e+00, 3.05e+00]    [4.17e-03, 3.82e-05, 1.15e-04, 3.05e+00, 3.05e+00]    []  
2000      [1.18e-10, 1.05e-03, 8.61e-17, 2.09e+03, 2.09e+03]    [1.18e-10, 1.05e-03, 8.61e-17, 2.09e+03, 2.09e+03]    []  
35000     [3.98e-04, 1.54e-04, 1.20e-06, 4.51e-01, 4.51e-01]    [3.98e-04, 1.54e-04, 1.20e-06, 4.51e-01, 4.51e-01]    []  
9000      [1.66e-04, 9.02e-05, 7.68e-07, 4.28e+00, 4.28e+00]    [1.66e-04, 9.02e-05, 7.68e-07, 4.28e+00, 4.28e+00]    []  
15000     [1.12e-03, 4.49e-06, 5.62e-05, 9.20e+00, 9.20e+00]    [1.12e-03, 4.49e-06, 5.62e-05, 9.20e+00, 9.20e+00]    []  
5000      [8.26e-03, 2.92e-04, 2.18e-03, 9.12e+00, 9.12e+00]    [8.26e-03, 2.92e-04, 2.18e-03, 9.12e+00, 9.12e+00]    []  
21000     [1.04e-03, 1.31e-03, 5.09e-08, 4.05e+00, 4.05e+00]    [1.04e-03, 1.31e-03, 5.09e-08, 4.05e+00, 4.05e+00]    []  
13000     [2.90e-03, 2.33e-05, 1.23e-04, 2.34e+00, 2.34e+00]    [2.90e-03, 2.33e-05, 1.23e-04, 2.34e+00, 2.34e+00]    []  
3000      [2.61e-10, 1.05e-03, 3.06e-15, 2.09e+03, 2.09e+03]    [2.61e-10, 1.05e-03, 3.06e-15, 2.09e+03, 2.09e+03]    []  
36000     [3.80e-04, 1.48e-04, 4.70e-07, 4.88e-01, 4.88e-01]    [3.80e-04, 1.48e-04, 4.70e-07, 4.88e-01, 4.88e-01]    []  
10000     [1.77e-04, 4.10e-05, 1.50e-07, 2.55e+00, 2.55e+00]    [1.77e-04, 4.10e-05, 1.50e-07, 2.55e+00, 2.55e+00]    []  
6000      [5.31e-03, 2.70e-04, 4.94e-04, 7.84e+00, 7.84e+00]    [5.31e-03, 2.70e-04, 4.94e-04, 7.84e+00, 7.84e+00]    []  
16000     [2.67e-04, 2.33e-06, 2.79e-05, 8.03e+00, 8.03e+00]    [2.67e-04, 2.33e-06, 2.79e-05, 8.03e+00, 8.03e+00]    []  
4000      [5.07e-03, 9.36e-04, 4.64e-05, 2.62e+01, 2.62e+01]    [5.07e-03, 9.36e-04, 4.64e-05, 2.62e+01, 2.62e+01]    []  
22000     [7.39e-04, 1.85e-03, 6.62e-08, 3.66e+00, 3.66e+00]    [7.39e-04, 1.85e-03, 6.62e-08, 3.66e+00, 3.66e+00]    []  
14000     [1.99e-03, 6.94e-05, 4.33e-05, 1.86e+00, 1.86e+00]    [1.99e-03, 6.94e-05, 4.33e-05, 1.86e+00, 1.86e+00]    []  
37000     [3.64e-04, 1.49e-04, 1.38e-06, 3.63e-01, 3.63e-01]    [3.64e-04, 1.49e-04, 1.38e-06, 3.63e-01, 3.63e-01]    []  
11000     [4.45e-04, 1.44e-04, 1.03e-06, 2.33e+00, 2.33e+00]    [4.45e-04, 1.44e-04, 1.03e-06, 2.33e+00, 2.33e+00]    []  
5000      [8.74e-04, 1.14e-03, 6.80e-05, 1.09e+01, 1.09e+01]    [8.74e-04, 1.14e-03, 6.80e-05, 1.09e+01, 1.09e+01]    []  
7000      [5.60e-03, 2.73e-04, 4.33e-04, 6.63e+00, 6.63e+00]    [5.60e-03, 2.73e-04, 4.33e-04, 6.63e+00, 6.63e+00]    []  
17000     [3.54e-04, 1.46e-06, 1.84e-05, 6.93e+00, 6.93e+00]    [3.54e-04, 1.46e-06, 1.84e-05, 6.93e+00, 6.93e+00]    []  
23000     [6.75e-04, 1.77e-03, 9.89e-07, 3.15e+00, 3.15e+00]    [6.75e-04, 1.77e-03, 9.89e-07, 3.15e+00, 3.15e+00]    []  
15000     [2.04e-03, 3.14e-04, 1.18e-05, 1.82e+00, 1.82e+00]    [2.04e-03, 3.14e-04, 1.18e-05, 1.82e+00, 1.82e+00]    []  
                                                                                                                           38000     [3.48e-04, 1.56e-04, 7.21e-07, 5.96e-01, 5.96e-01]    [3.48e-04, 1.56e-04, 7.21e-07, 5.96e-01, 5.96e-01]    []  
6000      [8.10e-04, 6.85e-04, 6.86e-06, 9.29e+00, 9.29e+00]    [8.10e-04, 6.85e-04, 6.86e-06, 9.29e+00, 9.29e+00]    []  
12000     [7.13e-04, 5.39e-04, 1.80e-06, 2.40e+00, 2.40e+00]    [7.13e-04, 5.39e-04, 1.80e-06, 2.40e+00, 2.40e+00]    []  
8000      [4.62e-03, 1.70e-04, 1.71e-03, 5.43e+00, 5.43e+00]    [4.62e-03, 1.70e-04, 1.71e-03, 5.43e+00, 5.43e+00]    []  
18000     [4.15e-04, 4.16e-06, 2.21e-05, 6.10e+00, 6.10e+00]    [4.15e-04, 4.16e-06, 2.21e-05, 6.10e+00, 6.10e+00]    []  
24000     [4.96e-04, 1.42e-03, 8.81e-07, 2.88e+00, 2.88e+00]    [4.96e-04, 1.42e-03, 8.81e-07, 2.88e+00, 2.88e+00]    []  
16000     [2.34e-03, 5.77e-04, 1.85e-05, 1.32e+00, 1.32e+00]    [2.34e-03, 5.77e-04, 1.85e-05, 1.32e+00, 1.32e+00]    []  
                                                                                                                           39000     [3.04e-04, 1.56e-04, 4.43e-07, 3.47e-01, 3.47e-01]    [3.04e-04, 1.56e-04, 4.43e-07, 3.47e-01, 3.47e-01]    []  
7000      [3.66e-04, 6.12e-04, 2.23e-06, 7.65e+00, 7.65e+00]    [3.66e-04, 6.12e-04, 2.23e-06, 7.65e+00, 7.65e+00]    []  
13000     [8.23e-04, 1.07e-03, 1.12e-05, 1.50e+00, 1.50e+00]    [8.23e-04, 1.07e-03, 1.12e-05, 1.50e+00, 1.50e+00]    []  
9000      [8.07e-03, 1.53e-04, 2.62e-04, 4.21e+00, 4.21e+00]    [8.07e-03, 1.53e-04, 2.62e-04, 4.21e+00, 4.21e+00]    []  
25000     [3.72e-04, 1.00e-03, 3.63e-07, 2.13e+00, 2.13e+00]    [3.72e-04, 1.00e-03, 3.63e-07, 2.13e+00, 2.13e+00]    []  
19000     [5.69e-04, 1.64e-05, 1.18e-05, 5.38e+00, 5.38e+00]    [5.69e-04, 1.64e-05, 1.18e-05, 5.38e+00, 5.38e+00]    []  
17000     [1.12e-03, 5.47e-04, 7.11e-06, 1.15e+00, 1.15e+00]    [1.12e-03, 5.47e-04, 7.11e-06, 1.15e+00, 1.15e+00]    []  
8000      [4.14e-04, 2.71e-04, 5.21e-07, 6.95e+00, 6.95e+00]    [4.14e-04, 2.71e-04, 5.21e-07, 6.95e+00, 6.95e+00]    []  
40000     [2.97e-04, 1.52e-04, 7.07e-07, 3.19e-01, 3.19e-01]    [2.97e-04, 1.52e-04, 7.07e-07, 3.19e-01, 3.19e-01]    []  
                                                                                                                           14000     [8.84e-04, 1.14e-03, 3.42e-05, 1.31e+00, 1.31e+00]    [8.84e-04, 1.14e-03, 3.42e-05, 1.31e+00, 1.31e+00]    []  
10000     [4.31e-03, 3.75e-04, 2.50e-04, 3.49e+00, 3.49e+00]    [4.31e-03, 3.75e-04, 2.50e-04, 3.49e+00, 3.49e+00]    []  
26000     [3.78e-04, 4.75e-04, 7.54e-07, 1.78e+00, 1.78e+00]    [3.78e-04, 4.75e-04, 7.54e-07, 1.78e+00, 1.78e+00]    []  
20000     [4.11e-04, 2.66e-05, 8.59e-06, 5.12e+00, 5.12e+00]    [4.11e-04, 2.66e-05, 8.59e-06, 5.12e+00, 5.12e+00]    []  
18000     [8.98e-04, 4.59e-04, 1.69e-05, 1.00e+00, 1.00e+00]    [8.98e-04, 4.59e-04, 1.69e-05, 1.00e+00, 1.00e+00]    []  
9000      [6.66e-04, 4.51e-04, 1.29e-06, 6.15e+00, 6.15e+00]    [6.66e-04, 4.51e-04, 1.29e-06, 6.15e+00, 6.15e+00]    []  
41000     [5.16e-04, 1.49e-04, 1.09e-06, 2.85e-01, 2.85e-01]    [5.16e-04, 1.49e-04, 1.09e-06, 2.85e-01, 2.85e-01]    []  
15000     [8.46e-04, 9.65e-04, 1.70e-05, 1.13e+00, 1.13e+00]    [8.46e-04, 9.65e-04, 1.70e-05, 1.13e+00, 1.13e+00]    []  
11000     [3.50e-03, 4.30e-04, 1.37e-03, 2.83e+00, 2.83e+00]    [3.50e-03, 4.30e-04, 1.37e-03, 2.83e+00, 2.83e+00]    []  
27000     [2.93e-04, 3.16e-04, 4.14e-07, 1.41e+00, 1.41e+00]    [2.93e-04, 3.16e-04, 4.14e-07, 1.41e+00, 1.41e+00]    []  
10000     [5.65e-04, 1.14e-03, 2.59e-06, 5.45e+00, 5.45e+00]    [5.65e-04, 1.14e-03, 2.59e-06, 5.45e+00, 5.45e+00]    []  
21000     [3.68e-04, 4.17e-05, 5.45e-06, 4.37e+00, 4.37e+00]    [3.68e-04, 4.17e-05, 5.45e-06, 4.37e+00, 4.37e+00]    []  
19000     [9.23e-04, 3.34e-04, 9.63e-06, 9.29e-01, 9.29e-01]    [9.23e-04, 3.34e-04, 9.63e-06, 9.29e-01, 9.29e-01]    []  
42000     [2.51e-04, 1.48e-04, 5.48e-06, 2.72e-01, 2.72e-01]    [2.51e-04, 1.48e-04, 5.48e-06, 2.72e-01, 2.72e-01]    []  
50000     [3.95e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.95e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  

Best model at step 1000:
  train loss: 4.19e+03
  test loss: 4.19e+03
  test metric: []

'train' took 16864.808434 s

[I 2023-10-09 01:26:48,627] Trial 14 finished with value: 45.66228964068075 and parameters: {'num_domain': 68415, 'num_boundary': 9119, 'resampling_period': 43743, 'lr': 0.06686274548496345}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000136 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.01e+01, 8.43e-05, 2.28e-07, 3.61e+03, 3.61e+03]    [3.01e+01, 8.43e-05, 2.28e-07, 3.61e+03, 3.61e+03]    []  
13000     [9.69e-04, 8.05e-04, 6.42e-06, 7.11e+00, 7.11e+00]    [9.69e-04, 8.05e-04, 6.42e-06, 7.11e+00, 7.11e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1000      [1.41e-08, 1.05e-03, 2.90e-16, 2.09e+03, 2.09e+03]    [1.41e-08, 1.05e-03, 2.90e-16, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  14000     [7.30e-04, 6.44e-04, 5.56e-06, 6.32e+00, 6.32e+00]    [7.30e-04, 6.44e-04, 5.56e-06, 6.32e+00, 6.32e+00]    []  
                                                                                                                                                                                                                                                      2000      [9.65e-11, 1.05e-03, 9.23e-17, 2.09e+03, 2.09e+03]    [9.65e-11, 1.05e-03, 9.23e-17, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        16000     [4.22e-03, 8.21e-04, 3.33e-04, 9.14e-01, 9.14e-01]    [4.22e-03, 8.21e-04, 3.33e-04, 9.14e-01, 9.14e-01]    []  
32000     [3.26e-04, 3.26e-05, 1.93e-07, 6.92e-01, 6.92e-01]    [3.26e-04, 3.26e-05, 1.93e-07, 6.92e-01, 6.92e-01]    []  
16000     [7.05e-04, 9.55e-04, 2.11e-05, 1.43e+00, 1.43e+00]    [7.05e-04, 9.55e-04, 2.11e-05, 1.43e+00, 1.43e+00]    []  
47000     [2.94e-04, 1.45e-04, 2.39e-06, 2.19e-01, 2.19e-01]    [2.94e-04, 1.45e-04, 2.39e-06, 2.19e-01, 2.19e-01]    []  
24000     [1.88e-03, 1.80e-04, 3.90e-05, 5.43e-01, 5.43e-01]    [1.88e-03, 1.80e-04, 3.90e-05, 5.43e-01, 5.43e-01]    []  
26000     [8.55e-04, 5.45e-04, 1.08e-04, 2.20e+00, 2.20e+00]    [8.55e-04, 5.45e-04, 1.08e-04, 2.20e+00, 2.20e+00]    []  
21000     [4.06e-04, 4.65e-04, 1.09e-05, 3.58e-01, 3.58e-01]    [4.06e-04, 4.65e-04, 1.09e-05, 3.58e-01, 3.58e-01]    []  
17000     [3.52e-03, 7.79e-04, 8.84e-04, 7.81e-01, 7.81e-01]    [3.52e-03, 7.79e-04, 8.84e-04, 7.81e-01, 7.81e-01]    []  
33000     [2.70e-04, 3.37e-05, 7.33e-07, 6.17e-01, 6.17e-01]    [2.70e-04, 3.37e-05, 7.33e-07, 6.17e-01, 6.17e-01]    []  
17000     [7.25e-04, 9.87e-04, 4.33e-05, 1.32e+00, 1.32e+00]    [7.25e-04, 9.87e-04, 4.33e-05, 1.32e+00, 1.32e+00]    []  
48000     [4.21e-04, 1.40e-04, 2.04e-05, 2.13e-01, 2.13e-01]    [4.21e-04, 1.40e-04, 2.04e-05, 2.13e-01, 2.13e-01]    []  
25000     [1.85e-03, 1.42e-04, 2.09e-05, 5.23e-01, 5.23e-01]    [1.85e-03, 1.42e-04, 2.09e-05, 5.23e-01, 5.23e-01]    []  
                                                                                                                           27000     [9.23e-04, 5.97e-04, 1.15e-05, 1.95e+00, 1.95e+00]    [9.23e-04, 5.97e-04, 1.15e-05, 1.95e+00, 1.95e+00]    []  
22000     [4.31e-04, 4.22e-04, 5.57e-06, 7.91e-01, 7.91e-01]    [4.31e-04, 4.22e-04, 5.57e-06, 7.91e-01, 7.91e-01]    []  
18000     [5.86e-04, 1.03e-03, 3.09e-05, 1.52e+00, 1.52e+00]    [5.86e-04, 1.03e-03, 3.09e-05, 1.52e+00, 1.52e+00]    []  
18000     [2.13e-03, 7.06e-04, 8.34e-04, 6.44e-01, 6.44e-01]    [2.13e-03, 7.06e-04, 8.34e-04, 6.44e-01, 6.44e-01]    []  
34000     [2.90e-04, 2.99e-05, 5.62e-07, 5.96e-01, 5.96e-01]    [2.90e-04, 2.99e-05, 5.62e-07, 5.96e-01, 5.96e-01]    []  
49000     [2.85e-04, 1.38e-04, 1.55e-06, 2.01e-01, 2.01e-01]    [2.85e-04, 1.38e-04, 1.55e-06, 2.01e-01, 2.01e-01]    []  
26000     [1.09e-03, 1.40e-04, 2.29e-05, 5.97e-01, 5.97e-01]    [1.09e-03, 1.40e-04, 2.29e-05, 5.97e-01, 5.97e-01]    []  
28000     [1.08e-03, 5.87e-04, 2.39e-05, 1.82e+00, 1.82e+00]    [1.08e-03, 5.87e-04, 2.39e-05, 1.82e+00, 1.82e+00]    []  
19000     [5.03e-04, 1.03e-03, 2.37e-05, 8.98e-01, 8.98e-01]    [5.03e-04, 1.03e-03, 2.37e-05, 8.98e-01, 8.98e-01]    []  
23000     [3.52e-04, 4.04e-04, 7.15e-06, 3.03e-01, 3.03e-01]    [3.52e-04, 4.04e-04, 7.15e-06, 3.03e-01, 3.03e-01]    []  
19000     [2.18e-03, 6.77e-04, 1.09e-03, 1.22e+00, 1.22e+00]    [2.18e-03, 6.77e-04, 1.09e-03, 1.22e+00, 1.22e+00]    []  
35000     [2.43e-04, 2.95e-05, 4.94e-07, 5.04e-01, 5.04e-01]    [2.43e-04, 2.95e-05, 4.94e-07, 5.04e-01, 5.04e-01]    []  
50000     [4.58e-04, 1.32e-04, 4.22e-07, 3.38e-01, 3.38e-01]    [4.58e-04, 1.32e-04, 4.22e-07, 3.38e-01, 3.38e-01]    []  

Best model at step 49000:
  train loss: 4.03e-01
  test loss: 4.03e-01
  test metric: []

'train' took 10182.091390 s

[I 2023-10-09 01:53:43,899] Trial 21 finished with value: 0.7733209805655289 and parameters: {'num_domain': 59745, 'num_boundary': 376, 'resampling_period': 2239, 'lr': 0.0001631590248971745}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000106 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.96e+01, 1.37e-04, 2.61e-07, 3.60e+03, 3.60e+03]    [1.96e+01, 1.37e-04, 2.61e-07, 3.60e+03, 3.60e+03]    []  
27000     [8.75e-04, 1.31e-04, 2.85e-05, 4.45e-01, 4.45e-01]    [8.75e-04, 1.31e-04, 2.85e-05, 4.45e-01, 4.45e-01]    []  
20000     [3.83e-04, 1.04e-03, 9.02e-06, 6.16e-01, 6.16e-01]    [3.83e-04, 1.04e-03, 9.02e-06, 6.16e-01, 6.16e-01]    []  
29000     [4.92e-04, 5.49e-04, 3.59e-06, 1.57e+00, 1.57e+00]    [4.92e-04, 5.49e-04, 3.59e-06, 1.57e+00, 1.57e+00]    []  
24000     [5.23e-04, 3.80e-04, 5.19e-06, 1.09e+00, 1.09e+00]    [5.23e-04, 3.80e-04, 5.19e-06, 1.09e+00, 1.09e+00]    []  
20000     [2.86e-03, 6.32e-04, 2.46e-04, 8.81e-01, 8.81e-01]    [2.86e-03, 6.32e-04, 2.46e-04, 8.81e-01, 8.81e-01]    []  
36000     [2.26e-04, 2.80e-05, 4.28e-07, 4.38e-01, 4.38e-01]    [2.26e-04, 2.80e-05, 4.28e-07, 4.38e-01, 4.38e-01]    []  
28000     [1.43e-03, 1.12e-04, 2.29e-05, 4.30e-01, 4.30e-01]    [1.43e-03, 1.12e-04, 2.29e-05, 4.30e-01, 4.30e-01]    []  
1000      [5.85e-11, 1.05e-03, 1.85e-16, 2.09e+03, 2.09e+03]    [5.85e-11, 1.05e-03, 1.85e-16, 2.09e+03, 2.09e+03]    []  
21000     [1.96e-04, 5.75e-04, 7.37e-06, 4.29e-01, 4.29e-01]    [1.96e-04, 5.75e-04, 7.37e-06, 4.29e-01, 4.29e-01]    []  
30000     [4.99e-04, 5.25e-04, 1.11e-05, 1.39e+00, 1.39e+00]    [4.99e-04, 5.25e-04, 1.11e-05, 1.39e+00, 1.39e+00]    []  
25000     [3.12e-04, 3.83e-04, 4.26e-06, 4.24e-01, 4.24e-01]    [3.12e-04, 3.83e-04, 4.26e-06, 4.24e-01, 4.24e-01]    []  
21000     [2.03e-03, 5.82e-04, 1.20e-04, 4.60e-01, 4.60e-01]    [2.03e-03, 5.82e-04, 1.20e-04, 4.60e-01, 4.60e-01]    []  
37000     [2.31e-04, 2.71e-05, 2.00e-07, 4.66e-01, 4.66e-01]    [2.31e-04, 2.71e-05, 2.00e-07, 4.66e-01, 4.66e-01]    []  
29000     [1.63e-03, 1.18e-04, 2.85e-04, 3.84e-01, 3.84e-01]    [1.63e-03, 1.18e-04, 2.85e-04, 3.84e-01, 3.84e-01]    []  
22000     [3.53e-04, 2.28e-04, 4.80e-05, 3.85e-01, 3.85e-01]    [3.53e-04, 2.28e-04, 4.80e-05, 3.85e-01, 3.85e-01]    []  
2000      [6.87e-11, 1.05e-03, 2.60e-17, 2.09e+03, 2.09e+03]    [6.87e-11, 1.05e-03, 2.60e-17, 2.09e+03, 2.09e+03]    []  
31000     [3.99e-04, 4.94e-04, 6.48e-06, 1.22e+00, 1.22e+00]    [3.99e-04, 4.94e-04, 6.48e-06, 1.22e+00, 1.22e+00]    []  
26000     [2.74e-04, 3.77e-04, 2.20e-06, 2.93e-01, 2.93e-01]    [2.74e-04, 3.77e-04, 2.20e-06, 2.93e-01, 2.93e-01]    []  
22000     [2.24e-03, 5.21e-04, 4.59e-04, 5.71e-01, 5.71e-01]    [2.24e-03, 5.21e-04, 4.59e-04, 5.71e-01, 5.71e-01]    []  
38000     [2.38e-04, 2.55e-05, 3.55e-07, 3.81e-01, 3.81e-01]    [2.38e-04, 2.55e-05, 3.55e-07, 3.81e-01, 3.81e-01]    []  
23000     [1.62e-04, 1.67e-04, 1.05e-05, 4.34e-01, 4.34e-01]    [1.62e-04, 1.67e-04, 1.05e-05, 4.34e-01, 4.34e-01]    []  
30000     [1.05e-03, 1.06e-04, 6.02e-05, 3.61e-01, 3.61e-01]    [1.05e-03, 1.06e-04, 6.02e-05, 3.61e-01, 3.61e-01]    []  
3000      [1.14e-02, 1.74e-04, 1.73e-05, 7.76e+01, 7.76e+01]    [1.14e-02, 1.74e-04, 1.73e-05, 7.76e+01, 7.76e+01]    []  
32000     [4.74e-04, 4.85e-04, 1.96e-05, 1.13e+00, 1.13e+00]    [4.74e-04, 4.85e-04, 1.96e-05, 1.13e+00, 1.13e+00]    []  
27000     [3.85e-04, 3.69e-04, 2.84e-06, 2.80e-01, 2.80e-01]    [3.85e-04, 3.69e-04, 2.84e-06, 2.80e-01, 2.80e-01]    []  
23000     [3.31e-03, 5.61e-04, 1.14e-03, 4.08e-01, 4.08e-01]    [3.31e-03, 5.61e-04, 1.14e-03, 4.08e-01, 4.08e-01]    []  
24000     [1.18e-04, 1.18e-04, 9.99e-06, 4.24e-01, 4.24e-01]    [1.18e-04, 1.18e-04, 9.99e-06, 4.24e-01, 4.24e-01]    []  
39000     [1.96e-04, 2.55e-05, 2.04e-07, 3.89e-01, 3.89e-01]    [1.96e-04, 2.55e-05, 2.04e-07, 3.89e-01, 3.89e-01]    []  
31000     [5.96e-04, 9.62e-05, 1.71e-05, 3.51e-01, 3.51e-01]    [5.96e-04, 9.62e-05, 1.71e-05, 3.51e-01, 3.51e-01]    []  
                                                                                                                           28000     [3.31e-04, 3.61e-04, 9.58e-06, 1.99e-01, 1.99e-01]    [3.31e-04, 3.61e-04, 9.58e-06, 1.99e-01, 1.99e-01]    []  
25000     [1.05e-04, 9.84e-05, 1.47e-05, 3.40e-01, 3.40e-01]    [1.05e-04, 9.84e-05, 1.47e-05, 3.40e-01, 3.40e-01]    []  
33000     [4.78e-04, 4.75e-04, 2.58e-05, 9.20e-01, 9.20e-01]    [4.78e-04, 4.75e-04, 2.58e-05, 9.20e-01, 9.20e-01]    []  
4000      [2.81e-03, 5.41e-04, 2.50e-05, 9.69e+00, 9.69e+00]    [2.81e-03, 5.41e-04, 2.50e-05, 9.69e+00, 9.69e+00]    []  
24000     [3.09e-03, 5.65e-04, 5.78e-04, 4.27e-01, 4.27e-01]    [3.09e-03, 5.65e-04, 5.78e-04, 4.27e-01, 4.27e-01]    []  
40000     [1.71e-04, 2.44e-05, 3.22e-07, 3.71e-01, 3.71e-01]    [1.71e-04, 2.44e-05, 3.22e-07, 3.71e-01, 3.71e-01]    []  
32000     [1.02e-03, 8.83e-05, 5.18e-05, 7.17e-01, 7.17e-01]    [1.02e-03, 8.83e-05, 5.18e-05, 7.17e-01, 7.17e-01]    []  
26000     [1.67e-04, 8.56e-05, 2.02e-05, 4.15e-01, 4.15e-01]    [1.67e-04, 8.56e-05, 2.02e-05, 4.15e-01, 4.15e-01]    []  
29000     [2.31e-04, 3.60e-04, 1.35e-05, 2.57e-01, 2.57e-01]    [2.31e-04, 3.60e-04, 1.35e-05, 2.57e-01, 2.57e-01]    []  
34000     [3.80e-04, 4.88e-04, 8.39e-05, 6.83e-01, 6.83e-01]    [3.80e-04, 4.88e-04, 8.39e-05, 6.83e-01, 6.83e-01]    []  
25000     [1.97e-03, 5.62e-04, 3.08e-04, 3.65e-01, 3.65e-01]    [1.97e-03, 5.62e-04, 3.08e-04, 3.65e-01, 3.65e-01]    []  
41000     [1.87e-04, 2.50e-05, 1.03e-07, 4.09e-01, 4.09e-01]    [1.87e-04, 2.50e-05, 1.03e-07, 4.09e-01, 4.09e-01]    []  
5000      [5.14e-04, 4.96e-04, 1.85e-06, 8.59e+00, 8.59e+00]    [5.14e-04, 4.96e-04, 1.85e-06, 8.59e+00, 8.59e+00]    []  
33000     [8.56e-04, 8.04e-05, 1.01e-04, 3.15e-01, 3.15e-01]    [8.56e-04, 8.04e-05, 1.01e-04, 3.15e-01, 3.15e-01]    []  
27000     [1.34e-04, 8.02e-05, 1.30e-05, 3.44e-01, 3.44e-01]    [1.34e-04, 8.02e-05, 1.30e-05, 3.44e-01, 3.44e-01]    []  
                                                                                                                           30000     [2.18e-04, 3.56e-04, 2.86e-06, 3.37e-01, 3.37e-01]    [2.18e-04, 3.56e-04, 2.86e-06, 3.37e-01, 3.37e-01]    []  
26000     [1.53e-03, 5.12e-04, 1.53e-04, 5.07e-01, 5.07e-01]    [1.53e-03, 5.12e-04, 1.53e-04, 5.07e-01, 5.07e-01]    []  
35000     [3.33e-04, 4.76e-04, 6.25e-06, 6.54e-01, 6.54e-01]    [3.33e-04, 4.76e-04, 6.25e-06, 6.54e-01, 6.54e-01]    []  
42000     [1.44e-04, 2.45e-05, 2.32e-07, 3.08e-01, 3.08e-01]    [1.44e-04, 2.45e-05, 2.32e-07, 3.08e-01, 3.08e-01]    []  
6000      [6.56e-04, 1.62e-04, 1.39e-06, 4.42e+00, 4.42e+00]    [6.56e-04, 1.62e-04, 1.39e-06, 4.42e+00, 4.42e+00]    []  
34000     [9.34e-04, 7.83e-05, 2.24e-05, 3.07e-01, 3.07e-01]    [9.34e-04, 7.83e-05, 2.24e-05, 3.07e-01, 3.07e-01]    []  
28000     [1.59e-04, 7.95e-05, 1.21e-05, 3.11e-01, 3.11e-01]    [1.59e-04, 7.95e-05, 1.21e-05, 3.11e-01, 3.11e-01]    []  
31000     [2.18e-04, 3.32e-04, 1.90e-06, 3.72e-01, 3.72e-01]    [2.18e-04, 3.32e-04, 1.90e-06, 3.72e-01, 3.72e-01]    []  
27000     [1.63e-03, 4.58e-04, 1.58e-04, 2.71e-01, 2.71e-01]    [1.63e-03, 4.58e-04, 1.58e-04, 2.71e-01, 2.71e-01]    []  
43000     [1.54e-04, 2.45e-05, 1.42e-07, 2.78e-01, 2.78e-01]    [1.54e-04, 2.45e-05, 1.42e-07, 2.78e-01, 2.78e-01]    []  
36000     [2.91e-04, 4.63e-04, 5.25e-06, 4.88e-01, 4.88e-01]    [2.91e-04, 4.63e-04, 5.25e-06, 4.88e-01, 4.88e-01]    []  
29000     [1.06e-04, 7.91e-05, 1.16e-05, 2.93e-01, 2.93e-01]    [1.06e-04, 7.91e-05, 1.16e-05, 2.93e-01, 2.93e-01]    []  
35000     [8.50e-04, 6.89e-05, 5.98e-05, 7.99e-01, 7.99e-01]    [8.50e-04, 6.89e-05, 5.98e-05, 7.99e-01, 7.99e-01]    []  
7000      [8.33e-04, 7.59e-04, 4.62e-06, 2.62e+00, 2.62e+00]    [8.33e-04, 7.59e-04, 4.62e-06, 2.62e+00, 2.62e+00]    []  
                                                                                                                           32000     [2.38e-04, 3.06e-04, 3.32e-05, 4.19e-01, 4.19e-01]    [2.38e-04, 3.06e-04, 3.32e-05, 4.19e-01, 4.19e-01]    []  
28000     [2.03e-03, 4.24e-04, 2.97e-04, 4.40e-01, 4.40e-01]    [2.03e-03, 4.24e-04, 2.97e-04, 4.40e-01, 4.40e-01]    []  
44000     [1.62e-04, 2.49e-05, 2.62e-07, 2.95e-01, 2.95e-01]    [1.62e-04, 2.49e-05, 2.62e-07, 2.95e-01, 2.95e-01]    []  
30000     [9.91e-05, 7.94e-05, 9.86e-06, 2.32e-01, 2.32e-01]    [9.91e-05, 7.94e-05, 9.86e-06, 2.32e-01, 2.32e-01]    []  
37000     [4.14e-04, 4.47e-04, 7.32e-06, 4.62e-01, 4.62e-01]    [4.14e-04, 4.47e-04, 7.32e-06, 4.62e-01, 4.62e-01]    []  
36000     [1.06e-03, 7.85e-05, 4.23e-05, 2.74e-01, 2.74e-01]    [1.06e-03, 7.85e-05, 4.23e-05, 2.74e-01, 2.74e-01]    []  
8000      [9.11e-04, 1.07e-03, 3.53e-06, 1.69e+00, 1.69e+00]    [9.11e-04, 1.07e-03, 3.53e-06, 1.69e+00, 1.69e+00]    []  
33000     [2.36e-04, 2.68e-04, 7.20e-06, 5.47e-01, 5.47e-01]    [2.36e-04, 2.68e-04, 7.20e-06, 5.47e-01, 5.47e-01]    []  
31000     [1.36e-04, 8.11e-05, 2.50e-05, 2.36e-01, 2.36e-01]    [1.36e-04, 8.11e-05, 2.50e-05, 2.36e-01, 2.36e-01]    []  
29000     [1.44e-03, 3.91e-04, 5.09e-04, 1.71e+00, 1.71e+00]    [1.44e-03, 3.91e-04, 5.09e-04, 1.71e+00, 1.71e+00]    []  
45000     [1.83e-04, 2.48e-05, 3.44e-07, 3.27e-01, 3.27e-01]    [1.83e-04, 2.48e-05, 3.44e-07, 3.27e-01, 3.27e-01]    []  
38000     [2.97e-04, 4.35e-04, 1.28e-05, 5.60e-01, 5.60e-01]    [2.97e-04, 4.35e-04, 1.28e-05, 5.60e-01, 5.60e-01]    []  
37000     [7.04e-04, 8.44e-05, 1.77e-05, 2.67e-01, 2.67e-01]    [7.04e-04, 8.44e-05, 1.77e-05, 2.67e-01, 2.67e-01]    []  
                                                                                                                           32000     [1.12e-04, 8.24e-05, 7.94e-06, 2.06e-01, 2.06e-01]    [1.12e-04, 8.24e-05, 7.94e-06, 2.06e-01, 2.06e-01]    []  
9000      [1.03e-03, 1.15e-03, 4.69e-06, 2.23e+00, 2.23e+00]    [1.03e-03, 1.15e-03, 4.69e-06, 2.23e+00, 2.23e+00]    []  
34000     [1.68e-04, 2.70e-04, 2.78e-06, 1.38e-01, 1.38e-01]    [1.68e-04, 2.70e-04, 2.78e-06, 1.38e-01, 1.38e-01]    []  
30000     [1.04e-03, 3.64e-04, 1.12e-04, 2.46e-01, 2.46e-01]    [1.04e-03, 3.64e-04, 1.12e-04, 2.46e-01, 2.46e-01]    []  
46000     [1.54e-04, 2.43e-05, 1.27e-06, 3.88e-01, 3.88e-01]    [1.54e-04, 2.43e-05, 1.27e-06, 3.88e-01, 3.88e-01]    []  
39000     [2.52e-04, 4.23e-04, 4.85e-06, 3.84e-01, 3.84e-01]    [2.52e-04, 4.23e-04, 4.85e-06, 3.84e-01, 3.84e-01]    []  
38000     [9.24e-04, 7.06e-05, 4.23e-05, 4.01e-01, 4.01e-01]    [9.24e-04, 7.06e-05, 4.23e-05, 4.01e-01, 4.01e-01]    []  
33000     [1.13e-04, 8.57e-05, 4.96e-06, 2.86e-01, 2.86e-01]    [1.13e-04, 8.57e-05, 4.96e-06, 2.86e-01, 2.86e-01]    []  
35000     [1.71e-04, 2.49e-04, 2.14e-06, 1.35e-01, 1.35e-01]    [1.71e-04, 2.49e-04, 2.14e-06, 1.35e-01, 1.35e-01]    []  
10000     [8.67e-04, 1.14e-03, 1.93e-06, 1.18e+00, 1.18e+00]    [8.67e-04, 1.14e-03, 1.93e-06, 1.18e+00, 1.18e+00]    []  
31000     [1.02e-03, 3.63e-04, 2.70e-04, 2.32e-01, 2.32e-01]    [1.02e-03, 3.63e-04, 2.70e-04, 2.32e-01, 2.32e-01]    []  
47000     [1.76e-04, 2.53e-05, 2.53e-07, 3.30e-01, 3.30e-01]    [1.76e-04, 2.53e-05, 2.53e-07, 3.30e-01, 3.30e-01]    []  
40000     [3.54e-04, 4.14e-04, 8.94e-06, 3.93e-01, 3.93e-01]    [3.54e-04, 4.14e-04, 8.94e-06, 3.93e-01, 3.93e-01]    []  
                                                                                                                           39000     [1.12e-03, 7.59e-05, 2.76e-04, 3.88e-01, 3.88e-01]    [1.12e-03, 7.59e-05, 2.76e-04, 3.88e-01, 3.88e-01]    []  
34000     [1.33e-04, 8.88e-05, 1.09e-05, 4.83e-01, 4.83e-01]    [1.33e-04, 8.88e-05, 1.09e-05, 4.83e-01, 4.83e-01]    []  
36000     [1.66e-04, 2.48e-04, 5.74e-06, 2.90e-01, 2.90e-01]    [1.66e-04, 2.48e-04, 5.74e-06, 2.90e-01, 2.90e-01]    []  
48000     [1.23e-04, 2.57e-05, 1.75e-07, 2.17e-01, 2.17e-01]    [1.23e-04, 2.57e-05, 1.75e-07, 2.17e-01, 2.17e-01]    []  
32000     [1.01e-03, 3.36e-04, 3.35e-04, 2.56e-01, 2.56e-01]    [1.01e-03, 3.36e-04, 3.35e-04, 2.56e-01, 2.56e-01]    []  
11000     [8.22e-04, 1.10e-03, 7.37e-06, 1.28e+00, 1.28e+00]    [8.22e-04, 1.10e-03, 7.37e-06, 1.28e+00, 1.28e+00]    []  
40000     [1.72e-03, 7.47e-05, 2.64e-04, 6.37e-01, 6.37e-01]    [1.72e-03, 7.47e-05, 2.64e-04, 6.37e-01, 6.37e-01]    []  
35000     [1.03e-04, 9.10e-05, 1.65e-05, 4.07e-01, 4.07e-01]    [1.03e-04, 9.10e-05, 1.65e-05, 4.07e-01, 4.07e-01]    []  
41000     [2.55e-04, 4.02e-04, 4.62e-06, 3.53e-01, 3.53e-01]    [2.55e-04, 4.02e-04, 4.62e-06, 3.53e-01, 3.53e-01]    []  
                                                                                                                           37000     [1.65e-04, 2.30e-04, 2.32e-06, 2.13e-01, 2.13e-01]    [1.65e-04, 2.30e-04, 2.32e-06, 2.13e-01, 2.13e-01]    []  
49000     [1.43e-04, 2.61e-05, 2.18e-07, 2.12e-01, 2.12e-01]    [1.43e-04, 2.61e-05, 2.18e-07, 2.12e-01, 2.12e-01]    []  
33000     [9.63e-04, 3.22e-04, 1.15e-04, 1.78e-01, 1.78e-01]    [9.63e-04, 3.22e-04, 1.15e-04, 1.78e-01, 1.78e-01]    []  
36000     [1.91e-04, 9.17e-05, 5.19e-05, 8.88e-01, 8.88e-01]    [1.91e-04, 9.17e-05, 5.19e-05, 8.88e-01, 8.88e-01]    []  
12000     [7.21e-04, 1.15e-03, 5.18e-06, 1.57e+00, 1.57e+00]    [7.21e-04, 1.15e-03, 5.18e-06, 1.57e+00, 1.57e+00]    []  
41000     [9.04e-04, 7.11e-05, 3.44e-05, 2.56e-01, 2.56e-01]    [9.04e-04, 7.11e-05, 3.44e-05, 2.56e-01, 2.56e-01]    []  
42000     [3.11e-04, 3.95e-04, 1.55e-05, 3.73e-01, 3.73e-01]    [3.11e-04, 3.95e-04, 1.55e-05, 3.73e-01, 3.73e-01]    []  
38000     [1.60e-04, 2.32e-04, 7.42e-07, 1.34e-01, 1.34e-01]    [1.60e-04, 2.32e-04, 7.42e-07, 1.34e-01, 1.34e-01]    []  
50000     [1.20e-04, 2.65e-05, 1.14e-07, 2.00e-01, 2.00e-01]    [1.20e-04, 2.65e-05, 1.14e-07, 2.00e-01, 2.00e-01]    []  

Best model at step 50000:
  train loss: 4.01e-01
  test loss: 4.01e-01
  test metric: []

'train' took 10517.916823 s

[I 2023-10-09 02:45:56,396] Trial 23 finished with value: 0.6730810037934841 and parameters: {'num_domain': 65980, 'num_boundary': 105, 'resampling_period': 421, 'lr': 0.00018121199486695967}. Best is trial 0 with value: 0.4913720930752669.
34000     [7.13e-04, 3.08e-04, 1.80e-04, 1.77e-01, 1.77e-01]    [7.13e-04, 3.08e-04, 1.80e-04, 1.77e-01, 1.77e-01]    []  
37000     [1.16e-04, 9.63e-05, 9.59e-06, 1.52e-01, 1.52e-01]    [1.16e-04, 9.63e-05, 9.59e-06, 1.52e-01, 1.52e-01]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000115 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [6.87e+01, 1.85e-04, 2.20e-07, 3.63e+03, 3.63e+03]    [6.87e+01, 1.85e-04, 2.20e-07, 3.63e+03, 3.63e+03]    []  
42000     [7.93e-04, 6.30e-05, 1.87e-05, 2.14e-01, 2.14e-01]    [7.93e-04, 6.30e-05, 1.87e-05, 2.14e-01, 2.14e-01]    []  
43000     [2.56e-04, 3.86e-04, 1.09e-05, 3.39e-01, 3.39e-01]    [2.56e-04, 3.86e-04, 1.09e-05, 3.39e-01, 3.39e-01]    []  
13000     [5.93e-04, 1.12e-03, 2.56e-05, 7.42e-01, 7.42e-01]    [5.93e-04, 1.12e-03, 2.56e-05, 7.42e-01, 7.42e-01]    []  
38000     [1.52e-04, 1.00e-04, 6.99e-06, 1.63e-01, 1.63e-01]    [1.52e-04, 1.00e-04, 6.99e-06, 1.63e-01, 1.63e-01]    []  
39000     [1.29e-04, 2.12e-04, 3.36e-06, 1.07e-01, 1.07e-01]    [1.29e-04, 2.12e-04, 3.36e-06, 1.07e-01, 1.07e-01]    []  
35000     [8.18e-04, 3.02e-04, 1.37e-04, 1.70e-01, 1.70e-01]    [8.18e-04, 3.02e-04, 1.37e-04, 1.70e-01, 1.70e-01]    []  
1000      [2.54e-10, 1.05e-03, 5.02e-16, 2.09e+03, 2.09e+03]    [2.54e-10, 1.05e-03, 5.02e-16, 2.09e+03, 2.09e+03]    []  
43000     [7.67e-04, 6.53e-05, 4.93e-05, 3.98e-01, 3.98e-01]    [7.67e-04, 6.53e-05, 4.93e-05, 3.98e-01, 3.98e-01]    []  
44000     [5.26e-04, 3.76e-04, 8.34e-06, 3.77e-01, 3.77e-01]    [5.26e-04, 3.76e-04, 8.34e-06, 3.77e-01, 3.77e-01]    []  
                                                                                                                           14000     [4.90e-04, 1.08e-03, 3.66e-06, 1.49e+00, 1.49e+00]    [4.90e-04, 1.08e-03, 3.66e-06, 1.49e+00, 1.49e+00]    []  
39000     [1.51e-04, 1.02e-04, 1.49e-05, 7.79e-01, 7.79e-01]    [1.51e-04, 1.02e-04, 1.49e-05, 7.79e-01, 7.79e-01]    []  
40000     [1.34e-04, 2.23e-04, 1.13e-06, 1.34e-01, 1.34e-01]    [1.34e-04, 2.23e-04, 1.13e-06, 1.34e-01, 1.34e-01]    []  
36000     [1.18e-03, 2.91e-04, 2.36e-04, 3.16e-01, 3.16e-01]    [1.18e-03, 2.91e-04, 2.36e-04, 3.16e-01, 3.16e-01]    []  
44000     [5.19e-04, 6.32e-05, 1.71e-05, 2.12e-01, 2.12e-01]    [5.19e-04, 6.32e-05, 1.71e-05, 2.12e-01, 2.12e-01]    []  
2000      [9.34e-11, 1.05e-03, 4.02e-16, 2.09e+03, 2.09e+03]    [9.34e-11, 1.05e-03, 4.02e-16, 2.09e+03, 2.09e+03]    []  
40000     [1.86e-04, 1.18e-04, 4.46e-05, 1.63e-01, 1.63e-01]    [1.86e-04, 1.18e-04, 4.46e-05, 1.63e-01, 1.63e-01]    []  
45000     [3.78e-04, 3.69e-04, 2.89e-05, 2.94e-01, 2.94e-01]    [3.78e-04, 3.69e-04, 2.89e-05, 2.94e-01, 2.94e-01]    []  
15000     [4.33e-04, 1.03e-03, 1.67e-06, 7.85e-01, 7.85e-01]    [4.33e-04, 1.03e-03, 1.67e-06, 7.85e-01, 7.85e-01]    []  
41000     [1.59e-04, 2.26e-04, 4.56e-06, 3.08e-01, 3.08e-01]    [1.59e-04, 2.26e-04, 4.56e-06, 3.08e-01, 3.08e-01]    []  
37000     [8.85e-04, 3.07e-04, 3.39e-04, 1.75e-01, 1.75e-01]    [8.85e-04, 3.07e-04, 3.39e-04, 1.75e-01, 1.75e-01]    []  
45000     [7.39e-04, 5.49e-05, 2.60e-05, 1.94e-01, 1.94e-01]    [7.39e-04, 5.49e-05, 2.60e-05, 1.94e-01, 1.94e-01]    []  
41000     [2.13e-04, 1.16e-04, 2.33e-05, 2.20e-01, 2.20e-01]    [2.13e-04, 1.16e-04, 2.33e-05, 2.20e-01, 2.20e-01]    []  
3000      [2.54e-02, 2.61e-04, 2.19e-04, 2.50e+01, 2.50e+01]    [2.54e-02, 2.61e-04, 2.19e-04, 2.50e+01, 2.50e+01]    []  
46000     [3.75e-04, 3.66e-04, 5.12e-06, 2.75e-01, 2.75e-01]    [3.75e-04, 3.66e-04, 5.12e-06, 2.75e-01, 2.75e-01]    []  
42000     [1.96e-04, 2.27e-04, 2.97e-06, 2.35e-01, 2.35e-01]    [1.96e-04, 2.27e-04, 2.97e-06, 2.35e-01, 2.35e-01]    []  
16000     [3.92e-04, 9.62e-04, 3.29e-06, 4.60e-01, 4.60e-01]    [3.92e-04, 9.62e-04, 3.29e-06, 4.60e-01, 4.60e-01]    []  
38000     [8.18e-04, 3.07e-04, 2.47e-04, 2.65e-01, 2.65e-01]    [8.18e-04, 3.07e-04, 2.47e-04, 2.65e-01, 2.65e-01]    []  
42000     [1.14e-04, 1.19e-04, 9.66e-06, 1.28e-01, 1.28e-01]    [1.14e-04, 1.19e-04, 9.66e-06, 1.28e-01, 1.28e-01]    []  
46000     [7.89e-04, 6.24e-05, 1.66e-05, 1.95e-01, 1.95e-01]    [7.89e-04, 6.24e-05, 1.66e-05, 1.95e-01, 1.95e-01]    []  
47000     [2.08e-04, 3.59e-04, 5.66e-06, 2.67e-01, 2.67e-01]    [2.08e-04, 3.59e-04, 5.66e-06, 2.67e-01, 2.67e-01]    []  
4000      [3.21e-03, 4.74e-04, 5.89e-05, 9.32e+00, 9.32e+00]    [3.21e-03, 4.74e-04, 5.89e-05, 9.32e+00, 9.32e+00]    []  
43000     [1.31e-04, 2.25e-04, 4.70e-06, 8.86e-02, 8.86e-02]    [1.31e-04, 2.25e-04, 4.70e-06, 8.86e-02, 8.86e-02]    []  
39000     [9.82e-04, 3.02e-04, 5.80e-05, 2.31e-01, 2.31e-01]    [9.82e-04, 3.02e-04, 5.80e-05, 2.31e-01, 2.31e-01]    []  
43000     [1.51e-04, 1.21e-04, 1.51e-05, 5.62e-01, 5.62e-01]    [1.51e-04, 1.21e-04, 1.51e-05, 5.62e-01, 5.62e-01]    []  
17000     [3.55e-04, 8.52e-04, 1.63e-06, 4.93e-01, 4.93e-01]    [3.55e-04, 8.52e-04, 1.63e-06, 4.93e-01, 4.93e-01]    []  
47000     [5.13e-04, 5.85e-05, 2.23e-05, 1.98e-01, 1.98e-01]    [5.13e-04, 5.85e-05, 2.23e-05, 1.98e-01, 1.98e-01]    []  
                                                                                                                                                                                                                                                      48000     [2.04e-04, 3.54e-04, 5.03e-06, 2.53e-01, 2.53e-01]    [2.04e-04, 3.54e-04, 5.03e-06, 2.53e-01, 2.53e-01]    []  
5000      [2.22e-03, 8.86e-04, 2.42e-05, 8.80e+00, 8.80e+00]    [2.22e-03, 8.86e-04, 2.42e-05, 8.80e+00, 8.80e+00]    []  
44000     [1.37e-04, 2.24e-04, 3.39e-06, 8.49e-02, 8.49e-02]    [1.37e-04, 2.24e-04, 3.39e-06, 8.49e-02, 8.49e-02]    []  
44000     [1.27e-04, 1.25e-04, 9.00e-06, 1.04e-01, 1.04e-01]    [1.27e-04, 1.25e-04, 9.00e-06, 1.04e-01, 1.04e-01]    []  
40000     [7.60e-04, 3.03e-04, 1.60e-04, 1.49e-01, 1.49e-01]    [7.60e-04, 3.03e-04, 1.60e-04, 1.49e-01, 1.49e-01]    []  
48000     [6.29e-04, 5.74e-05, 1.58e-05, 1.75e-01, 1.75e-01]    [6.29e-04, 5.74e-05, 1.58e-05, 1.75e-01, 1.75e-01]    []  
18000     [3.43e-04, 7.26e-04, 5.25e-06, 8.47e-01, 8.47e-01]    [3.43e-04, 7.26e-04, 5.25e-06, 8.47e-01, 8.47e-01]    []  
45000     [1.00e-04, 1.26e-04, 7.25e-06, 9.31e-02, 9.31e-02]    [1.00e-04, 1.26e-04, 7.25e-06, 9.31e-02, 9.31e-02]    []  
49000     [1.44e-03, 3.45e-04, 8.62e-06, 2.66e-01, 2.66e-01]    [1.44e-03, 3.45e-04, 8.62e-06, 2.66e-01, 2.66e-01]    []  
45000     [1.94e-04, 2.23e-04, 7.88e-05, 5.70e-01, 5.70e-01]    [1.94e-04, 2.23e-04, 7.88e-05, 5.70e-01, 5.70e-01]    []  
6000      [1.62e-03, 1.24e-03, 2.10e-06, 4.31e+00, 4.31e+00]    [1.62e-03, 1.24e-03, 2.10e-06, 4.31e+00, 4.31e+00]    []  
41000     [9.24e-04, 3.36e-04, 2.67e-04, 2.30e-01, 2.30e-01]    [9.24e-04, 3.36e-04, 2.67e-04, 2.30e-01, 2.30e-01]    []  
21000     [6.32e-04, 6.23e-04, 4.61e-07, 6.20e-01, 6.20e-01]    [6.32e-04, 6.23e-04, 4.61e-07, 6.20e-01, 6.20e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                 50000     [2.92e-04, 3.46e-04, 5.68e-06, 3.19e-01, 3.19e-01]    [2.92e-04, 3.46e-04, 5.68e-06, 3.19e-01, 3.19e-01]    []  

Best model at step 48000:
  train loss: 5.07e-01
  test loss: 5.07e-01
  test metric: []

'train' took 11149.829751 s

[I 2023-10-09 03:13:37,897] Trial 24 finished with value: 0.7645305463554036 and parameters: {'num_domain': 62736, 'num_boundary': 6745, 'resampling_period': 695, 'lr': 0.0001781585587769306}. Best is trial 0 with value: 0.4913720930752669.
46000     [2.15e-04, 2.26e-04, 1.10e-04, 7.84e-02, 7.84e-02]    [2.15e-04, 2.26e-04, 1.10e-04, 7.84e-02, 7.84e-02]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000103 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.54e+02, 2.48e-03, 1.28e-07, 3.62e+03, 3.62e+03]    [1.54e+02, 2.48e-03, 1.28e-07, 3.62e+03, 3.62e+03]    []  
[W 2023-10-09 03:13:52,027] Trial 32 failed with parameters: {'num_domain': 98630, 'num_boundary': 7083, 'resampling_period': 8798, 'lr': 0.0007843597141848498} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 102.00 MiB (GPU 3; 10.75 GiB total capacity; 9.87 GiB already allocated; 31.62 MiB free; 9.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 335, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 303, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 291, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/pde.py", line 127, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 92, in pde2d_vm
    dw_dt = dde.grad.jacobian(y, x, i=1, j=2)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 181, in jacobian
    return jacobian._Jacobians(ys, xs, i=i, j=j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 153, in __call__
    return self.Js[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 3; 10.75 GiB total capacity; 9.87 GiB already allocated; 31.62 MiB free; 9.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-09 03:13:52,058] Trial 32 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 80, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 335, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 303, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 291, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/pde.py", line 127, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 92, in pde2d_vm
    dw_dt = dde.grad.jacobian(y, x, i=1, j=2)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 181, in jacobian
    return jacobian._Jacobians(ys, xs, i=i, j=j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 153, in __call__
    return self.Js[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 3; 10.75 GiB total capacity; 9.87 GiB already allocated; 31.62 MiB free; 9.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
42000     [8.89e-04, 3.11e-04, 2.20e-04, 1.35e-01, 1.35e-01]    [8.89e-04, 3.11e-04, 2.20e-04, 1.35e-01, 1.35e-01]    []  
7000      [1.44e-03, 8.27e-04, 5.49e-06, 2.85e+00, 2.85e+00]    [1.44e-03, 8.27e-04, 5.49e-06, 2.85e+00, 2.85e+00]    []  
50000     [6.80e-04, 5.31e-05, 1.71e-05, 1.77e-01, 1.77e-01]    [6.80e-04, 5.31e-05, 1.71e-05, 1.77e-01, 1.77e-01]    []  

Best model at step 48000:
  train loss: 3.51e-01
  test loss: 3.51e-01
  test metric: []

'train' took 10596.550742 s

[I 2023-10-09 03:15:23,037] Trial 25 finished with value: 0.6489061528687999 and parameters: {'num_domain': 52545, 'num_boundary': 7051, 'resampling_period': 566, 'lr': 0.00019988881081725972}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000105 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.94e+01, 9.61e-05, 1.31e-07, 3.60e+03, 3.60e+03]    [4.94e+01, 9.61e-05, 1.31e-07, 3.60e+03, 3.60e+03]    []  
47000     [8.26e-05, 1.31e-04, 1.00e-05, 8.31e-02, 8.31e-02]    [8.26e-05, 1.31e-04, 1.00e-05, 8.31e-02, 8.31e-02]    []  
20000     [3.06e-04, 4.92e-04, 4.46e-06, 4.21e-01, 4.21e-01]    [3.06e-04, 4.92e-04, 4.46e-06, 4.21e-01, 4.21e-01]    []  
47000     [1.52e-04, 2.17e-04, 7.80e-06, 7.48e-02, 7.48e-02]    [1.52e-04, 2.17e-04, 7.80e-06, 7.48e-02, 7.48e-02]    []  
43000     [9.31e-04, 3.10e-04, 1.25e-04, 1.23e-01, 1.23e-01]    [9.31e-04, 3.10e-04, 1.25e-04, 1.23e-01, 1.23e-01]    []  
48000     [1.18e-04, 1.30e-04, 1.10e-05, 1.76e-01, 1.76e-01]    [1.18e-04, 1.30e-04, 1.10e-05, 1.76e-01, 1.76e-01]    []  
8000      [1.16e-03, 1.03e-03, 2.22e-06, 3.46e+00, 3.46e+00]    [1.16e-03, 1.03e-03, 2.22e-06, 3.46e+00, 3.46e+00]    []  
1000      [1.31e-09, 1.05e-03, 4.50e-16, 2.09e+03, 2.09e+03]    [1.31e-09, 1.05e-03, 4.50e-16, 2.09e+03, 2.09e+03]    []  
21000     [2.36e-04, 4.11e-04, 2.28e-06, 2.89e-01, 2.89e-01]    [2.36e-04, 4.11e-04, 2.28e-06, 2.89e-01, 2.89e-01]    []  
48000     [1.58e-04, 2.18e-04, 8.04e-06, 7.37e-02, 7.37e-02]    [1.58e-04, 2.18e-04, 8.04e-06, 7.37e-02, 7.37e-02]    []  
44000     [6.45e-04, 2.76e-04, 1.17e-04, 2.62e-01, 2.62e-01]    [6.45e-04, 2.76e-04, 1.17e-04, 2.62e-01, 2.62e-01]    []  
49000     [1.15e-04, 1.30e-04, 5.46e-06, 9.67e-02, 9.67e-02]    [1.15e-04, 1.30e-04, 5.46e-06, 9.67e-02, 9.67e-02]    []  
9000      [1.01e-03, 1.29e-03, 2.16e-06, 1.79e+00, 1.79e+00]    [1.01e-03, 1.29e-03, 2.16e-06, 1.79e+00, 1.79e+00]    []  
2000      [5.96e-02, 1.05e-03, 6.71e-13, 1.81e+03, 1.81e+03]    [5.96e-02, 1.05e-03, 6.71e-13, 1.81e+03, 1.81e+03]    []  
49000     [1.45e-04, 2.11e-04, 3.91e-06, 8.86e-02, 8.86e-02]    [1.45e-04, 2.11e-04, 3.91e-06, 8.86e-02, 8.86e-02]    []  
50000     [2.10e-04, 1.21e-04, 1.92e-05, 5.45e-01, 5.45e-01]    [2.10e-04, 1.21e-04, 1.92e-05, 5.45e-01, 5.45e-01]    []  

Best model at step 47000:
  train loss: 1.66e-01
  test loss: 1.66e-01
  test metric: []

'train' took 8939.542728 s

[I 2023-10-09 03:24:45,638] Trial 28 finished with value: 0.8656658280027814 and parameters: {'num_domain': 24707, 'num_boundary': 7043, 'resampling_period': 7699, 'lr': 0.0005331157329263377}. Best is trial 0 with value: 0.4913720930752669.
45000     [6.96e-04, 2.61e-04, 1.55e-04, 1.17e-01, 1.17e-01]    [6.96e-04, 2.61e-04, 1.55e-04, 1.17e-01, 1.17e-01]    []  
22000     [2.47e-04, 3.78e-04, 9.05e-06, 2.76e-01, 2.76e-01]    [2.47e-04, 3.78e-04, 9.05e-06, 2.76e-01, 2.76e-01]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000101 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [6.38e+01, 2.86e-04, 1.34e-07, 3.62e+03, 3.62e+03]    [6.38e+01, 2.86e-04, 1.34e-07, 3.62e+03, 3.62e+03]    []  
[W 2023-10-09 03:24:58,432] Trial 34 failed with parameters: {'num_domain': 83860, 'num_boundary': 9954, 'resampling_period': 10842, 'lr': 0.0006507520889507765} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 10.75 GiB total capacity; 9.64 GiB already allocated; 71.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 10.75 GiB total capacity; 9.64 GiB already allocated; 71.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-09 03:24:58,441] Trial 34 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 80, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 10.75 GiB total capacity; 9.64 GiB already allocated; 71.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
24000     [4.20e-04, 3.93e-04, 5.57e-07, 4.00e-01, 4.00e-01]    [4.20e-04, 3.93e-04, 5.57e-07, 4.00e-01, 4.00e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    29000     [5.80e-04, 4.94e-04, 4.58e-06, 5.44e-01, 5.44e-01]    [5.80e-04, 4.94e-04, 4.58e-06, 5.44e-01, 5.44e-01]    []  
                                                                                                                                                                                                                                                      25000     [3.67e-04, 3.10e-04, 2.13e-06, 4.74e-01, 4.74e-01]    [3.67e-04, 3.10e-04, 2.13e-06, 4.74e-01, 4.74e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        26000     [3.20e-04, 2.19e-04, 5.23e-07, 3.14e-01, 3.14e-01]    [3.20e-04, 2.19e-04, 5.23e-07, 3.14e-01, 3.14e-01]    []  
30000     [5.24e-04, 4.21e-04, 5.51e-06, 7.76e-01, 7.76e-01]    [5.24e-04, 4.21e-04, 5.51e-06, 7.76e-01, 7.76e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       27000     [2.84e-04, 1.75e-04, 5.74e-07, 3.46e-01, 3.46e-01]    [2.84e-04, 1.75e-04, 5.74e-07, 3.46e-01, 3.46e-01]    []  
6000      [6.79e-04, 1.04e-03, 1.43e-05, 6.17e+00, 6.17e+00]    [6.79e-04, 1.04e-03, 1.43e-05, 6.17e+00, 6.17e+00]    []  
26000     [2.35e-04, 1.82e-04, 1.53e-05, 4.82e-01, 4.82e-01]    [2.35e-04, 1.82e-04, 1.53e-05, 4.82e-01, 4.82e-01]    []  
4000      [4.69e-03, 6.36e-04, 7.37e-04, 1.18e+01, 1.18e+01]    [4.69e-03, 6.36e-04, 7.37e-04, 1.18e+01, 1.18e+01]    []  
50000     [7.90e-04, 2.20e-04, 1.30e-04, 1.91e-01, 1.91e-01]    [7.90e-04, 2.20e-04, 1.30e-04, 1.91e-01, 1.91e-01]    []  

Best model at step 45000:
  train loss: 2.36e-01
  test loss: 2.36e-01
  test metric: []

'train' took 10588.039621 s

[I 2023-10-09 03:42:27,811] Trial 27 finished with value: 0.6220387094790877 and parameters: {'num_domain': 52516, 'num_boundary': 7102, 'resampling_period': 7655, 'lr': 0.00047804456156208026}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000176 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [8.46e+00, 2.09e-04, 8.11e-08, 3.62e+03, 3.62e+03]    [8.46e+00, 2.09e-04, 8.11e-08, 3.62e+03, 3.62e+03]    []  
14000     [3.66e-04, 4.70e-05, 1.08e-05, 6.55e-01, 6.55e-01]    [3.66e-04, 4.70e-05, 1.08e-05, 6.55e-01, 6.55e-01]    []  
5000      [1.12e-03, 9.90e-04, 7.41e-06, 9.11e+00, 9.11e+00]    [1.12e-03, 9.90e-04, 7.41e-06, 9.11e+00, 9.11e+00]    []  
7000      [9.51e-04, 3.45e-04, 9.85e-05, 5.27e+00, 5.27e+00]    [9.51e-04, 3.45e-04, 9.85e-05, 5.27e+00, 5.27e+00]    []  
1000      [8.65e-11, 1.05e-03, 1.96e-16, 2.09e+03, 2.09e+03]    [8.65e-11, 1.05e-03, 1.96e-16, 2.09e+03, 2.09e+03]    []  
27000     [2.31e-04, 1.49e-04, 1.07e-05, 3.45e-01, 3.45e-01]    [2.31e-04, 1.49e-04, 1.07e-05, 3.45e-01, 3.45e-01]    []  
15000     [3.63e-04, 4.34e-05, 2.79e-05, 1.42e+00, 1.42e+00]    [3.63e-04, 4.34e-05, 2.79e-05, 1.42e+00, 1.42e+00]    []  
2000      [4.77e-03, 8.90e-04, 2.63e-04, 9.77e+00, 9.77e+00]    [4.77e-03, 8.90e-04, 2.63e-04, 9.77e+00, 9.77e+00]    []  
6000      [8.16e-04, 9.66e-04, 3.35e-05, 7.32e+00, 7.32e+00]    [8.16e-04, 9.66e-04, 3.35e-05, 7.32e+00, 7.32e+00]    []  
8000      [8.49e-04, 3.60e-04, 6.25e-06, 3.79e+00, 3.79e+00]    [8.49e-04, 3.60e-04, 6.25e-06, 3.79e+00, 3.79e+00]    []  
28000     [2.40e-04, 1.22e-04, 1.78e-05, 4.02e-01, 4.02e-01]    [2.40e-04, 1.22e-04, 1.78e-05, 4.02e-01, 4.02e-01]    []  
                                                                                                                                                                                                                                                      3000      [6.55e-04, 1.15e-03, 3.33e-05, 4.94e+00, 4.94e+00]    [6.55e-04, 1.15e-03, 3.33e-05, 4.94e+00, 4.94e+00]    []  
16000     [3.09e-04, 4.88e-05, 1.10e-05, 5.86e-01, 5.86e-01]    [3.09e-04, 4.88e-05, 1.10e-05, 5.86e-01, 5.86e-01]    []  
7000      [9.40e-04, 7.06e-04, 4.48e-05, 8.73e+00, 8.73e+00]    [9.40e-04, 7.06e-04, 4.48e-05, 8.73e+00, 8.73e+00]    []  
4000      [7.93e-04, 1.62e-03, 1.23e-05, 3.70e+00, 3.70e+00]    [7.93e-04, 1.62e-03, 1.23e-05, 3.70e+00, 3.70e+00]    []  
9000      [9.73e-04, 9.51e-04, 5.21e-05, 2.69e+00, 2.69e+00]    [9.73e-04, 9.51e-04, 5.21e-05, 2.69e+00, 2.69e+00]    []  
29000     [2.00e-04, 1.08e-04, 4.91e-06, 1.32e-01, 1.32e-01]    [2.00e-04, 1.08e-04, 4.91e-06, 1.32e-01, 1.32e-01]    []  
8000      [1.20e-03, 6.94e-04, 1.70e-05, 4.46e+00, 4.46e+00]    [1.20e-03, 6.94e-04, 1.70e-05, 4.46e+00, 4.46e+00]    []  
17000     [2.48e-04, 5.10e-05, 2.97e-05, 5.72e-01, 5.72e-01]    [2.48e-04, 5.10e-05, 2.97e-05, 5.72e-01, 5.72e-01]    []  
5000      [7.02e-04, 1.36e-03, 4.05e-05, 2.00e+00, 2.00e+00]    [7.02e-04, 1.36e-03, 4.05e-05, 2.00e+00, 2.00e+00]    []  
30000     [2.36e-04, 1.03e-04, 7.81e-06, 1.72e-01, 1.72e-01]    [2.36e-04, 1.03e-04, 7.81e-06, 1.72e-01, 1.72e-01]    []  
10000     [2.39e-03, 6.96e-04, 4.55e-05, 2.32e+00, 2.32e+00]    [2.39e-03, 6.96e-04, 4.55e-05, 2.32e+00, 2.32e+00]    []  
9000      [7.12e-04, 8.42e-04, 1.35e-05, 3.05e+00, 3.05e+00]    [7.12e-04, 8.42e-04, 1.35e-05, 3.05e+00, 3.05e+00]    []  
6000      [9.08e-04, 1.12e-03, 1.94e-05, 2.11e+00, 2.11e+00]    [9.08e-04, 1.12e-03, 1.94e-05, 2.11e+00, 2.11e+00]    []  
18000     [2.51e-04, 4.51e-05, 1.42e-05, 5.58e-01, 5.58e-01]    [2.51e-04, 4.51e-05, 1.42e-05, 5.58e-01, 5.58e-01]    []  
31000     [1.60e-04, 9.83e-05, 1.05e-06, 5.39e-01, 5.39e-01]    [1.60e-04, 9.83e-05, 1.05e-06, 5.39e-01, 5.39e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             32000     [1.43e-04, 9.60e-05, 5.46e-07, 2.07e-01, 2.07e-01]    [1.43e-04, 9.60e-05, 5.46e-07, 2.07e-01, 2.07e-01]    []  
                                                                                                                           34000     [3.39e-04, 3.39e-04, 1.39e-06, 3.98e-01, 3.98e-01]    [3.39e-04, 3.39e-04, 1.39e-06, 3.98e-01, 3.98e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       33000     [1.35e-04, 9.44e-05, 1.18e-06, 2.52e-01, 2.52e-01]    [1.35e-04, 9.44e-05, 1.18e-06, 2.52e-01, 2.52e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       35000     [3.64e-04, 3.40e-04, 3.24e-06, 3.92e-01, 3.92e-01]    [3.64e-04, 3.40e-04, 3.24e-06, 3.92e-01, 3.92e-01]    []  
34000     [1.24e-04, 8.33e-05, 1.99e-06, 1.80e-01, 1.80e-01]    [1.24e-04, 8.33e-05, 1.99e-06, 1.80e-01, 1.80e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  35000     [1.14e-04, 8.01e-05, 1.20e-06, 2.28e-01, 2.28e-01]    [1.14e-04, 8.01e-05, 1.20e-06, 2.28e-01, 2.28e-01]    []  
14000     [3.46e-04, 2.85e-04, 6.34e-06, 3.94e-01, 3.94e-01]    [3.46e-04, 2.85e-04, 6.34e-06, 3.94e-01, 3.94e-01]    []  
16000     [6.04e-04, 5.25e-04, 5.12e-05, 6.29e-01, 6.29e-01]    [6.04e-04, 5.25e-04, 5.12e-05, 6.29e-01, 6.29e-01]    []  
36000     [1.59e-04, 9.23e-05, 6.34e-06, 8.06e-02, 8.06e-02]    [1.59e-04, 9.23e-05, 6.34e-06, 8.06e-02, 8.06e-02]    []  
16000     [9.16e-04, 8.11e-04, 6.36e-05, 1.19e+00, 1.19e+00]    [9.16e-04, 8.11e-04, 6.36e-05, 1.19e+00, 1.19e+00]    []  
24000     [1.75e-04, 4.87e-05, 8.10e-06, 2.08e-01, 2.08e-01]    [1.75e-04, 4.87e-05, 8.10e-06, 2.08e-01, 2.08e-01]    []  
15000     [2.96e-04, 2.52e-04, 3.03e-06, 1.82e-01, 1.82e-01]    [2.96e-04, 2.52e-04, 3.03e-06, 1.82e-01, 1.82e-01]    []  
17000     [5.68e-04, 4.52e-04, 5.86e-06, 7.83e-01, 7.83e-01]    [5.68e-04, 4.52e-04, 5.86e-06, 7.83e-01, 7.83e-01]    []  
                                                                                                                           37000     [1.57e-04, 9.19e-05, 1.23e-05, 7.93e-02, 7.93e-02]    [1.57e-04, 9.19e-05, 1.23e-05, 7.93e-02, 7.93e-02]    []  
16000     [2.22e-04, 2.16e-04, 8.11e-06, 1.60e-01, 1.60e-01]    [2.22e-04, 2.16e-04, 8.11e-06, 1.60e-01, 1.60e-01]    []  
17000     [8.06e-04, 7.55e-04, 4.96e-05, 6.12e-01, 6.12e-01]    [8.06e-04, 7.55e-04, 4.96e-05, 6.12e-01, 6.12e-01]    []  
25000     [1.51e-04, 4.91e-05, 8.52e-06, 2.85e-01, 2.85e-01]    [1.51e-04, 4.91e-05, 8.52e-06, 2.85e-01, 2.85e-01]    []  
18000     [4.59e-04, 3.84e-04, 2.45e-05, 6.78e-01, 6.78e-01]    [4.59e-04, 3.84e-04, 2.45e-05, 6.78e-01, 6.78e-01]    []  
17000     [2.75e-04, 1.61e-04, 1.29e-05, 4.11e-01, 4.11e-01]    [2.75e-04, 1.61e-04, 1.29e-05, 4.11e-01, 4.11e-01]    []  
38000     [1.50e-04, 7.26e-05, 7.81e-06, 7.37e-02, 7.37e-02]    [1.50e-04, 7.26e-05, 7.81e-06, 7.37e-02, 7.37e-02]    []  
18000     [6.51e-04, 7.53e-04, 9.58e-05, 4.81e-01, 4.81e-01]    [6.51e-04, 7.53e-04, 9.58e-05, 4.81e-01, 4.81e-01]    []  
26000     [1.31e-04, 4.97e-05, 5.12e-06, 2.21e-01, 2.21e-01]    [1.31e-04, 4.97e-05, 5.12e-06, 2.21e-01, 2.21e-01]    []  
19000     [3.89e-04, 3.39e-04, 6.61e-06, 4.33e-01, 4.33e-01]    [3.89e-04, 3.39e-04, 6.61e-06, 4.33e-01, 4.33e-01]    []  
18000     [1.61e-04, 1.69e-04, 1.14e-05, 2.82e-01, 2.82e-01]    [1.61e-04, 1.69e-04, 1.14e-05, 2.82e-01, 2.82e-01]    []  
39000     [1.50e-04, 6.53e-05, 1.01e-05, 8.66e-02, 8.66e-02]    [1.50e-04, 6.53e-05, 1.01e-05, 8.66e-02, 8.66e-02]    []  
20000     [3.90e-04, 3.06e-04, 6.88e-06, 5.23e-01, 5.23e-01]    [3.90e-04, 3.06e-04, 6.88e-06, 5.23e-01, 5.23e-01]    []  
19000     [1.87e-04, 1.70e-04, 4.52e-06, 1.61e-01, 1.61e-01]    [1.87e-04, 1.70e-04, 4.52e-06, 1.61e-01, 1.61e-01]    []  
19000     [5.47e-04, 7.64e-04, 3.75e-05, 5.82e-01, 5.82e-01]    [5.47e-04, 7.64e-04, 3.75e-05, 5.82e-01, 5.82e-01]    []  
27000     [1.42e-04, 5.22e-05, 1.17e-05, 3.41e-01, 3.41e-01]    [1.42e-04, 5.22e-05, 1.17e-05, 3.41e-01, 3.41e-01]    []  
20000     [2.17e-04, 1.94e-04, 9.15e-06, 1.07e-01, 1.07e-01]    [2.17e-04, 1.94e-04, 9.15e-06, 1.07e-01, 1.07e-01]    []  
40000     [2.87e-04, 6.55e-05, 1.40e-05, 1.46e-01, 1.46e-01]    [2.87e-04, 6.55e-05, 1.40e-05, 1.46e-01, 1.46e-01]    []  
21000     [3.80e-04, 3.02e-04, 5.55e-05, 4.13e-01, 4.13e-01]    [3.80e-04, 3.02e-04, 5.55e-05, 4.13e-01, 4.13e-01]    []  
28000     [2.05e-04, 5.31e-05, 1.75e-05, 9.74e-01, 9.74e-01]    [2.05e-04, 5.31e-05, 1.75e-05, 9.74e-01, 9.74e-01]    []  
20000     [5.55e-04, 7.38e-04, 4.52e-05, 4.41e-01, 4.41e-01]    [5.55e-04, 7.38e-04, 4.52e-05, 4.41e-01, 4.41e-01]    []  
                                                                                                                           21000     [2.42e-04, 1.67e-04, 4.99e-05, 1.78e+00, 1.78e+00]    [2.42e-04, 1.67e-04, 4.99e-05, 1.78e+00, 1.78e+00]    []  
22000     [4.02e-04, 2.81e-04, 2.42e-05, 6.36e-01, 6.36e-01]    [4.02e-04, 2.81e-04, 2.42e-05, 6.36e-01, 6.36e-01]    []  
41000     [1.32e-04, 6.20e-05, 8.91e-06, 1.38e-01, 1.38e-01]    [1.32e-04, 6.20e-05, 8.91e-06, 1.38e-01, 1.38e-01]    []  
22000     [2.05e-04, 1.66e-04, 4.60e-06, 2.59e+00, 2.59e+00]    [2.05e-04, 1.66e-04, 4.60e-06, 2.59e+00, 2.59e+00]    []  
29000     [1.31e-04, 5.24e-05, 4.29e-06, 1.38e-01, 1.38e-01]    [1.31e-04, 5.24e-05, 4.29e-06, 1.38e-01, 1.38e-01]    []  
21000     [4.17e-04, 6.68e-04, 8.29e-05, 9.31e-01, 9.31e-01]    [4.17e-04, 6.68e-04, 8.29e-05, 9.31e-01, 9.31e-01]    []  
23000     [3.50e-04, 2.67e-04, 9.48e-06, 5.14e-01, 5.14e-01]    [3.50e-04, 2.67e-04, 9.48e-06, 5.14e-01, 5.14e-01]    []  
                                                                                                                                                                                                                                                      23000     [2.01e-04, 2.06e-04, 1.40e-05, 1.90e-01, 1.90e-01]    [2.01e-04, 2.06e-04, 1.40e-05, 1.90e-01, 1.90e-01]    []  
42000     [1.18e-04, 5.76e-05, 1.93e-05, 5.76e-02, 5.76e-02]    [1.18e-04, 5.76e-05, 1.93e-05, 5.76e-02, 5.76e-02]    []  
30000     [9.80e-05, 5.21e-05, 4.28e-06, 1.02e-01, 1.02e-01]    [9.80e-05, 5.21e-05, 4.28e-06, 1.02e-01, 1.02e-01]    []  
22000     [3.68e-04, 5.42e-04, 2.46e-05, 5.32e-01, 5.32e-01]    [3.68e-04, 5.42e-04, 2.46e-05, 5.32e-01, 5.32e-01]    []  
24000     [3.84e-04, 2.39e-04, 8.78e-06, 3.37e-01, 3.37e-01]    [3.84e-04, 2.39e-04, 8.78e-06, 3.37e-01, 3.37e-01]    []  
24000     [1.78e-04, 1.31e-04, 1.13e-05, 3.13e-01, 3.13e-01]    [1.78e-04, 1.31e-04, 1.13e-05, 3.13e-01, 3.13e-01]    []  
43000     [1.24e-04, 5.41e-05, 8.55e-06, 5.51e-02, 5.51e-02]    [1.24e-04, 5.41e-05, 8.55e-06, 5.51e-02, 5.51e-02]    []  
31000     [1.30e-04, 5.07e-05, 4.59e-06, 1.33e-01, 1.33e-01]    [1.30e-04, 5.07e-05, 4.59e-06, 1.33e-01, 1.33e-01]    []  
25000     [3.94e-04, 2.39e-04, 1.04e-05, 4.95e-01, 4.95e-01]    [3.94e-04, 2.39e-04, 1.04e-05, 4.95e-01, 4.95e-01]    []  
25000     [3.02e-04, 1.56e-04, 1.05e-04, 1.69e+00, 1.69e+00]    [3.02e-04, 1.56e-04, 1.05e-04, 1.69e+00, 1.69e+00]    []  
23000     [3.31e-04, 5.05e-04, 2.40e-05, 5.21e-01, 5.21e-01]    [3.31e-04, 5.05e-04, 2.40e-05, 5.21e-01, 5.21e-01]    []  
                                                                                                                           26000     [1.30e-04, 1.39e-04, 8.46e-07, 5.27e-02, 5.27e-02]    [1.30e-04, 1.39e-04, 8.46e-07, 5.27e-02, 5.27e-02]    []  
44000     [1.43e-04, 5.45e-05, 1.62e-05, 7.41e-02, 7.41e-02]    [1.43e-04, 5.45e-05, 1.62e-05, 7.41e-02, 7.41e-02]    []  
26000     [3.75e-04, 2.27e-04, 9.21e-06, 7.83e-01, 7.83e-01]    [3.75e-04, 2.27e-04, 9.21e-06, 7.83e-01, 7.83e-01]    []  
32000     [1.42e-04, 5.45e-05, 2.85e-06, 1.01e-01, 1.01e-01]    [1.42e-04, 5.45e-05, 2.85e-06, 1.01e-01, 1.01e-01]    []  
24000     [3.26e-04, 4.83e-04, 6.10e-05, 4.78e-01, 4.78e-01]    [3.26e-04, 4.83e-04, 6.10e-05, 4.78e-01, 4.78e-01]    []  
27000     [2.12e-04, 1.78e-04, 2.29e-06, 6.37e-01, 6.37e-01]    [2.12e-04, 1.78e-04, 2.29e-06, 6.37e-01, 6.37e-01]    []  
27000     [3.06e-04, 2.07e-04, 7.29e-06, 3.43e-01, 3.43e-01]    [3.06e-04, 2.07e-04, 7.29e-06, 3.43e-01, 3.43e-01]    []  
45000     [1.22e-04, 4.56e-05, 3.78e-06, 5.25e-02, 5.25e-02]    [1.22e-04, 4.56e-05, 3.78e-06, 5.25e-02, 5.25e-02]    []  
33000     [1.61e-04, 5.31e-05, 4.03e-05, 4.14e-01, 4.14e-01]    [1.61e-04, 5.31e-05, 4.03e-05, 4.14e-01, 4.14e-01]    []  
28000     [1.19e-04, 9.51e-05, 1.88e-06, 4.57e-02, 4.57e-02]    [1.19e-04, 9.51e-05, 1.88e-06, 4.57e-02, 4.57e-02]    []  
25000     [3.10e-04, 4.59e-04, 5.44e-05, 4.45e-01, 4.45e-01]    [3.10e-04, 4.59e-04, 5.44e-05, 4.45e-01, 4.45e-01]    []  
                                                                                                                                                                                                                                                      28000     [3.58e-04, 1.83e-04, 2.67e-05, 3.87e-01, 3.87e-01]    [3.58e-04, 1.83e-04, 2.67e-05, 3.87e-01, 3.87e-01]    []  
46000     [1.14e-04, 4.58e-05, 4.98e-06, 4.93e-02, 4.93e-02]    [1.14e-04, 4.58e-05, 4.98e-06, 4.93e-02, 4.93e-02]    []  
29000     [8.59e-05, 1.01e-04, 8.83e-06, 1.56e+00, 1.56e+00]    [8.59e-05, 1.01e-04, 8.83e-06, 1.56e+00, 1.56e+00]    []  
34000     [1.26e-04, 5.19e-05, 6.21e-06, 9.47e-02, 9.47e-02]    [1.26e-04, 5.19e-05, 6.21e-06, 9.47e-02, 9.47e-02]    []  
26000     [3.08e-04, 4.48e-04, 4.42e-05, 3.11e-01, 3.11e-01]    [3.08e-04, 4.48e-04, 4.42e-05, 3.11e-01, 3.11e-01]    []  
29000     [2.94e-04, 1.69e-04, 7.33e-06, 2.66e-01, 2.66e-01]    [2.94e-04, 1.69e-04, 7.33e-06, 2.66e-01, 2.66e-01]    []  
30000     [8.11e-05, 9.76e-05, 8.66e-06, 3.95e-02, 3.95e-02]    [8.11e-05, 9.76e-05, 8.66e-06, 3.95e-02, 3.95e-02]    []  
47000     [1.76e-04, 3.99e-05, 1.49e-05, 4.80e-01, 4.80e-01]    [1.76e-04, 3.99e-05, 1.49e-05, 4.80e-01, 4.80e-01]    []  
35000     [1.06e-04, 5.30e-05, 3.10e-06, 7.79e-02, 7.79e-02]    [1.06e-04, 5.30e-05, 3.10e-06, 7.79e-02, 7.79e-02]    []  
30000     [2.57e-04, 1.57e-04, 6.31e-06, 2.26e-01, 2.26e-01]    [2.57e-04, 1.57e-04, 6.31e-06, 2.26e-01, 2.26e-01]    []  
31000     [6.73e-05, 9.67e-05, 1.68e-06, 3.74e-02, 3.74e-02]    [6.73e-05, 9.67e-05, 1.68e-06, 3.74e-02, 3.74e-02]    []  
27000     [3.01e-04, 4.37e-04, 1.69e-05, 3.08e-01, 3.08e-01]    [3.01e-04, 4.37e-04, 1.69e-05, 3.08e-01, 3.08e-01]    []  
                                                                                                                                                                                                                                                      48000     [9.70e-05, 3.82e-05, 3.24e-06, 4.52e-02, 4.52e-02]    [9.70e-05, 3.82e-05, 3.24e-06, 4.52e-02, 4.52e-02]    []  
32000     [6.41e-05, 5.22e-05, 3.03e-06, 7.84e-02, 7.84e-02]    [6.41e-05, 5.22e-05, 3.03e-06, 7.84e-02, 7.84e-02]    []  
36000     [1.06e-04, 4.77e-05, 8.11e-05, 8.66e-02, 8.66e-02]    [1.06e-04, 4.77e-05, 8.11e-05, 8.66e-02, 8.66e-02]    []  
31000     [2.77e-04, 1.45e-04, 7.13e-06, 3.34e-01, 3.34e-01]    [2.77e-04, 1.45e-04, 7.13e-06, 3.34e-01, 3.34e-01]    []  
28000     [3.32e-04, 4.29e-04, 3.30e-05, 3.04e-01, 3.04e-01]    [3.32e-04, 4.29e-04, 3.30e-05, 3.04e-01, 3.04e-01]    []  
33000     [8.17e-05, 5.60e-05, 2.36e-05, 4.26e-01, 4.26e-01]    [8.17e-05, 5.60e-05, 2.36e-05, 4.26e-01, 4.26e-01]    []  
49000     [1.12e-04, 3.27e-05, 1.65e-05, 4.37e-02, 4.37e-02]    [1.12e-04, 3.27e-05, 1.65e-05, 4.37e-02, 4.37e-02]    []  
32000     [3.94e-04, 1.43e-04, 4.15e-06, 1.74e+00, 1.74e+00]    [3.94e-04, 1.43e-04, 4.15e-06, 1.74e+00, 1.74e+00]    []  
37000     [1.06e-04, 4.61e-05, 8.71e-06, 1.10e-01, 1.10e-01]    [1.06e-04, 4.61e-05, 8.71e-06, 1.10e-01, 1.10e-01]    []  
34000     [1.12e-04, 9.87e-05, 7.05e-05, 3.00e-01, 3.00e-01]    [1.12e-04, 9.87e-05, 7.05e-05, 3.00e-01, 3.00e-01]    []  
29000     [3.25e-04, 4.18e-04, 2.06e-05, 2.88e-01, 2.88e-01]    [3.25e-04, 4.18e-04, 2.06e-05, 2.88e-01, 2.88e-01]    []  
33000     [2.12e-04, 1.20e-04, 5.06e-06, 2.88e-01, 2.88e-01]    [2.12e-04, 1.20e-04, 5.06e-06, 2.88e-01, 2.88e-01]    []  
50000     [1.63e-04, 3.93e-05, 5.21e-05, 3.18e+00, 3.18e+00]    [1.63e-04, 3.93e-05, 5.21e-05, 3.18e+00, 3.18e+00]    []  

Best model at step 49000:
  train loss: 8.76e-02
  test loss: 8.76e-02
  test metric: []

'train' took 12407.361522 s

[I 2023-10-09 05:20:44,313] Trial 30 finished with value: 1.9059942416027675 and parameters: {'num_domain': 80790, 'num_boundary': 7348, 'resampling_period': 9727, 'lr': 0.0007675684688984115}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000127 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [9.23e+01, 4.90e-04, 1.99e-07, 3.61e+03, 3.61e+03]    [9.23e+01, 4.90e-04, 1.99e-07, 3.61e+03, 3.61e+03]    []  
35000     [1.15e-04, 8.34e-05, 2.50e-05, 3.14e-01, 3.14e-01]    [1.15e-04, 8.34e-05, 2.50e-05, 3.14e-01, 3.14e-01]    []  
38000     [9.03e-05, 4.43e-05, 2.58e-06, 7.03e-02, 7.03e-02]    [9.03e-05, 4.43e-05, 2.58e-06, 7.03e-02, 7.03e-02]    []  
                                                                                                                           30000     [3.17e-04, 4.09e-04, 2.85e-05, 6.79e-01, 6.79e-01]    [3.17e-04, 4.09e-04, 2.85e-05, 6.79e-01, 6.79e-01]    []  
34000     [2.31e-04, 1.10e-04, 9.77e-06, 5.26e-01, 5.26e-01]    [2.31e-04, 1.10e-04, 9.77e-06, 5.26e-01, 5.26e-01]    []  
1000      [5.61e-07, 1.05e-03, 3.91e-16, 2.09e+03, 2.09e+03]    [5.61e-07, 1.05e-03, 3.91e-16, 2.09e+03, 2.09e+03]    []  
36000     [5.70e-05, 5.86e-05, 1.02e-06, 2.78e-02, 2.78e-02]    [5.70e-05, 5.86e-05, 1.02e-06, 2.78e-02, 2.78e-02]    []  
39000     [1.14e-04, 4.41e-05, 7.03e-06, 7.17e-02, 7.17e-02]    [1.14e-04, 4.41e-05, 7.03e-06, 7.17e-02, 7.17e-02]    []  
2000      [1.49e-06, 5.68e-04, 8.12e-20, 8.78e+02, 8.78e+02]    [1.49e-06, 5.68e-04, 8.12e-20, 8.78e+02, 8.78e+02]    []  
35000     [1.87e-04, 9.50e-05, 5.97e-06, 2.55e-01, 2.55e-01]    [1.87e-04, 9.50e-05, 5.97e-06, 2.55e-01, 2.55e-01]    []  
37000     [6.16e-05, 4.33e-05, 2.45e-06, 3.18e-02, 3.18e-02]    [6.16e-05, 4.33e-05, 2.45e-06, 3.18e-02, 3.18e-02]    []  
31000     [2.90e-04, 4.00e-04, 2.96e-05, 2.04e-01, 2.04e-01]    [2.90e-04, 4.00e-04, 2.96e-05, 2.04e-01, 2.04e-01]    []  
40000     [9.30e-05, 3.98e-05, 4.16e-06, 6.34e-02, 6.34e-02]    [9.30e-05, 3.98e-05, 4.16e-06, 6.34e-02, 6.34e-02]    []  
38000     [6.10e-05, 4.89e-05, 7.04e-06, 8.06e-02, 8.06e-02]    [6.10e-05, 4.89e-05, 7.04e-06, 8.06e-02, 8.06e-02]    []  
3000      [1.40e-02, 8.13e-05, 8.55e-05, 2.00e+01, 2.00e+01]    [1.40e-02, 8.13e-05, 8.55e-05, 2.00e+01, 2.00e+01]    []  
36000     [1.36e-04, 8.98e-05, 3.38e-06, 1.44e-01, 1.44e-01]    [1.36e-04, 8.98e-05, 3.38e-06, 1.44e-01, 1.44e-01]    []  
32000     [3.19e-04, 3.92e-04, 4.69e-05, 3.47e-01, 3.47e-01]    [3.19e-04, 3.92e-04, 4.69e-05, 3.47e-01, 3.47e-01]    []  
39000     [8.61e-05, 7.72e-05, 5.75e-05, 6.28e-01, 6.28e-01]    [8.61e-05, 7.72e-05, 5.75e-05, 6.28e-01, 6.28e-01]    []  
4000      [2.17e-03, 5.01e-05, 1.42e-05, 1.19e+01, 1.19e+01]    [2.17e-03, 5.01e-05, 1.42e-05, 1.19e+01, 1.19e+01]    []  
41000     [6.80e-05, 3.96e-05, 3.60e-06, 5.62e-02, 5.62e-02]    [6.80e-05, 3.96e-05, 3.60e-06, 5.62e-02, 5.62e-02]    []  
37000     [1.80e-04, 8.21e-05, 4.06e-06, 2.46e-01, 2.46e-01]    [1.80e-04, 8.21e-05, 4.06e-06, 2.46e-01, 2.46e-01]    []  
                                                                                                                           40000     [4.81e-05, 6.02e-05, 3.07e-06, 2.43e-02, 2.43e-02]    [4.81e-05, 6.02e-05, 3.07e-06, 2.43e-02, 2.43e-02]    []  
5000      [1.09e-03, 9.32e-04, 2.78e-05, 5.77e+00, 5.77e+00]    [1.09e-03, 9.32e-04, 2.78e-05, 5.77e+00, 5.77e+00]    []  
33000     [3.11e-04, 3.84e-04, 2.51e-05, 2.13e-01, 2.13e-01]    [3.11e-04, 3.84e-04, 2.51e-05, 2.13e-01, 2.13e-01]    []  
38000     [1.63e-04, 8.20e-05, 4.83e-06, 1.64e-01, 1.64e-01]    [1.63e-04, 8.20e-05, 4.83e-06, 1.64e-01, 1.64e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                           42000     [1.11e-04, 3.79e-05, 1.12e-05, 7.03e-02, 7.03e-02]    [1.11e-04, 3.79e-05, 1.12e-05, 7.03e-02, 7.03e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                          41000     [4.14e-05, 5.98e-05, 1.15e-06, 2.23e-02, 2.23e-02]    [4.14e-05, 5.98e-05, 1.15e-06, 2.23e-02, 2.23e-02]    []  
6000      [8.24e-04, 1.11e-03, 1.56e-05, 3.57e+00, 3.57e+00]    [8.24e-04, 1.11e-03, 1.56e-05, 3.57e+00, 3.57e+00]    []  
34000     [3.36e-04, 3.85e-04, 4.94e-05, 2.78e-01, 2.78e-01]    [3.36e-04, 3.85e-04, 4.94e-05, 2.78e-01, 2.78e-01]    []  
39000     [1.50e-04, 7.63e-05, 6.18e-06, 2.72e-01, 2.72e-01]    [1.50e-04, 7.63e-05, 6.18e-06, 2.72e-01, 2.72e-01]    []  
42000     [8.09e-05, 1.12e-04, 5.94e-07, 2.27e-02, 2.27e-02]    [8.09e-05, 1.12e-04, 5.94e-07, 2.27e-02, 2.27e-02]    []  
7000      [3.44e-03, 9.97e-04, 1.28e-04, 6.74e+00, 6.74e+00]    [3.44e-03, 9.97e-04, 1.28e-04, 6.74e+00, 6.74e+00]    []  
43000     [1.26e-04, 3.53e-05, 1.80e-05, 2.15e-01, 2.15e-01]    [1.26e-04, 3.53e-05, 1.80e-05, 2.15e-01, 2.15e-01]    []  
                                                                                                                           43000     [5.66e-05, 3.39e-05, 2.08e-06, 3.96e-01, 3.96e-01]    [5.66e-05, 3.39e-05, 2.08e-06, 3.96e-01, 3.96e-01]    []  
40000     [1.42e-04, 7.03e-05, 6.64e-06, 1.17e-01, 1.17e-01]    [1.42e-04, 7.03e-05, 6.64e-06, 1.17e-01, 1.17e-01]    []  
35000     [3.55e-04, 3.83e-04, 5.13e-05, 4.01e-01, 4.01e-01]    [3.55e-04, 3.83e-04, 5.13e-05, 4.01e-01, 4.01e-01]    []  
8000      [5.47e-02, 3.07e-04, 5.68e-05, 1.88e+01, 1.88e+01]    [5.47e-02, 3.07e-04, 5.68e-05, 1.88e+01, 1.88e+01]    []  
44000     [1.04e-04, 3.32e-05, 5.14e-06, 7.19e-02, 7.19e-02]    [1.04e-04, 3.32e-05, 5.14e-06, 7.19e-02, 7.19e-02]    []  
44000     [2.90e-05, 3.34e-05, 2.24e-08, 2.12e-02, 2.12e-02]    [2.90e-05, 3.34e-05, 2.24e-08, 2.12e-02, 2.12e-02]    []  
41000     [1.50e-04, 6.53e-05, 9.13e-06, 2.29e-01, 2.29e-01]    [1.50e-04, 6.53e-05, 9.13e-06, 2.29e-01, 2.29e-01]    []  
9000      [2.52e-03, 1.84e-04, 1.07e-06, 1.52e+01, 1.52e+01]    [2.52e-03, 1.84e-04, 1.07e-06, 1.52e+01, 1.52e+01]    []  
36000     [3.57e-04, 3.82e-04, 3.54e-05, 1.38e-01, 1.38e-01]    [3.57e-04, 3.82e-04, 3.54e-05, 1.38e-01, 1.38e-01]    []  
45000     [2.51e-05, 3.28e-05, 1.77e-08, 3.75e-02, 3.75e-02]    [2.51e-05, 3.28e-05, 1.77e-08, 3.75e-02, 3.75e-02]    []  
45000     [8.44e-05, 3.10e-05, 9.56e-06, 3.77e-01, 3.77e-01]    [8.44e-05, 3.10e-05, 9.56e-06, 3.77e-01, 3.77e-01]    []  
42000     [1.39e-04, 6.41e-05, 2.38e-06, 9.35e-02, 9.35e-02]    [1.39e-04, 6.41e-05, 2.38e-06, 9.35e-02, 9.35e-02]    []  
10000     [4.39e-03, 5.65e-04, 7.19e-06, 9.67e+00, 9.67e+00]    [4.39e-03, 5.65e-04, 7.19e-06, 9.67e+00, 9.67e+00]    []  
46000     [2.63e-04, 6.49e-05, 8.23e-07, 3.96e-01, 3.96e-01]    [2.63e-04, 6.49e-05, 8.23e-07, 3.96e-01, 3.96e-01]    []  
37000     [3.41e-04, 3.67e-04, 1.81e-05, 1.36e-01, 1.36e-01]    [3.41e-04, 3.67e-04, 1.81e-05, 1.36e-01, 1.36e-01]    []  
11000     [5.02e-03, 8.16e-04, 4.48e-05, 1.05e+01, 1.05e+01]    [5.02e-03, 8.16e-04, 4.48e-05, 1.05e+01, 1.05e+01]    []  
43000     [1.44e-04, 6.28e-05, 4.06e-06, 2.73e-01, 2.73e-01]    [1.44e-04, 6.28e-05, 4.06e-06, 2.73e-01, 2.73e-01]    []  
46000     [5.23e-05, 3.07e-05, 4.61e-06, 4.58e-02, 4.58e-02]    [5.23e-05, 3.07e-05, 4.61e-06, 4.58e-02, 4.58e-02]    []  
47000     [4.94e-05, 4.79e-06, 1.06e-07, 2.39e-01, 2.39e-01]    [4.94e-05, 4.79e-06, 1.06e-07, 2.39e-01, 2.39e-01]    []  
12000     [1.31e-02, 4.67e-04, 3.28e-04, 6.17e+00, 6.17e+00]    [1.31e-02, 4.67e-04, 3.28e-04, 6.17e+00, 6.17e+00]    []  
                                                                                                                           38000     [3.59e-04, 3.55e-04, 2.24e-05, 1.57e-01, 1.57e-01]    [3.59e-04, 3.55e-04, 2.24e-05, 1.57e-01, 1.57e-01]    []  
44000     [1.68e-04, 5.32e-05, 2.23e-05, 2.67e-01, 2.67e-01]    [1.68e-04, 5.32e-05, 2.23e-05, 2.67e-01, 2.67e-01]    []  
48000     [3.49e-05, 5.82e-06, 9.52e-08, 1.07e-01, 1.07e-01]    [3.49e-05, 5.82e-06, 9.52e-08, 1.07e-01, 1.07e-01]    []  
47000     [1.57e-04, 2.68e-05, 1.46e-05, 4.54e+00, 4.54e+00]    [1.57e-04, 2.68e-05, 1.46e-05, 4.54e+00, 4.54e+00]    []  
                                                                                                                           13000     [2.28e-02, 4.47e-04, 3.71e-06, 5.52e+00, 5.52e+00]    [2.28e-02, 4.47e-04, 3.71e-06, 5.52e+00, 5.52e+00]    []  
49000     [2.00e-05, 2.23e-06, 5.41e-07, 7.92e-02, 7.92e-02]    [2.00e-05, 2.23e-06, 5.41e-07, 7.92e-02, 7.92e-02]    []  
45000     [1.98e-04, 7.67e-05, 1.80e-05, 6.59e-01, 6.59e-01]    [1.98e-04, 7.67e-05, 1.80e-05, 6.59e-01, 6.59e-01]    []  
39000     [3.13e-04, 3.41e-04, 2.06e-05, 1.11e-01, 1.11e-01]    [3.13e-04, 3.41e-04, 2.06e-05, 1.11e-01, 1.11e-01]    []  
48000     [1.13e-04, 2.72e-05, 5.47e-06, 1.18e-01, 1.18e-01]    [1.13e-04, 2.72e-05, 5.47e-06, 1.18e-01, 1.18e-01]    []  
14000     [5.45e-04, 1.31e-03, 1.51e-11, 2.61e+01, 2.61e+01]    [5.45e-04, 1.31e-03, 1.51e-11, 2.61e+01, 2.61e+01]    []  
50000     [2.84e-05, 1.83e-05, 1.88e-07, 1.54e-01, 1.54e-01]    [2.84e-05, 1.83e-05, 1.88e-07, 1.54e-01, 1.54e-01]    []  

Best model at step 44000:
  train loss: 4.25e-02
  test loss: 4.25e-02
  test metric: []

'train' took 8461.636711 s

[I 2023-10-09 06:03:37,221] Trial 36 finished with value: 0.6281191468067064 and parameters: {'num_domain': 22577, 'num_boundary': 4622, 'resampling_period': 21794, 'lr': 0.0018754884921931385}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000094 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [5.00e+01, 2.90e-05, 1.09e-07, 3.61e+03, 3.61e+03]    [5.00e+01, 2.90e-05, 1.09e-07, 3.61e+03, 3.61e+03]    []  
46000     [1.30e-04, 5.00e-05, 3.66e-06, 7.93e-02, 7.93e-02]    [1.30e-04, 5.00e-05, 3.66e-06, 7.93e-02, 7.93e-02]    []  
                                                                                                                                                                                                                                                      15000     [4.14e-05, 9.41e-05, 3.59e-08, 8.08e+00, 8.08e+00]    [4.14e-05, 9.41e-05, 3.59e-08, 8.08e+00, 8.08e+00]    []  
40000     [3.22e-04, 3.26e-04, 3.23e-05, 1.92e-01, 1.92e-01]    [3.22e-04, 3.26e-04, 3.23e-05, 1.92e-01, 1.92e-01]    []  
49000     [5.60e-05, 2.74e-05, 8.38e-06, 4.09e-02, 4.09e-02]    [5.60e-05, 2.74e-05, 8.38e-06, 4.09e-02, 4.09e-02]    []  
1000      [6.56e-10, 1.05e-03, 1.76e-16, 2.09e+03, 2.09e+03]    [6.56e-10, 1.05e-03, 1.76e-16, 2.09e+03, 2.09e+03]    []  
47000     [1.41e-04, 4.62e-05, 6.80e-06, 7.54e-02, 7.54e-02]    [1.41e-04, 4.62e-05, 6.80e-06, 7.54e-02, 7.54e-02]    []  
16000     [3.39e-04, 5.05e-04, 1.47e-07, 9.06e+00, 9.06e+00]    [3.39e-04, 5.05e-04, 1.47e-07, 9.06e+00, 9.06e+00]    []  
2000      [1.75e-05, 5.04e-04, 1.72e-07, 2.30e+01, 2.30e+01]    [1.75e-05, 5.04e-04, 1.72e-07, 2.30e+01, 2.30e+01]    []  
41000     [3.28e-04, 3.24e-04, 1.89e-05, 1.02e-01, 1.02e-01]    [3.28e-04, 3.24e-04, 1.89e-05, 1.02e-01, 1.02e-01]    []  
50000     [5.59e-05, 2.69e-05, 7.45e-06, 4.25e-02, 4.25e-02]    [5.59e-05, 2.69e-05, 7.45e-06, 4.25e-02, 4.25e-02]    []  

Best model at step 49000:
  train loss: 8.20e-02
  test loss: 8.20e-02
  test metric: []

'train' took 12260.235238 s

[I 2023-10-09 06:10:30,065] Trial 31 finished with value: 0.49604713970409237 and parameters: {'num_domain': 79230, 'num_boundary': 7302, 'resampling_period': 10198, 'lr': 0.00065158008696473}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000106 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.91e+01, 1.39e-05, 2.90e-07, 3.60e+03, 3.60e+03]    [3.91e+01, 1.39e-05, 2.90e-07, 3.60e+03, 3.60e+03]    []  
48000     [1.40e-04, 5.06e-05, 1.04e-05, 7.32e-02, 7.32e-02]    [1.40e-04, 5.06e-05, 1.04e-05, 7.32e-02, 7.32e-02]    []  
17000     [3.67e-04, 1.38e-04, 1.97e-06, 7.16e+00, 7.16e+00]    [3.67e-04, 1.38e-04, 1.97e-06, 7.16e+00, 7.16e+00]    []  
3000      [1.20e-04, 1.09e-04, 1.41e-06, 7.43e+00, 7.43e+00]    [1.20e-04, 1.09e-04, 1.41e-06, 7.43e+00, 7.43e+00]    []  

Best model at step 49000:
  train loss: 4.05e-01
  test loss: 4.05e-01
  test metric: []

'train' took 23316.789997 s

[I 2023-10-09 06:12:22,737] Trial 22 finished with value: 0.6576000383337623 and parameters: {'num_domain': 99511, 'num_boundary': 1090, 'resampling_period': 2148, 'lr': 0.0001631842212860032}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000179 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [5.34e+01, 1.40e-04, 1.31e-07, 3.62e+03, 3.62e+03]    [5.34e+01, 1.40e-04, 1.31e-07, 3.62e+03, 3.62e+03]    []  
12000     [3.16e-04, 2.83e-05, 1.78e-05, 2.21e-01, 2.21e-01]    [3.16e-04, 2.83e-05, 1.78e-05, 2.21e-01, 2.21e-01]    []  
49000     [1.27e-04, 3.43e-05, 8.11e-06, 7.02e-02, 7.02e-02]    [1.27e-04, 3.43e-05, 8.11e-06, 7.02e-02, 7.02e-02]    []  
1000      [1.39e-02, 1.05e-03, 1.19e-17, 2.09e+03, 2.09e+03]    [1.39e-02, 1.05e-03, 1.19e-17, 2.09e+03, 2.09e+03]    []  
18000     [8.74e-05, 4.57e-04, 6.67e-07, 5.18e+00, 5.18e+00]    [8.74e-05, 4.57e-04, 6.67e-07, 5.18e+00, 5.18e+00]    []  
4000      [4.54e-05, 3.14e-05, 9.95e-08, 2.39e+00, 2.39e+00]    [4.54e-05, 3.14e-05, 9.95e-08, 2.39e+00, 2.39e+00]    []  
13000     [1.19e-01, 2.90e-05, 6.62e-05, 6.19e-01, 6.19e-01]    [1.19e-01, 2.90e-05, 6.62e-05, 6.19e-01, 6.19e-01]    []  
19000     [3.74e-04, 3.58e-04, 1.79e-08, 5.57e+00, 5.57e+00]    [3.74e-04, 3.58e-04, 1.79e-08, 5.57e+00, 5.57e+00]    []  
50000     [1.90e-04, 5.46e-05, 1.40e-05, 2.74e-01, 2.74e-01]    [1.90e-04, 5.46e-05, 1.40e-05, 2.74e-01, 2.74e-01]    []  

Best model at step 49000:
  train loss: 1.41e-01
  test loss: 1.41e-01
  test metric: []

'train' took 10187.268467 s

[I 2023-10-09 06:17:52,616] Trial 35 finished with value: 0.7117029279781167 and parameters: {'num_domain': 52027, 'num_boundary': 4419, 'resampling_period': 8059, 'lr': 0.0005832215721769445}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000105 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.86e+01, 1.23e-04, 1.03e-07, 3.60e+03, 3.60e+03]    [1.86e+01, 1.23e-04, 1.03e-07, 3.60e+03, 3.60e+03]    []  
2000      [4.94e-05, 2.26e-04, 5.45e-07, 6.70e+00, 6.70e+00]    [4.94e-05, 2.26e-04, 5.45e-07, 6.70e+00, 6.70e+00]    []  
43000     [3.55e-04, 3.18e-04, 3.70e-05, 1.50e-01, 1.50e-01]    [3.55e-04, 3.18e-04, 3.70e-05, 1.50e-01, 1.50e-01]    []  
1000      [9.05e-04, 3.14e-04, 4.83e-06, 2.75e+01, 2.75e+01]    [9.05e-04, 3.14e-04, 4.83e-06, 2.75e+01, 2.75e+01]    []  
14000     [7.12e-03, 3.51e-05, 5.49e-05, 2.48e-01, 2.48e-01]    [7.12e-03, 3.51e-05, 5.49e-05, 2.48e-01, 2.48e-01]    []  
20000     [1.20e-04, 6.08e-04, 2.90e-07, 2.32e+00, 2.32e+00]    [1.20e-04, 6.08e-04, 2.90e-07, 2.32e+00, 2.32e+00]    []  
6000      [1.31e-04, 1.41e-05, 2.70e-08, 1.30e+00, 1.30e+00]    [1.31e-04, 1.41e-05, 2.70e-08, 1.30e+00, 1.30e+00]    []  
1000      [2.47e-04, 3.85e-04, 9.27e-07, 2.57e+01, 2.57e+01]    [2.47e-04, 3.85e-04, 9.27e-07, 2.57e+01, 2.57e+01]    []  
3000      [3.92e-04, 3.43e-05, 9.27e-06, 6.58e+00, 6.58e+00]    [3.92e-04, 3.43e-05, 9.27e-06, 6.58e+00, 6.58e+00]    []  
44000     [3.35e-04, 3.03e-04, 4.71e-05, 1.03e-01, 1.03e-01]    [3.35e-04, 3.03e-04, 4.71e-05, 1.03e-01, 1.03e-01]    []  
21000     [5.12e-04, 7.35e-04, 1.05e-06, 5.73e+00, 5.73e+00]    [5.12e-04, 7.35e-04, 1.05e-06, 5.73e+00, 5.73e+00]    []  
7000      [1.79e-04, 2.87e-05, 4.56e-08, 6.50e-01, 6.50e-01]    [1.79e-04, 2.87e-05, 4.56e-08, 6.50e-01, 6.50e-01]    []  
16000     [6.46e-03, 6.20e-04, 1.03e-04, 2.74e+00, 2.74e+00]    [6.46e-03, 6.20e-04, 1.03e-04, 2.74e+00, 2.74e+00]    []  
2000      [2.13e-02, 3.19e-04, 3.47e-06, 1.30e+01, 1.30e+01]    [2.13e-02, 3.19e-04, 3.47e-06, 1.30e+01, 1.30e+01]    []  
22000     [1.67e-03, 3.58e-04, 2.55e-07, 2.18e+00, 2.18e+00]    [1.67e-03, 3.58e-04, 2.55e-07, 2.18e+00, 2.18e+00]    []  
45000     [4.30e-04, 2.83e-04, 6.13e-05, 8.28e-01, 8.28e-01]    [4.30e-04, 2.83e-04, 6.13e-05, 8.28e-01, 8.28e-01]    []  
8000      [1.91e-04, 2.79e-05, 3.50e-08, 6.29e-01, 6.29e-01]    [1.91e-04, 2.79e-05, 3.50e-08, 6.29e-01, 6.29e-01]    []  
                                                                                                                           23000     [8.22e-04, 8.51e-04, 9.86e-06, 2.43e+00, 2.43e+00]    [8.22e-04, 8.51e-04, 9.86e-06, 2.43e+00, 2.43e+00]    []  
5000      [1.24e-04, 5.22e-05, 9.96e-07, 1.02e+00, 1.02e+00]    [1.24e-04, 5.22e-05, 9.96e-07, 1.02e+00, 1.02e+00]    []  
3000      [1.22e-03, 2.59e-04, 8.55e-06, 1.67e+00, 1.67e+00]    [1.22e-03, 2.59e-04, 8.55e-06, 1.67e+00, 1.67e+00]    []  
9000      [1.67e-04, 2.80e-05, 2.89e-08, 5.46e-01, 5.46e-01]    [1.67e-04, 2.80e-05, 2.89e-08, 5.46e-01, 5.46e-01]    []  
46000     [2.84e-04, 2.89e-04, 2.76e-05, 8.58e-02, 8.58e-02]    [2.84e-04, 2.89e-04, 2.76e-05, 8.58e-02, 8.58e-02]    []  
24000     [5.19e-04, 8.97e-04, 1.18e-06, 1.78e+00, 1.78e+00]    [5.19e-04, 8.97e-04, 1.18e-06, 1.78e+00, 1.78e+00]    []  
10000     [1.12e-04, 3.13e-05, 1.20e-07, 4.66e-01, 4.66e-01]    [1.12e-04, 3.13e-05, 1.20e-07, 4.66e-01, 4.66e-01]    []  
6000      [2.24e-04, 5.48e-05, 2.30e-06, 8.60e-01, 8.60e-01]    [2.24e-04, 5.48e-05, 2.30e-06, 8.60e-01, 8.60e-01]    []  
4000      [1.34e-03, 1.94e-04, 3.62e-06, 1.89e+00, 1.89e+00]    [1.34e-03, 1.94e-04, 3.62e-06, 1.89e+00, 1.89e+00]    []  
25000     [8.35e-04, 5.45e-04, 3.91e-07, 5.23e+00, 5.23e+00]    [8.35e-04, 5.45e-04, 3.91e-07, 5.23e+00, 5.23e+00]    []  
47000     [3.58e-04, 3.11e-04, 5.02e-05, 1.73e-01, 1.73e-01]    [3.58e-04, 3.11e-04, 5.02e-05, 1.73e-01, 1.73e-01]    []  
11000     [7.16e-05, 1.78e-05, 8.53e-08, 3.80e-01, 3.80e-01]    [7.16e-05, 1.78e-05, 8.53e-08, 3.80e-01, 3.80e-01]    []  
7000      [4.15e-04, 8.02e-05, 1.61e-05, 4.55e+00, 4.55e+00]    [4.15e-04, 8.02e-05, 1.61e-05, 4.55e+00, 4.55e+00]    []  
26000     [3.89e-04, 1.77e-04, 8.98e-07, 1.59e+00, 1.59e+00]    [3.89e-04, 1.77e-04, 8.98e-07, 1.59e+00, 1.59e+00]    []  
5000      [4.64e-04, 1.37e-04, 2.95e-06, 1.70e+00, 1.70e+00]    [4.64e-04, 1.37e-04, 2.95e-06, 1.70e+00, 1.70e+00]    []  
48000     [3.16e-04, 2.66e-04, 1.16e-05, 7.78e-02, 7.78e-02]    [3.16e-04, 2.66e-04, 1.16e-05, 7.78e-02, 7.78e-02]    []  
12000     [8.84e-06, 2.56e-06, 4.53e-09, 3.13e-01, 3.13e-01]    [8.84e-06, 2.56e-06, 4.53e-09, 3.13e-01, 3.13e-01]    []  
                                                                                                                           27000     [2.34e-04, 1.03e-04, 4.70e-08, 1.50e+00, 1.50e+00]    [2.34e-04, 1.03e-04, 4.70e-08, 1.50e+00, 1.50e+00]    []  
8000      [2.16e-04, 4.16e-05, 2.74e-06, 5.98e-01, 5.98e-01]    [2.16e-04, 4.16e-05, 2.74e-06, 5.98e-01, 5.98e-01]    []  
13000     [1.45e-05, 2.09e-06, 7.97e-08, 7.57e-01, 7.57e-01]    [1.45e-05, 2.09e-06, 7.97e-08, 7.57e-01, 7.57e-01]    []  
6000      [1.39e-03, 2.68e-04, 9.31e-05, 3.93e+00, 3.93e+00]    [1.39e-03, 2.68e-04, 9.31e-05, 3.93e+00, 3.93e+00]    []  
49000     [2.88e-04, 2.66e-04, 2.73e-05, 6.93e-02, 6.93e-02]    [2.88e-04, 2.66e-04, 2.73e-05, 6.93e-02, 6.93e-02]    []  
28000     [5.96e-04, 9.59e-04, 2.19e-07, 7.10e+00, 7.10e+00]    [5.96e-04, 9.59e-04, 2.19e-07, 7.10e+00, 7.10e+00]    []  
9000      [2.27e-04, 3.51e-05, 5.64e-07, 3.80e-01, 3.80e-01]    [2.27e-04, 3.51e-05, 5.64e-07, 3.80e-01, 3.80e-01]    []  
14000     [2.21e-04, 4.01e-06, 2.45e-09, 6.45e+00, 6.45e+00]    [2.21e-04, 4.01e-06, 2.45e-09, 6.45e+00, 6.45e+00]    []  
7000      [2.05e-03, 4.61e-04, 1.06e-04, 2.74e+00, 2.74e+00]    [2.05e-03, 4.61e-04, 1.06e-04, 2.74e+00, 2.74e+00]    []  
29000     [2.25e-04, 1.19e-04, 1.34e-08, 1.19e+00, 1.19e+00]    [2.25e-04, 1.19e-04, 1.34e-08, 1.19e+00, 1.19e+00]    []  
50000     [2.80e-04, 2.72e-04, 3.08e-05, 7.04e-02, 7.04e-02]    [2.80e-04, 2.72e-04, 3.08e-05, 7.04e-02, 7.04e-02]    []  
5000      [4.24e-04, 1.03e-04, 1.97e-06, 1.52e+00, 1.52e+00]    [4.24e-04, 1.03e-04, 1.97e-06, 1.52e+00, 1.52e+00]   s

[I 2023-10-09 06:48:15,487] Trial 33 finished with value: 0.5586303865922092 and parameters: {'num_domain': 80664, 'num_boundary': 9989, 'resampling_period': 10070, 'lr': 0.0005900726015447464}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000161 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [5.65e+01, 8.87e-05, 3.58e-07, 3.62e+03, 3.62e+03]    [5.65e+01, 8.87e-05, 3.58e-07, 3.62e+03, 3.62e+03]    []  
15000     [1.85e-05, 1.26e-06, 2.55e-08, 2.10e-01, 2.10e-01]    [1.85e-05, 1.26e-06, 2.55e-08, 2.10e-01, 2.10e-01]    []  
10000     [1.24e-04, 2.19e-05, 2.84e-07, 2.25e-01, 2.25e-01]    [1.24e-04, 2.19e-05, 2.84e-07, 2.25e-01, 2.25e-01]    []  
30000     [2.05e-04, 3.95e-05, 1.86e-07, 1.19e+00, 1.19e+00]    [2.05e-04, 3.95e-05, 1.86e-07, 1.19e+00, 1.19e+00]    []  
8000      [5.90e-04, 4.72e-04, 1.17e-04, 5.97e-01, 5.97e-01]    [5.90e-04, 4.72e-04, 1.17e-04, 5.97e-01, 5.97e-01]    []  
16000     [2.60e-05, 1.55e-06, 7.61e-09, 1.75e-01, 1.75e-01]    [2.60e-05, 1.55e-06, 7.61e-09, 1.75e-01, 1.75e-01]    []  
1000      [1.94e-03, 4.45e-04, 7.64e-12, 5.83e+01, 5.83e+01]    [1.94e-03, 4.45e-04, 7.64e-12, 5.83e+01, 5.83e+01]    []  
11000     [9.29e-05, 1.16e-05, 5.49e-07, 3.83e-01, 3.83e-01]    [9.29e-05, 1.16e-05, 5.49e-07, 3.83e-01, 3.83e-01]    []  
31000     [2.01e-05, 2.52e-06, 5.88e-09, 2.44e+00, 2.44e+00]    [2.01e-05, 2.52e-06, 5.88e-09, 2.44e+00, 2.44e+00]    []  
                                                                                                                           17000     [1.47e-05, 3.77e-07, 2.49e-08, 2.19e-01, 2.19e-01]    [1.47e-05, 3.77e-07, 2.49e-08, 2.19e-01, 2.19e-01]    []  
9000      [5.43e-04, 5.09e-04, 5.90e-05, 6.21e-01, 6.21e-01]    [5.43e-04, 5.09e-04, 5.90e-05, 6.21e-01, 6.21e-01]    []  
2000      [1.15e-03, 2.75e-04, 1.66e-05, 6.52e+00, 6.52e+00]    [1.15e-03, 2.75e-04, 1.66e-05, 6.52e+00, 6.52e+00]    []  
32000     [1.74e-05, 9.22e-07, 3.44e-09, 1.44e+00, 1.44e+00]    [1.74e-05, 9.22e-07, 3.44e-09, 1.44e+00, 1.44e+00]    []  
12000     [7.53e-05, 1.40e-05, 6.92e-07, 1.79e+00, 1.79e+00]    [7.53e-05, 1.40e-05, 6.92e-07, 1.79e+00, 1.79e+00]    []  
18000     [2.13e-06, 3.21e-07, 5.79e-09, 3.18e-01, 3.18e-01]    [2.13e-06, 3.21e-07, 5.79e-09, 3.18e-01, 3.18e-01]    []  
33000     [5.33e-05, 1.89e-05, 1.63e-08, 1.38e+00, 1.38e+00]    [5.33e-05, 1.89e-05, 1.63e-08, 1.38e+00, 1.38e+00]    []  
10000     [5.89e-04, 5.68e-04, 4.43e-05, 1.93e-01, 1.93e-01]    [5.89e-04, 5.68e-04, 4.43e-05, 1.93e-01, 1.93e-01]    []  
3000      [2.91e-04, 2.18e-04, 9.65e-07, 3.51e+00, 3.51e+00]    [2.91e-04, 2.18e-04, 9.65e-07, 3.51e+00, 3.51e+00]    []  
19000     [5.67e-06, 1.36e-08, 6.69e-08, 1.83e-01, 1.83e-01]    [5.67e-06, 1.36e-08, 6.69e-08, 1.83e-01, 1.83e-01]    []  
13000     [7.58e-05, 4.63e-06, 7.96e-07, 3.38e+00, 3.38e+00]    [7.58e-05, 4.63e-06, 7.96e-07, 3.38e+00, 3.38e+00]    []  
34000     [2.06e-05, 2.23e-06, 5.69e-09, 9.48e-01, 9.48e-01]    [2.06e-05, 2.23e-06, 5.69e-09, 9.48e-01, 9.48e-01]    []  
11000     [1.65e-03, 2.39e-04, 4.33e-05, 2.12e-01, 2.12e-01]    [1.65e-03, 2.39e-04, 4.33e-05, 2.12e-01, 2.12e-01]    []  
20000     [1.62e-05, 1.10e-06, 6.97e-10, 1.23e-01, 1.23e-01]    [1.62e-05, 1.10e-06, 6.97e-10, 1.23e-01, 1.23e-01]    []  
4000      [5.63e-04, 4.23e-04, 6.94e-07, 3.03e+00, 3.03e+00]    [5.63e-04, 4.23e-04, 6.94e-07, 3.03e+00, 3.03e+00]    []  
14000     [3.48e-05, 2.96e-06, 2.18e-08, 1.60e-01, 1.60e-01]    [3.48e-05, 2.96e-06, 2.18e-08, 1.60e-01, 1.60e-01]    []  
35000     [2.50e-05, 2.41e-06, 5.17e-08, 8.35e-01, 8.35e-01]    [2.50e-05, 2.41e-06, 5.17e-08, 8.35e-01, 8.35e-01]    []  
21000     [3.86e-06, 3.90e-07, 1.35e-07, 1.27e-01, 1.27e-01]    [3.86e-06, 3.90e-07, 1.35e-07, 1.27e-01, 1.27e-01]    []  
12000     [6.60e-04, 1.29e-03, 2.95e-06, 1.01e+00, 1.01e+00]    [6.60e-04, 1.29e-03, 2.95e-06, 1.01e+00, 1.01e+00]    []  
5000      [2.61e-04, 1.26e-04, 7.58e-07, 1.08e+00, 1.08e+00]    [2.61e-04, 1.26e-04, 7.58e-07, 1.08e+00, 1.08e+00]    []  
36000     [4.04e-05, 3.41e-06, 1.28e-08, 7.23e-01, 7.23e-01]    [4.04e-05, 3.41e-06, 1.28e-08, 7.23e-01, 7.23e-01]    []  
15000     [2.11e-05, 1.55e-06, 6.01e-07, 1.56e-01, 1.56e-01]    [2.11e-05, 1.55e-06, 6.01e-07, 1.56e-01, 1.56e-01]    []  
                                                                                                                           22000     [5.51e-06, 4.45e-07, 7.36e-10, 1.06e-01, 1.06e-01]    [5.51e-06, 4.45e-07, 7.36e-10, 1.06e-01, 1.06e-01]    []  
37000     [3.14e-05, 5.31e-06, 6.29e-09, 7.42e-01, 7.42e-01]    [3.14e-05, 5.31e-06, 6.29e-09, 7.42e-01, 7.42e-01]    []  
13000     [8.65e-04, 1.09e-03, 2.63e-05, 8.87e-01, 8.87e-01]    [8.65e-04, 1.09e-03, 2.63e-05, 8.87e-01, 8.87e-01]    []  
6000      [2.51e-04, 8.71e-05, 1.01e-06, 1.06e+00, 1.06e+00]    [2.51e-04, 8.71e-05, 1.01e-06, 1.06e+00, 1.06e+00]    []  
16000     [3.08e-05, 1.70e-06, 1.71e-07, 2.98e-01, 2.98e-01]    [3.08e-05, 1.70e-06, 1.71e-07, 2.98e-01, 2.98e-01]    []  
23000     [1.70e-05, 1.52e-07, 2.14e-09, 2.29e-01, 2.29e-01]    [1.70e-05, 1.52e-07, 2.14e-09, 2.29e-01, 2.29e-01]    []  
38000     [1.07e-03, 1.56e-03, 5.96e-09, 2.37e+01, 2.37e+01]    [1.07e-03, 1.56e-03, 5.96e-09, 2.37e+01, 2.37e+01]    []  
                                                                                                                           24000     [1.09e-05, 8.33e-07, 1.87e-08, 1.29e-01, 1.29e-01]    [1.09e-05, 8.33e-07, 1.87e-08, 1.29e-01, 1.29e-01]    []  
7000      [1.60e-04, 4.22e-05, 9.53e-07, 1.11e+00, 1.11e+00]    [1.60e-04, 4.22e-05, 9.53e-07, 1.11e+00, 1.11e+00]    []  
14000     [6.53e-04, 1.17e-03, 3.88e-05, 5.83e-01, 5.83e-01]    [6.53e-04, 1.17e-03, 3.88e-05, 5.83e-01, 5.83e-01]    []  
17000     [1.54e-05, 1.27e-06, 4.70e-08, 2.15e+00, 2.15e+00]    [1.54e-05, 1.27e-06, 4.70e-08, 2.15e+00, 2.15e+00]    []  
39000     [1.58e-04, 1.43e-04, 1.73e-10, 8.12e+00, 8.12e+00]    [1.58e-04, 1.43e-04, 1.73e-10, 8.12e+00, 8.12e+00]    []  
                                                                                                                           25000     [1.24e-05, 7.74e-07, 1.06e-08, 8.58e-02, 8.58e-02]    [1.24e-05, 7.74e-07, 1.06e-08, 8.58e-02, 8.58e-02]    []  
8000      [3.84e-04, 8.57e-05, 1.86e-06, 7.30e+00, 7.30e+00]    [3.84e-04, 8.57e-05, 1.86e-06, 7.30e+00, 7.30e+00]    []  
18000     [2.34e-05, 4.61e-07, 1.80e-07, 1.11e-01, 1.11e-01]    [2.34e-05, 4.61e-07, 1.80e-07, 1.11e-01, 1.11e-01]    []  
40000     [5.49e-05, 6.19e-05, 7.09e-11, 5.02e+00, 5.02e+00]    [5.49e-05, 6.19e-05, 7.09e-11, 5.02e+00, 5.02e+00]    []  
15000     [6.65e-04, 1.05e-03, 1.09e-05, 6.26e-01, 6.26e-01]    [6.65e-04, 1.05e-03, 1.09e-05, 6.26e-01, 6.26e-01]    []  
26000     [2.33e-05, 9.46e-07, 9.41e-10, 5.83e+00, 5.83e+00]    [2.33e-05, 9.46e-07, 9.41e-10, 5.83e+00, 5.83e+00]    []  
41000     [3.06e-05, 4.23e-05, 2.10e-07, 5.05e+00, 5.05e+00]    [3.06e-05, 4.23e-05, 2.10e-07, 5.05e+00, 5.05e+00]    []  
9000      [1.52e-04, 5.13e-05, 6.17e-07, 3.22e-01, 3.22e-01]    [1.52e-04, 5.13e-05, 6.17e-07, 3.22e-01, 3.22e-01]    []  
19000     [2.96e-05, 1.08e-06, 1.23e-07, 1.26e-01, 1.26e-01]    [2.96e-05, 1.08e-06, 1.23e-07, 1.26e-01, 1.26e-01]    []  
16000     [6.19e-04, 4.79e-04, 1.72e-04, 1.22e+00, 1.22e+00]    [6.19e-04, 4.79e-04, 1.72e-04, 1.22e+00, 1.22e+00]    []  
27000     [1.78e-05, 1.74e-06, 5.69e-09, 5.73e-01, 5.73e-01]    [1.78e-05, 1.74e-06, 5.69e-09, 5.73e-01, 5.73e-01]    []  
                                                                                                                           42000     [2.33e-04, 5.16e-05, 6.34e-09, 2.83e+00, 2.83e+00]    [2.33e-04, 5.16e-05, 6.34e-09, 2.83e+00, 2.83e+00]    []  
                                                                                                                           36000     [6.03e-04, 1.63e-03, 5.98e-08, 5.66e-02, 5.66e-02]    [6.03e-04, 1.63e-03, 5.98e-08, 5.66e-02, 5.66e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       37000     [7.20e-05, 4.89e-05, 1.19e-06, 4.39e+00, 4.39e+00]    [7.20e-05, 4.89e-05, 1.19e-06, 4.39e+00, 4.39e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            38000     [5.07e-06, 1.20e-06, 4.05e-09, 7.22e-01, 7.22e-01]    [5.07e-06, 1.20e-06, 4.05e-09, 7.22e-01, 7.22e-01]    []  
11000     [3.46e-04, 3.48e-04, 9.04e-07, 1.37e+00, 1.37e+00]    [3.46e-04, 3.48e-04, 9.04e-07, 1.37e+00, 1.37e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       39000     [3.79e-06, 3.65e-07, 5.39e-10, 3.95e-01, 3.95e-01]    [3.79e-06, 3.65e-07, 5.39e-10, 3.95e-01, 3.95e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            40000     [1.47e-06, 4.74e-07, 5.70e-09, 6.74e-01, 6.74e-01]    [1.47e-06, 4.74e-07, 5.70e-09, 6.74e-01, 6.74e-01]    []  
                                                                                                                                                                                                                                                      12000     [2.62e-04, 2.36e-04, 1.63e-05, 7.87e-01, 7.87e-01]    [2.62e-04, 2.36e-04, 1.63e-05, 7.87e-01, 7.87e-01]    []  
                                                                                                                                                                                                                                                      41000     [5.45e-06, 4.57e-07, 7.26e-09, 1.67e-01, 1.67e-01]    [5.45e-06, 4.57e-07, 7.26e-09, 1.67e-01, 1.67e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                 42000     [4.04e-06, 8.72e-07, 1.11e-08, 1.70e-01, 1.70e-01]    [4.04e-06, 8.72e-07, 1.11e-08, 1.70e-01, 1.70e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       43000     [4.57e-06, 3.35e-07, 1.33e-08, 6.59e-01, 6.59e-01]    [4.57e-06, 3.35e-07, 1.33e-08, 6.59e-01, 6.59e-01]    []  
13000     [2.99e-04, 1.54e-04, 2.13e-05, 2.77e-01, 2.77e-01]    [2.99e-04, 1.54e-04, 2.13e-05, 2.77e-01, 2.77e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    44000     [5.51e-06, 2.37e-07, 6.68e-09, 1.00e-01, 1.00e-01]    [5.51e-06, 2.37e-07, 6.68e-09, 1.00e-01, 1.00e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            45000     [3.12e-06, 1.32e-07, 5.12e-09, 9.41e-02, 9.41e-02]    [3.12e-06, 1.32e-07, 5.12e-09, 9.41e-02, 9.41e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                 14000     [1.87e-04, 1.56e-04, 7.35e-06, 6.54e+00, 6.54e+00]    [1.87e-04, 1.56e-04, 7.35e-06, 6.54e+00, 6.54e+00]    []  
46000     [5.11e-06, 6.74e-08, 1.34e-09, 1.05e-01, 1.05e-01]    [5.11e-06, 6.74e-08, 1.34e-09, 1.05e-01, 1.05e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                 47000     [2.54e-03, 1.92e-06, 2.60e-05, 2.18e+01, 2.18e+01]    [2.54e-03, 1.92e-06, 2.60e-05, 2.18e+01, 2.18e+01]    []  
                                                                                                                                                                                                                                                      48000     [6.41e-05, 1.40e-05, 9.53e-09, 7.02e-01, 7.02e-01]    [6.41e-05, 1.40e-05, 9.53e-09, 7.02e-01, 7.02e-01]    []  
15000     [1.84e-04, 4.55e-04, 1.73e-05, 3.21e-01, 3.21e-01]    [1.84e-04, 4.55e-04, 1.73e-05, 3.21e-01, 3.21e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       49000     [6.22e-05, 5.20e-06, 5.01e-09, 2.52e-01, 2.52e-01]    [6.22e-05, 5.20e-06, 5.01e-09, 2.52e-01, 2.52e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            50000     [2.12e-05, 3.14e-06, 1.36e-09, 1.85e+00, 1.85e+00]    [2.12e-05, 3.14e-06, 1.36e-09, 1.85e+00, 1.85e+00]    []  

Best model at step 36000:
  train loss: 1.15e-01
  test loss: 1.15e-01
  test metric: []

'train' took 9217.360265 s

[I 2023-10-09 08:11:23,455] Trial 38 finished with value: 1.5584880790051454 and parameters: {'num_domain': 24830, 'num_boundary': 4311, 'resampling_period': 19528, 'lr': 0.0059600257312321025}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000159 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.59e+01, 4.86e-05, 1.63e-07, 3.60e+03, 3.60e+03]    [4.59e+01, 4.86e-05, 1.63e-07, 3.60e+03, 3.60e+03]    []  
                                                                                                                           16000     [1.30e-04, 4.93e-04, 1.36e-06, 1.50e-01, 1.50e-01]    [1.30e-04, 4.93e-04, 1.36e-06, 1.50e-01, 1.50e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1000      [4.39e-10, 1.05e-03, 3.85e-16, 2.09e+03, 2.09e+03]    [4.39e-10, 1.05e-03, 3.85e-16, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            8000      [5.73e-04, 4.22e-04, 1.72e-06, 1.20e+00, 1.20e+00]    [5.73e-04, 4.22e-04, 1.72e-06, 1.20e+00, 1.20e+00]    []  
46000     [8.47e-07, 7.66e-06, 1.42e-09, 2.39e-01, 2.39e-01]    [8.47e-07, 7.66e-06, 1.42e-09, 2.39e-01, 2.39e-01]    []  
34000     [2.80e-05, 1.14e-05, 1.80e-07, 1.25e-01, 1.25e-01]    [2.80e-05, 1.14e-05, 1.80e-07, 1.25e-01, 1.25e-01]    []  
30000     [1.37e-05, 8.47e-07, 1.11e-08, 6.43e-02, 6.43e-02]    [1.37e-05, 8.47e-07, 1.11e-08, 6.43e-02, 6.43e-02]    []  
24000     [2.07e-05, 4.87e-05, 7.20e-07, 8.02e-02, 8.02e-02]    [2.07e-05, 4.87e-05, 7.20e-07, 8.02e-02, 8.02e-02]    []  
47000     [7.82e-07, 1.53e-07, 1.13e-09, 5.40e-02, 5.40e-02]    [7.82e-07, 1.53e-07, 1.13e-09, 5.40e-02, 5.40e-02]    []  
9000      [4.19e-04, 3.02e-04, 3.17e-06, 9.87e-01, 9.87e-01]    [4.19e-04, 3.02e-04, 3.17e-06, 9.87e-01, 9.87e-01]    []  
35000     [7.63e-05, 2.26e-06, 2.25e-07, 3.57e+00, 3.57e+00]    [7.63e-05, 2.26e-06, 2.25e-07, 3.57e+00, 3.57e+00]    []  
31000     [9.21e-06, 4.38e-07, 6.99e-09, 1.39e-01, 1.39e-01]    [9.21e-06, 4.38e-07, 6.99e-09, 1.39e-01, 1.39e-01]    []  
25000     [2.88e-03, 5.41e-05, 1.17e-07, 6.99e-01, 6.99e-01]    [2.88e-03, 5.41e-05, 1.17e-07, 6.99e-01, 6.99e-01]    []  
48000     [7.11e-06, 4.24e-07, 1.03e-09, 4.41e-01, 4.41e-01]    [7.11e-06, 4.24e-07, 1.03e-09, 4.41e-01, 4.41e-01]    []  
10000     [2.97e-04, 2.09e-04, 2.07e-06, 7.48e-01, 7.48e-01]    [2.97e-04, 2.09e-04, 2.07e-06, 7.48e-01, 7.48e-01]    []  
36000     [3.92e-05, 2.60e-06, 3.80e-09, 2.41e+00, 2.41e+00]    [3.92e-05, 2.60e-06, 3.80e-09, 2.41e+00, 2.41e+00]    []  
                                                                                                                           32000     [4.77e-05, 1.78e-06, 7.74e-09, 4.20e-01, 4.20e-01]    [4.77e-05, 1.78e-06, 7.74e-09, 4.20e-01, 4.20e-01]    []  
49000     [3.64e-06, 1.55e-07, 4.74e-13, 1.89e-01, 1.89e-01]    [3.64e-06, 1.55e-07, 4.74e-13, 1.89e-01, 1.89e-01]    []  
26000     [1.60e-04, 6.51e-05, 3.89e-07, 4.87e-01, 4.87e-01]    [1.60e-04, 6.51e-05, 3.89e-07, 4.87e-01, 4.87e-01]    []  
11000     [2.38e-04, 1.67e-04, 1.14e-07, 8.06e-01, 8.06e-01]    [2.38e-04, 1.67e-04, 1.14e-07, 8.06e-01, 8.06e-01]    []  
37000     [4.11e-05, 6.57e-06, 4.32e-09, 1.18e-01, 1.18e-01]    [4.11e-05, 6.57e-06, 4.32e-09, 1.18e-01, 1.18e-01]    []  
50000     [3.04e-06, 8.02e-08, 1.34e-11, 1.04e-01, 1.04e-01]    [3.04e-06, 8.02e-08, 1.34e-11, 1.04e-01, 1.04e-01]    []  

Best model at step 45000:
  train loss: 8.12e-02
  test loss: 8.12e-02
  test metric: []

'train' took 9006.311912 s

[I 2023-10-09 08:33:52,687] Trial 39 finished with value: 0.7546652030514752 and parameters: {'num_domain': 33895, 'num_boundary': 3129, 'resampling_period': 18993, 'lr': 0.004555296607819428}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000102 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [5.29e+01, 1.09e-04, 4.57e-07, 3.60e+03, 3.60e+03]    [5.29e+01, 1.09e-04, 4.57e-07, 3.60e+03, 3.60e+03]    []  
[W 2023-10-09 08:34:05,896] Trial 46 failed with parameters: {'num_domain': 88045, 'num_boundary': 7913, 'resampling_period': 14395, 'lr': 0.0017662680311708936} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 98.00 MiB (GPU 7; 10.75 GiB total capacity; 9.65 GiB already allocated; 75.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 7; 10.75 GiB total capacity; 9.65 GiB already allocated; 75.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-09 08:34:05,911] Trial 46 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 80, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 7; 10.75 GiB total capacity; 9.65 GiB already allocated; 75.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
33000     [2.15e-05, 2.15e-06, 1.72e-09, 2.90e-01, 2.90e-01]    [2.15e-05, 2.15e-06, 1.72e-09, 2.90e-01, 2.90e-01]    []  
27000     [1.65e-05, 8.69e-05, 8.29e-08, 6.22e-02, 6.22e-02]    [1.65e-05, 8.69e-05, 8.29e-08, 6.22e-02, 6.22e-02]    []  
                                                                                                                           12000     [1.08e-04, 3.05e-05, 1.01e-06, 9.54e-01, 9.54e-01]    [1.08e-04, 3.05e-05, 1.01e-06, 9.54e-01, 9.54e-01]    []  
38000     [7.02e-06, 3.73e-07, 3.27e-09, 5.77e-02, 5.77e-02]    [7.02e-06, 3.73e-07, 3.27e-09, 5.77e-02, 5.77e-02]    []  
28000     [2.19e-04, 1.30e-04, 1.81e-06, 6.88e-02, 6.88e-02]    [2.19e-04, 1.30e-04, 1.81e-06, 6.88e-02, 6.88e-02]    []  
34000     [1.69e-05, 2.36e-06, 7.91e-08, 1.50e-01, 1.50e-01]    [1.69e-05, 2.36e-06, 7.91e-08, 1.50e-01, 1.50e-01]    []  
13000     [6.91e-05, 1.26e-05, 2.41e-06, 8.65e-01, 8.65e-01]    [6.91e-05, 1.26e-05, 2.41e-06, 8.65e-01, 8.65e-01]    []  
39000     [8.54e-06, 7.38e-07, 4.51e-08, 9.42e-02, 9.42e-02]    [8.54e-06, 7.38e-07, 4.51e-08, 9.42e-02, 9.42e-02]    []  
                                                                                                                           29000     [2.22e-02, 1.56e-04, 3.01e-07, 5.75e-02, 5.75e-02]    [2.22e-02, 1.56e-04, 3.01e-07, 5.75e-02, 5.75e-02]    []  
35000     [7.45e-05, 7.70e-05, 1.52e-06, 1.17e+00, 1.17e+00]    [7.45e-05, 7.70e-05, 1.52e-06, 1.17e+00, 1.17e+00]    []  
14000     [6.94e-05, 1.24e-05, 1.07e-06, 8.82e-01, 8.82e-01]    [6.94e-05, 1.24e-05, 1.07e-06, 8.82e-01, 8.82e-01]    []  
40000     [8.60e-06, 4.02e-07, 2.07e-10, 5.87e-02, 5.87e-02]    [8.60e-06, 4.02e-07, 2.07e-10, 5.87e-02, 5.87e-02]    []  
                                                                                                                           30000     [9.74e-02, 1.73e-04, 4.19e-06, 9.42e+00, 9.42e+00]    [9.74e-02, 1.73e-04, 4.19e-06, 9.42e+00, 9.42e+00]    []  
36000     [4.76e-05, 1.55e-06, 1.55e-07, 9.42e-02, 9.42e-02]    [4.76e-05, 1.55e-06, 1.55e-07, 9.42e-02, 9.42e-02]    []  
15000     [6.78e-05, 1.19e-05, 1.75e-06, 4.75e-01, 4.75e-01]    [6.78e-05, 1.19e-05, 1.75e-06, 4.75e-01, 4.75e-01]    []  
41000     [5.39e-06, 2.62e-07, 2.12e-10, 4.86e-02, 4.86e-02]    [5.39e-06, 2.62e-07, 2.12e-10, 4.86e-02, 4.86e-02]    []  
31000     [2.50e-04, 1.61e-04, 6.77e-06, 3.92e-01, 3.92e-01]    [2.50e-04, 1.61e-04, 6.77e-06, 3.92e-01, 3.92e-01]    []  
37000     [1.61e-05, 9.25e-07, 3.90e-07, 9.02e-02, 9.02e-02]    [1.61e-05, 9.25e-07, 3.90e-07, 9.02e-02, 9.02e-02]    []  
16000     [6.63e-05, 1.07e-05, 3.54e-06, 4.20e-01, 4.20e-01]    [6.63e-05, 1.07e-05, 3.54e-06, 4.20e-01, 4.20e-01]    []  
42000     [1.45e-06, 1.42e-07, 5.19e-10, 7.15e-02, 7.15e-02]    [1.45e-06, 1.42e-07, 5.19e-10, 7.15e-02, 7.15e-02]    []  
                                                                                                                           32000     [1.74e-04, 1.63e-04, 4.15e-06, 3.94e-01, 3.94e-01]    [1.74e-04, 1.63e-04, 4.15e-06, 3.94e-01, 3.94e-01]    []  
38000     [3.04e-05, 2.03e-05, 1.42e-09, 8.99e-01, 8.99e-01]    [3.04e-05, 2.03e-05, 1.42e-09, 8.99e-01, 8.99e-01]    []  
17000     [6.27e-05, 9.09e-06, 1.41e-06, 3.34e-01, 3.34e-01]    [6.27e-05, 9.09e-06, 1.41e-06, 3.34e-01, 3.34e-01]    []  
43000     [1.04e-05, 1.03e-06, 2.76e-10, 9.43e-01, 9.43e-01]    [1.04e-05, 1.03e-06, 2.76e-10, 9.43e-01, 9.43e-01]    []  
33000     [2.98e-04, 1.67e-04, 4.39e-07, 1.40e-01, 1.40e-01]    [2.98e-04, 1.67e-04, 4.39e-07, 1.40e-01, 1.40e-01]    []  
18000     [6.15e-05, 8.66e-06, 5.53e-07, 2.19e-01, 2.19e-01]    [6.15e-05, 8.66e-06, 5.53e-07, 2.19e-01, 2.19e-01]    []  
39000     [7.36e-06, 2.62e-06, 4.21e-07, 4.41e-01, 4.41e-01]    [7.36e-06, 2.62e-06, 4.21e-07, 4.41e-01, 4.41e-01]    []  
44000     [8.94e-05, 2.82e-06, 1.47e-06, 2.75e-01, 2.75e-01]    [8.94e-05, 2.82e-06, 1.47e-06, 2.75e-01, 2.75e-01]    []  
                                                                                                                           34000     [3.12e-04, 1.81e-04, 1.79e-06, 4.54e-01, 4.54e-01]    [3.12e-04, 1.81e-04, 1.79e-06, 4.54e-01, 4.54e-01]    []  
19000     [5.98e-05, 9.57e-06, 1.59e-06, 9.38e-01, 9.38e-01]    [5.98e-05, 9.57e-06, 1.59e-06, 9.38e-01, 9.38e-01]    []  
40000     [1.49e-05, 9.97e-07, 2.92e-09, 3.56e-01, 3.56e-01]    [1.49e-05, 9.97e-07, 2.92e-09, 3.56e-01, 3.56e-01]    []  
45000     [1.11e-05, 4.54e-07, 7.26e-09, 4.86e-02, 4.86e-02]    [1.11e-05, 4.54e-07, 7.26e-09, 4.86e-02, 4.86e-02]    []  
                                                                                                                           35000     [5.57e-03, 2.10e-04, 3.77e-07, 3.58e+00, 3.58e+00]    [5.57e-03, 2.10e-04, 3.77e-07, 3.58e+00, 3.58e+00]    []  
20000     [5.74e-05, 8.17e-06, 2.88e-06, 4.91e-01, 4.91e-01]    [5.74e-05, 8.17e-06, 2.88e-06, 4.91e-01, 4.91e-01]    []  
41000     [2.29e-05, 1.94e-06, 4.27e-08, 1.62e-01, 1.62e-01]    [2.29e-05, 1.94e-06, 4.27e-08, 1.62e-01, 1.62e-01]    []  
46000     [3.90e-06, 1.40e-07, 2.83e-10, 3.57e-02, 3.57e-02]    [3.90e-06, 1.40e-07, 2.83e-10, 3.57e-02, 3.57e-02]    []  
                                                                                                                           36000     [3.61e-03, 2.39e-04, 4.92e-06, 1.57e-01, 1.57e-01]    [3.61e-03, 2.39e-04, 4.92e-06, 1.57e-01, 1.57e-01]    []  
21000     [5.85e-05, 6.94e-06, 2.06e-06, 3.95e-01, 3.95e-01]    [5.85e-05, 6.94e-06, 2.06e-06, 3.95e-01, 3.95e-01]    []  
47000     [1.45e-05, 2.54e-08, 5.26e-09, 1.14e+00, 1.14e+00]    [1.45e-05, 2.54e-08, 5.26e-09, 1.14e+00, 1.14e+00]    []  
42000     [1.36e-05, 6.15e-07, 1.59e-07, 4.38e+00, 4.38e+00]    [1.36e-05, 6.15e-07, 1.59e-07, 4.38e+00, 4.38e+00]    []  
37000     [5.72e-04, 2.85e-04, 9.37e-08, 7.01e-02, 7.01e-02]    [5.72e-04, 2.85e-04, 9.37e-08, 7.01e-02, 7.01e-02]    []  
22000     [3.80e-05, 8.31e-06, 3.05e-07, 3.27e-01, 3.27e-01]    [3.80e-05, 8.31e-06, 3.05e-07, 3.27e-01, 3.27e-01]    []  
48000     [9.42e-05, 9.36e-06, 3.86e-07, 1.22e+00, 1.22e+00]    [9.42e-05, 9.36e-06, 3.86e-07, 1.22e+00, 1.22e+00]    []  
43000     [6.67e-06, 5.20e-07, 2.95e-08, 6.62e-02, 6.62e-02]    [6.67e-06, 5.20e-07, 2.95e-08, 6.62e-02, 6.62e-02]    []  
                                                                                                                                                                                                                                                      38000     [1.42e-05, 2.92e-04, 2.86e-07, 7.29e-02, 7.29e-02]    [1.42e-05, 2.92e-04, 2.86e-07, 7.29e-02, 7.29e-02]    []  
23000     [3.52e-05, 8.64e-06, 2.62e-07, 1.34e-01, 1.34e-01]    [3.52e-05, 8.64e-06, 2.62e-07, 1.34e-01, 1.34e-01]    []  
49000     [7.68e-06, 8.53e-06, 5.61e-10, 3.50e-02, 3.50e-02]    [7.68e-06, 8.53e-06, 5.61e-10, 3.50e-02, 3.50e-02]    []  
44000     [9.20e-07, 8.35e-07, 6.19e-10, 1.04e-01, 1.04e-01]    [9.20e-07, 8.35e-07, 6.19e-10, 1.04e-01, 1.04e-01]    []  
39000     [3.39e-03, 3.05e-04, 5.61e-08, 4.60e-01, 4.60e-01]    [3.39e-03, 3.05e-04, 5.61e-08, 4.60e-01, 4.60e-01]    []  
24000     [3.81e-05, 8.69e-06, 5.45e-07, 1.23e-01, 1.23e-01]    [3.81e-05, 8.69e-06, 5.45e-07, 1.23e-01, 1.23e-01]    []  
50000     [4.89e-06, 2.03e-07, 9.17e-09, 3.78e-01, 3.78e-01]    [4.89e-06, 2.03e-07, 9.17e-09, 3.78e-01, 3.78e-01]    []  

Best model at step 49000:
  train loss: 7.00e-02
  test loss: 7.00e-02
  test metric: []

'train' took 11578.946003 s

[I 2023-10-09 09:23:41,439] Trial 40 finished with value: 0.9036588304902867 and parameters: {'num_domain': 76657, 'num_boundary': 3089, 'resampling_period': 5556, 'lr': 0.0055618665768740255}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000106 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.30e+01, 2.35e-04, 1.16e-07, 3.60e+03, 3.60e+03]    [4.30e+01, 2.35e-04, 1.16e-07, 3.60e+03, 3.60e+03]    []  
[W 2023-10-09 09:23:55,301] Trial 47 failed with parameters: {'num_domain': 87990, 'num_boundary': 7908, 'resampling_period': 12540, 'lr': 0.0015034834323713008} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 98.00 MiB (GPU 2; 10.75 GiB total capacity; 9.65 GiB already allocated; 75.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 2; 10.75 GiB total capacity; 9.65 GiB already allocated; 75.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-09 09:23:55,323] Trial 47 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 80, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 2; 10.75 GiB total capacity; 9.65 GiB already allocated; 75.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
45000     [1.42e-05, 1.11e-06, 9.48e-09, 1.57e-01, 1.57e-01]    [1.42e-05, 1.11e-06, 9.48e-09, 1.57e-01, 1.57e-01]    []  
40000     [1.39e-03, 3.14e-04, 5.97e-08, 3.05e-01, 3.05e-01]    [1.39e-03, 3.14e-04, 5.97e-08, 3.05e-01, 3.05e-01]    []  
25000     [4.32e-05, 9.90e-06, 3.47e-06, 1.84e-01, 1.84e-01]    [4.32e-05, 9.90e-06, 3.47e-06, 1.84e-01, 1.84e-01]    []  
46000     [1.27e-04, 5.78e-06, 9.41e-07, 8.23e-01, 8.23e-01]    [1.27e-04, 5.78e-06, 9.41e-07, 8.23e-01, 8.23e-01]    []  
                                                                                                                           41000     [2.45e-04, 3.24e-04, 9.42e-08, 8.84e-02, 8.84e-02]    [2.45e-04, 3.24e-04, 9.42e-08, 8.84e-02, 8.84e-02]    []  
26000     [4.34e-05, 1.08e-05, 1.65e-06, 5.02e-01, 5.02e-01]    [4.34e-05, 1.08e-05, 1.65e-06, 5.02e-01, 5.02e-01]    []  
                                                                                                                           47000     [3.37e-05, 2.81e-06, 2.55e-09, 1.92e-01, 1.92e-01]    [3.37e-05, 2.81e-06, 2.55e-09, 1.92e-01, 1.92e-01]    []  
42000     [3.85e-04, 3.35e-04, 9.26e-08, 8.07e-02, 8.07e-02]    [3.85e-04, 3.35e-04, 9.26e-08, 8.07e-02, 8.07e-02]    []  
27000     [4.40e-05, 9.27e-06, 7.13e-07, 9.74e-02, 9.74e-02]    [4.40e-05, 9.27e-06, 7.13e-07, 9.74e-02, 9.74e-02]    []  
                                                                                                                           48000     [4.42e-04, 7.26e-06, 1.09e-07, 2.59e-01, 2.59e-01]    [4.42e-04, 7.26e-06, 1.09e-07, 2.59e-01, 2.59e-01]    []  
28000     [4.28e-05, 9.79e-06, 1.51e-06, 1.04e-01, 1.04e-01]    [4.28e-05, 9.79e-06, 1.51e-06, 1.04e-01, 1.04e-01]    []  
43000     [1.14e-05, 3.64e-04, 1.47e-09, 5.53e-02, 5.53e-02]    [1.14e-05, 3.64e-04, 1.47e-09, 5.53e-02, 5.53e-02]    []  
                                                                                                                                                                                                                                                      49000     [9.66e-06, 4.68e-07, 8.88e-09, 8.35e-02, 8.35e-02]    [9.66e-06, 4.68e-07, 8.88e-09, 8.35e-02, 8.35e-02]    []  
29000     [3.98e-05, 8.42e-06, 8.35e-07, 2.48e-01, 2.48e-01]    [3.98e-05, 8.42e-06, 8.35e-07, 2.48e-01, 2.48e-01]    []  
44000     [2.18e-06, 3.65e-04, 2.38e-08, 4.34e-02, 4.34e-02]    [2.18e-06, 3.65e-04, 2.38e-08, 4.34e-02, 4.34e-02]    []  
50000     [6.76e-06, 4.11e-07, 6.71e-09, 1.32e-01, 1.32e-01]    [6.76e-06, 4.11e-07, 6.71e-09, 1.32e-01, 1.32e-01]    []  

Best model at step 29000:
  train loss: 1.21e-01
  test loss: 1.21e-01
  test metric: []

'train' took 12434.860704 s

[I 2023-10-09 09:45:20,775] Trial 42 finished with value: 0.8159008873560821 and parameters: {'num_domain': 76898, 'num_boundary': 9880, 'resampling_period': 4701, 'lr': 0.0062407970625817195}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000107 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.89e+01, 1.01e-04, 1.63e-07, 3.60e+03, 3.60e+03]    [1.89e+01, 1.01e-04, 1.63e-07, 3.60e+03, 3.60e+03]    []  
30000     [3.30e-05, 8.22e-06, 3.25e-07, 7.52e-02, 7.52e-02]    [3.30e-05, 8.22e-06, 3.25e-07, 7.52e-02, 7.52e-02]    []  
45000     [2.51e-05, 3.77e-04, 2.92e-08, 2.69e-01, 2.69e-01]    [2.51e-05, 3.77e-04, 2.92e-08, 2.69e-01, 2.69e-01]    []  
                                                                                                                                                                                                                                                      1000      [1.53e-10, 1.05e-03, 2.58e-16, 2.09e+03, 2.09e+03]    [1.53e-10, 1.05e-03, 2.58e-16, 2.09e+03, 2.09e+03]    []  
31000     [9.13e-05, 1.41e-05, 1.16e-06, 2.70e-01, 2.70e-01]    [9.13e-05, 1.41e-05, 1.16e-06, 2.70e-01, 2.70e-01]    []  
46000     [2.09e-02, 3.85e-04, 1.35e-07, 3.77e-02, 3.77e-02]    [2.09e-02, 3.85e-04, 1.35e-07, 3.77e-02, 3.77e-02]    []  
2000      [7.33e-03, 5.94e-04, 5.09e-04, 1.84e+01, 1.84e+01]    [7.33e-03, 5.94e-04, 5.09e-04, 1.84e+01, 1.84e+01]    []  
                                                                                                                           32000     [3.22e-05, 7.60e-06, 2.72e-07, 9.30e-02, 9.30e-02]    [3.22e-05, 7.60e-06, 2.72e-07, 9.30e-02, 9.30e-02]    []  
3000      [6.20e-04, 1.21e-03, 4.96e-06, 6.97e+00, 6.97e+00]    [6.20e-04, 1.21e-03, 4.96e-06, 6.97e+00, 6.97e+00]    []  
47000     [2.00e-04, 3.92e-04, 2.67e-08, 1.28e-01, 1.28e-01]    [2.00e-04, 3.92e-04, 2.67e-08, 1.28e-01, 1.28e-01]    []  
                                                                                                                           4000      [6.32e-04, 4.41e-04, 2.04e-06, 3.23e+00, 3.23e+00]    [6.32e-04, 4.41e-04, 2.04e-06, 3.23e+00, 3.23e+00]    []  
33000     [2.93e-05, 5.76e-06, 5.60e-07, 6.33e-02, 6.33e-02]    [2.93e-05, 5.76e-06, 5.60e-07, 6.33e-02, 6.33e-02]    []  
48000     [2.54e-06, 3.98e-04, 4.01e-09, 3.31e-02, 3.31e-02]    [2.54e-06, 3.98e-04, 4.01e-09, 3.31e-02, 3.31e-02]    []  
                                                                                                                           5000      [8.08e-04, 8.47e-04, 1.61e-05, 3.20e+00, 3.20e+00]    [8.08e-04, 8.47e-04, 1.61e-05, 3.20e+00, 3.20e+00]    []  
34000     [2.37e-05, 5.05e-06, 5.29e-07, 2.13e-01, 2.13e-01]    [2.37e-05, 5.05e-06, 5.29e-07, 2.13e-01, 2.13e-01]    []  
49000     [1.85e-06, 4.52e-04, 2.33e-09, 3.12e-02, 3.12e-02]    [1.85e-06, 4.52e-04, 2.33e-09, 3.12e-02, 3.12e-02]    []  
6000      [8.68e-04, 9.01e-04, 2.09e-05, 2.06e+00, 2.06e+00]    [8.68e-04, 9.01e-04, 2.09e-05, 2.06e+00, 2.06e+00]    []  
                                                                                                                                                                                                                                                      7000      [6.49e-04, 9.81e-04, 2.93e-05, 1.62e+00, 1.62e+00]    [6.49e-04, 9.81e-04, 2.93e-05, 1.62e+00, 1.62e+00]    []  
35000     [2.43e-05, 3.40e-06, 1.84e-06, 2.14e-01, 2.14e-01]    [2.43e-05, 3.40e-06, 1.84e-06, 2.14e-01, 2.14e-01]    []  
50000     [1.07e-02, 4.58e-04, 3.22e-09, 3.05e-02, 3.05e-02]    [1.07e-02, 4.58e-04, 3.22e-09, 3.05e-02, 3.05e-02]    []  

Best model at step 49000:
  train loss: 6.28e-02
  test loss: 6.28e-02
  test metric: []

'train' took 11840.254575 s

[I 2023-10-09 10:05:47,998] Trial 43 finished with value: 0.7144031913506163 and parameters: {'num_domain': 74039, 'num_boundary': 6155, 'resampling_period': 20008, 'lr': 0.004932851159653569}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000109 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.30e+01, 3.49e-05, 2.69e-07, 3.63e+03, 3.63e+03]    [3.30e+01, 3.49e-05, 2.69e-07, 3.63e+03, 3.63e+03]    []  
[W 2023-10-09 10:06:01,228] Trial 49 failed with parameters: {'num_domain': 87661, 'num_boundary': 7860, 'resampling_period': 12812, 'lr': 0.0012658451876754611} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 98.00 MiB (GPU 4; 10.75 GiB total capacity; 9.63 GiB already allocated; 75.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 4; 10.75 GiB total capacity; 9.63 GiB already allocated; 75.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-09 10:06:01,242] Trial 49 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 80, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 4; 10.75 GiB total capacity; 9.63 GiB already allocated; 75.62 MiB free; 9.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
8000      [7.87e-04, 9.85e-04, 8.21e-05, 1.09e+00, 1.09e+00]    [7.87e-04, 9.85e-04, 8.21e-05, 1.09e+00, 1.09e+00]    []  
36000     [1.95e-05, 3.11e-06, 4.59e-07, 1.16e-01, 1.16e-01]    [1.95e-05, 3.11e-06, 4.59e-07, 1.16e-01, 1.16e-01]    []  
9000      [6.87e-04, 9.14e-04, 6.82e-05, 1.17e+00, 1.17e+00]    [6.87e-04, 9.14e-04, 6.82e-05, 1.17e+00, 1.17e+00]    []  
                                                                                                                                                                                                                                                      37000     [3.41e-05, 3.55e-06, 2.12e-06, 3.59e-01, 3.59e-01]    [3.41e-05, 3.55e-06, 2.12e-06, 3.59e-01, 3.59e-01]    []  
10000     [5.70e-04, 6.65e-04, 8.36e-05, 4.71e-01, 4.71e-01]    [5.70e-04, 6.65e-04, 8.36e-05, 4.71e-01, 4.71e-01]    []  
11000     [3.39e-04, 3.22e-04, 3.49e-05, 3.61e-01, 3.61e-01]    [3.39e-04, 3.22e-04, 3.49e-05, 3.61e-01, 3.61e-01]    []  
38000     [2.95e-05, 3.69e-06, 3.32e-07, 4.86e-02, 4.86e-02]    [2.95e-05, 3.69e-06, 3.32e-07, 4.86e-02, 4.86e-02]    []  
                                                                                                                           12000     [3.43e-04, 1.47e-04, 8.83e-05, 9.33e-01, 9.33e-01]    [3.43e-04, 1.47e-04, 8.83e-05, 9.33e-01, 9.33e-01]    []  
                                                                                                                           39000     [2.06e-05, 2.58e-06, 4.57e-07, 4.54e-02, 4.54e-02]    [2.06e-05, 2.58e-06, 4.57e-07, 4.54e-02, 4.54e-02]    []  
13000     [2.99e-04, 1.39e-04, 4.76e-05, 4.63e-01, 4.63e-01]    [2.99e-04, 1.39e-04, 4.76e-05, 4.63e-01, 4.63e-01]    []  
                                                                                                                           40000     [2.07e-05, 2.54e-06, 1.36e-07, 4.24e-02, 4.24e-02]    [2.07e-05, 2.54e-06, 1.36e-07, 4.24e-02, 4.24e-02]    []  
14000     [2.89e-04, 1.31e-04, 6.27e-05, 1.02e+00, 1.02e+00]    [2.89e-04, 1.31e-04, 6.27e-05, 1.02e+00, 1.02e+00]    []  
15000     [2.59e-04, 1.15e-04, 2.43e-05, 2.03e-01, 2.03e-01]    [2.59e-04, 1.15e-04, 2.43e-05, 2.03e-01, 2.03e-01]    []  
41000     [2.23e-05, 3.00e-06, 8.68e-07, 1.01e-01, 1.01e-01]    [2.23e-05, 3.00e-06, 8.68e-07, 1.01e-01, 1.01e-01]    []  
                                                                                                                           16000     [2.02e-04, 1.20e-04, 3.93e-05, 3.45e-01, 3.45e-01]    [2.02e-04, 1.20e-04, 3.93e-05, 3.45e-01, 3.45e-01]    []  
42000     [2.16e-05, 2.93e-06, 3.78e-07, 5.49e-02, 5.49e-02]    [2.16e-05, 2.93e-06, 3.78e-07, 5.49e-02, 5.49e-02]    []  
17000     [1.64e-04, 1.00e-04, 2.27e-05, 1.62e-01, 1.62e-01]    [1.64e-04, 1.00e-04, 2.27e-05, 1.62e-01, 1.62e-01]    []  
                                                                                                                           18000     [1.78e-04, 8.44e-05, 4.71e-05, 4.74e-01, 4.74e-01]    [1.78e-04, 8.44e-05, 4.71e-05, 4.74e-01, 4.74e-01]    []  
43000     [1.49e-05, 2.46e-06, 7.53e-07, 3.59e-02, 3.59e-02]    [1.49e-05, 2.46e-06, 7.53e-07, 3.59e-02, 3.59e-02]    []  
19000     [1.33e-04, 9.30e-05, 1.60e-05, 5.57e-01, 5.57e-01]    [1.33e-04, 9.30e-05, 1.60e-05, 5.57e-01, 5.57e-01]    []  
44000     [1.48e-05, 2.34e-06, 1.37e-07, 3.46e-02, 3.46e-02]    [1.48e-05, 2.34e-06, 1.37e-07, 3.46e-02, 3.46e-02]    []  
                                                                                                                           20000     [1.61e-04, 6.40e-05, 8.23e-05, 2.82e-01, 2.82e-01]    [1.61e-04, 6.40e-05, 8.23e-05, 2.82e-01, 2.82e-01]    []  
45000     [1.86e-05, 2.55e-06, 5.02e-07, 9.38e-02, 9.38e-02]    [1.86e-05, 2.55e-06, 5.02e-07, 9.38e-02, 9.38e-02]    []  
21000     [1.17e-04, 8.08e-05, 1.83e-05, 1.49e-01, 1.49e-01]    [1.17e-04, 8.08e-05, 1.83e-05, 1.49e-01, 1.49e-01]    []  
                                                                                                                           22000     [1.05e-04, 7.17e-05, 4.19e-05, 1.00e-01, 1.00e-01]    [1.05e-04, 7.17e-05, 4.19e-05, 1.00e-01, 1.00e-01]    []  
46000     [1.38e-05, 1.51e-06, 9.68e-07, 3.35e-02, 3.35e-02]    [1.38e-05, 1.51e-06, 9.68e-07, 3.35e-02, 3.35e-02]    []  
23000     [1.51e-04, 6.24e-05, 4.84e-05, 1.49e+00, 1.49e+00]    [1.51e-04, 6.24e-05, 4.84e-05, 1.49e+00, 1.49e+00]    []  
                                                                                                                           47000     [1.52e-05, 1.40e-06, 3.33e-07, 8.25e-02, 8.25e-02]    [1.52e-05, 1.40e-06, 3.33e-07, 8.25e-02, 8.25e-02]    []  
                                                                                                                           24000     [2.83e-04, 1.14e-04, 3.50e-05, 1.97e-01, 1.97e-01]    [2.83e-04, 1.14e-04, 3.50e-05, 1.97e-01, 1.97e-01]    []  
48000     [1.42e-05, 1.87e-06, 2.66e-07, 4.27e-02, 4.27e-02]    [1.42e-05, 1.87e-06, 2.66e-07, 4.27e-02, 4.27e-02]    []  
25000     [1.30e-04, 4.96e-05, 1.25e-04, 5.14e-01, 5.14e-01]    [1.30e-04, 4.96e-05, 1.25e-04, 5.14e-01, 5.14e-01]    []  
26000     [9.02e-05, 5.14e-05, 2.41e-05, 6.26e-02, 6.26e-02]    [9.02e-05, 5.14e-05, 2.41e-05, 6.26e-02, 6.26e-02]    []  
49000     [3.77e-05, 2.38e-06, 8.48e-06, 1.91e-01, 1.91e-01]    [3.77e-05, 2.38e-06, 8.48e-06, 1.91e-01, 1.91e-01]    []  
                                                                                                                           27000     [7.68e-05, 4.64e-05, 2.53e-05, 6.88e-02, 6.88e-02]    [7.68e-05, 4.64e-05, 2.53e-05, 6.88e-02, 6.88e-02]    []  
50000     [1.73e-05, 1.63e-06, 1.15e-06, 5.57e-02, 5.57e-02]    [1.73e-05, 1.63e-06, 1.15e-06, 5.57e-02, 5.57e-02]    []  

Best model at step 46000:
  train loss: 6.70e-02
  test loss: 6.70e-02
  test metric: []

'train' took 11560.010685 s

[I 2023-10-09 11:02:58,266] Trial 44 finished with value: 0.5403845120646718 and parameters: {'num_domain': 75523, 'num_boundary': 3135, 'resampling_period': 5009, 'lr': 0.0012171989494007246}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000107 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.19e+01, 1.76e-04, 3.97e-07, 3.62e+03, 3.62e+03]    [1.19e+01, 1.76e-04, 3.97e-07, 3.62e+03, 3.62e+03]    []  
[W 2023-10-09 11:03:12,269] Trial 50 failed with parameters: {'num_domain': 92127, 'num_boundary': 7963, 'resampling_period': 13214, 'lr': 0.00114171689298802} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 100.00 MiB (GPU 5; 10.75 GiB total capacity; 9.66 GiB already allocated; 97.62 MiB free; 9.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 5; 10.75 GiB total capacity; 9.66 GiB already allocated; 97.62 MiB free; 9.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-09 11:03:12,278] Trial 50 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 80, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 5; 10.75 GiB total capacity; 9.66 GiB already allocated; 97.62 MiB free; 9.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
28000     [7.51e-05, 3.67e-05, 6.44e-06, 5.19e-02, 5.19e-02]    [7.51e-05, 3.67e-05, 6.44e-06, 5.19e-02, 5.19e-02]    []  
                                                                                                                           29000     [1.10e-04, 4.17e-05, 6.68e-05, 5.58e-02, 5.58e-02]    [1.10e-04, 4.17e-05, 6.68e-05, 5.58e-02, 5.58e-02]    []  
                                                                                                                           30000     [1.07e-04, 3.69e-05, 1.67e-04, 3.58e-01, 3.58e-01]    [1.07e-04, 3.69e-05, 1.67e-04, 3.58e-01, 3.58e-01]    []  
                                                                                                                           31000     [9.90e-05, 3.92e-05, 5.21e-05, 4.99e-02, 4.99e-02]    [9.90e-05, 3.92e-05, 5.21e-05, 4.99e-02, 4.99e-02]    []  
                                                                                                                           32000     [6.27e-05, 6.23e-05, 3.71e-05, 3.90e-02, 3.90e-02]    [6.27e-05, 6.23e-05, 3.71e-05, 3.90e-02, 3.90e-02]    []  
                                                                                                                           33000     [1.34e-04, 6.79e-05, 3.23e-05, 1.37e-01, 1.37e-01]    [1.34e-04, 6.79e-05, 3.23e-05, 1.37e-01, 1.37e-01]    []  
                                                                                                                           34000     [7.04e-05, 5.68e-05, 6.01e-05, 9.95e-02, 9.95e-02]    [7.04e-05, 5.68e-05, 6.01e-05, 9.95e-02, 9.95e-02]    []  
35000     [6.59e-05, 4.36e-05, 5.31e-06, 3.26e-02, 3.26e-02]    [6.59e-05, 4.36e-05, 5.31e-06, 3.26e-02, 3.26e-02]    []  
36000     [5.49e-05, 4.59e-05, 6.67e-06, 3.06e-02, 3.06e-02]    [5.49e-05, 4.59e-05, 6.67e-06, 3.06e-02, 3.06e-02]    []  
                                                                                                                                                                                                                                                      37000     [5.75e-05, 4.45e-05, 7.11e-06, 2.97e-02, 2.97e-02]    [5.75e-05, 4.45e-05, 7.11e-06, 2.97e-02, 2.97e-02]    []  
38000     [1.07e-04, 4.96e-05, 3.14e-05, 8.93e-01, 8.93e-01]    [1.07e-04, 4.96e-05, 3.14e-05, 8.93e-01, 8.93e-01]    []  
                                                                                                                           39000     [4.31e-05, 4.76e-05, 3.84e-06, 2.68e-02, 2.68e-02]    [4.31e-05, 4.76e-05, 3.84e-06, 2.68e-02, 2.68e-02]    []  
                                                                                                                           40000     [1.83e-04, 1.25e-05, 3.02e-05, 8.19e-01, 8.19e-01]    [1.83e-04, 1.25e-05, 3.02e-05, 8.19e-01, 8.19e-01]    []  
                                                                                                                           41000     [4.02e-05, 4.44e-05, 9.36e-07, 2.44e-02, 2.44e-02]    [4.02e-05, 4.44e-05, 9.36e-07, 2.44e-02, 2.44e-02]    []  
                                                                                                                           42000     [3.09e-04, 1.38e-05, 2.95e-05, 7.94e-01, 7.94e-01]    [3.09e-04, 1.38e-05, 2.95e-05, 7.94e-01, 7.94e-01]    []  
36000     [7.48e-05, 5.20e-05, 5.58e-06, 4.84e-02, 4.84e-02]    [7.48e-05, 5.20e-05, 5.58e-06, 4.84e-02, 4.84e-02]    []  
                                                                                                                                                                                                                                                      44000     [7.01e-05, 2.46e-06, 1.32e-09, 2.01e+00, 2.01e+00]    [7.01e-05, 2.46e-06, 1.32e-09, 2.01e+00, 2.01e+00]    []  
37000     [8.68e-05, 5.03e-05, 2.50e-05, 1.97e-01, 1.97e-01]    [8.68e-05, 5.03e-05, 2.50e-05, 1.97e-01, 1.97e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                 38000     [8.76e-05, 4.64e-05, 2.89e-05, 1.89e-01, 1.89e-01]    [8.76e-05, 4.64e-05, 2.89e-05, 1.89e-01, 1.89e-01]    []  
45000     [2.79e-05, 4.16e-07, 1.31e-09, 3.81e+00, 3.81e+00]    [2.79e-05, 4.16e-07, 1.31e-09, 3.81e+00, 3.81e+00]    []  
                                                                                                                                                                                                                                                      39000     [1.11e-04, 4.40e-05, 1.65e-05, 2.51e-01, 2.51e-01]    [1.11e-04, 4.40e-05, 1.65e-05, 2.51e-01, 2.51e-01]    []  
46000     [1.65e-05, 3.87e-06, 6.89e-10, 2.15e+00, 2.15e+00]    [1.65e-05, 3.87e-06, 6.89e-10, 2.15e+00, 2.15e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          40000     [7.25e-05, 4.26e-05, 2.89e-05, 7.71e-02, 7.71e-02]    [7.25e-05, 4.26e-05, 2.89e-05, 7.71e-02, 7.71e-02]    []  
                                                                                                                           47000     [7.63e-06, 3.45e-07, 5.89e-09, 3.88e-02, 3.88e-02]    [7.63e-06, 3.45e-07, 5.89e-09, 3.88e-02, 3.88e-02]    []  
                                                                                                                           41000     [1.34e-04, 4.34e-05, 2.85e-05, 9.48e-02, 9.48e-02]    [1.34e-04, 4.34e-05, 2.85e-05, 9.48e-02, 9.48e-02]    []  
                                                                                                                           48000     [7.32e-06, 3.35e-07, 1.90e-09, 4.29e-02, 4.29e-02]    [7.32e-06, 3.35e-07, 1.90e-09, 4.29e-02, 4.29e-02]    []  
42000     [1.24e-04, 4.16e-05, 1.18e-05, 4.15e-01, 4.15e-01]    [1.24e-04, 4.16e-05, 1.18e-05, 4.15e-01, 4.15e-01]    []  
                                                                                                                                                                                                                                                      43000     [1.19e-04, 4.27e-05, 3.38e-05, 3.70e-01, 3.70e-01]    [1.19e-04, 4.27e-05, 3.38e-05, 3.70e-01, 3.70e-01]    []  
49000     [7.98e-06, 4.22e-07, 1.13e-09, 3.88e-02, 3.88e-02]    [7.98e-06, 4.22e-07, 1.13e-09, 3.88e-02, 3.88e-02]    []  
6000      [5.43e-04, 3.07e-04, 4.26e-06, 3.37e+00, 3.37e+00]    [5.43e-04, 3.07e-04, 4.26e-06, 3.37e+00, 3.37e+00]    []  
                                                                                                                           7000      [3.26e-04, 1.55e-04, 3.48e-07, 3.50e+00, 3.50e+00]    [3.26e-04, 1.55e-04, 3.48e-07, 3.50e+00, 3.50e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         8000      [6.14e-04, 3.52e-04, 5.90e-07, 2.04e+00, 2.04e+00]    [6.14e-04, 3.52e-04, 5.90e-07, 2.04e+00, 2.04e+00]    []  
                                                                                                                           9000      [8.00e-04, 6.34e-04, 3.63e-06, 1.32e+00, 1.32e+00]    [8.00e-04, 6.34e-04, 3.63e-06, 1.32e+00, 1.32e+00]    []  
                                                                                                                                                                                                                                                      10000     [8.58e-04, 8.00e-04, 1.45e-06, 1.41e+00, 1.41e+00]    [8.58e-04, 8.00e-04, 1.45e-06, 1.41e+00, 1.41e+00]    []  
                                                                                                                                                                                                                                                      11000     [8.67e-04, 8.47e-04, 5.48e-06, 1.09e+00, 1.09e+00]    [8.67e-04, 8.47e-04, 5.48e-06, 1.09e+00, 1.09e+00]    []  
12000     [8.51e-04, 8.39e-04, 5.05e-07, 5.85e-01, 5.85e-01]    [8.51e-04, 8.39e-04, 5.05e-07, 5.85e-01, 5.85e-01]    []  
                                                                                                                                                                                                                                                      13000     [7.30e-04, 5.57e-04, 1.97e-06, 7.73e-01, 7.73e-01]    [7.30e-04, 5.57e-04, 1.97e-06, 7.73e-01, 7.73e-01]    []  
                                                                                                                           14000     [5.57e-04, 3.65e-04, 1.31e-06, 8.48e-01, 8.48e-01]    [5.57e-04, 3.65e-04, 1.31e-06, 8.48e-01, 8.48e-01]    []  
                                                                                                                           15000     [4.32e-04, 3.06e-04, 8.56e-06, 8.10e-01, 8.10e-01]    [4.32e-04, 3.06e-04, 8.56e-06, 8.10e-01, 8.10e-01]    []  
50000     [4.31e-05, 2.61e-05, 8.47e-06, 1.06e-01, 1.06e-01]    [4.31e-05, 2.61e-05, 8.47e-06, 1.06e-01, 1.06e-01]    []  

Best model at step 47000:
  train loss: 5.37e-02
  test loss: 5.37e-02
  test metric: []

'train' took 17798.956308 s

[I 2023-10-09 13:08:26,649] Trial 45 finished with value: 0.5765647055809131 and parameters: {'num_domain': 90464, 'num_boundary': 6185, 'resampling_period': 13435, 'lr': 0.0011902066211163186}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000170 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [7.38e+00, 2.09e-04, 2.44e-07, 3.61e+03, 3.61e+03]    [7.38e+00, 2.09e-04, 2.44e-07, 3.61e+03, 3.61e+03]    []  
                                                                                                                           5000      [1.16e-04, 1.06e-04, 6.68e-06, 5.55e+00, 5.55e+00]    [1.16e-04, 1.06e-04, 6.68e-06, 5.55e+00, 5.55e+00]    []  
1000      [2.16e-02, 4.38e-04, 3.86e-07, 1.37e+02, 1.37e+02]    [2.16e-02, 4.38e-04, 3.86e-07, 1.37e+02, 1.37e+02]    []  
                                                                                                                                                                                                                                                      2000      [2.20e-03, 7.77e-04, 4.10e-05, 7.76e+00, 7.76e+00]    [2.20e-03, 7.77e-04, 4.10e-05, 7.76e+00, 7.76e+00]    []  
6000      [9.00e-04, 8.27e-04, 1.31e-05, 2.85e+00, 2.85e+00]    [9.00e-04, 8.27e-04, 1.31e-05, 2.85e+00, 2.85e+00]    []  
                                                                                                                           3000      [9.74e-04, 1.07e-03, 2.04e-05, 4.48e+00, 4.48e+00]    [9.74e-04, 1.07e-03, 2.04e-05, 4.48e+00, 4.48e+00]    []  
                                                                                                                           7000      [7.07e-04, 1.72e-03, 3.94e-07, 2.09e+00, 2.09e+00]    [7.07e-04, 1.72e-03, 3.94e-07, 2.09e+00, 2.09e+00]    []  
4000      [1.14e-03, 7.15e-04, 3.19e-05, 3.25e+00, 3.25e+00]    [1.14e-03, 7.15e-04, 3.19e-05, 3.25e+00, 3.25e+00]    []  
                                                                                                                           8000      [8.55e-04, 1.58e-03, 1.97e-07, 1.62e+00, 1.62e+00]    [8.55e-04, 1.58e-03, 1.97e-07, 1.62e+00, 1.62e+00]    []  
                                                                                                                           5000      [7.59e-04, 4.08e-04, 3.18e-05, 1.42e+00, 1.42e+00]    [7.59e-04, 4.08e-04, 3.18e-05, 1.42e+00, 1.42e+00]    []  
                                                                                                                           6000      [6.97e-04, 4.73e-04, 1.04e-04, 1.07e+00, 1.07e+00]    [6.97e-04, 4.73e-04, 1.04e-04, 1.07e+00, 1.07e+00]    []  
9000      [9.18e-04, 1.36e-03, 4.58e-07, 1.03e+00, 1.03e+00]    [9.18e-04, 1.36e-03, 4.58e-07, 1.03e+00, 1.03e+00]    []  
                                                                                                                                                                                                                                                      7000      [8.19e-04, 5.18e-04, 2.79e-05, 1.49e+00, 1.49e+00]    [8.19e-04, 5.18e-04, 2.79e-05, 1.49e+00, 1.49e+00]    []  
10000     [5.56e-04, 4.12e-04, 1.86e-06, 1.07e+00, 1.07e+00]    [5.56e-04, 4.12e-04, 1.86e-06, 1.07e+00, 1.07e+00]    []  
                                                                                                                           8000      [8.12e-04, 8.15e-04, 2.29e-05, 7.10e-01, 7.10e-01]    [8.12e-04, 8.15e-04, 2.29e-05, 7.10e-01, 7.10e-01]    []  
                                                                                                                           11000     [3.80e-04, 1.87e-04, 2.33e-07, 8.49e-01, 8.49e-01]    [3.80e-04, 1.87e-04, 2.33e-07, 8.49e-01, 8.49e-01]    []  
9000      [7.19e-04, 9.92e-04, 3.17e-05, 8.97e-01, 8.97e-01]    [7.19e-04, 9.92e-04, 3.17e-05, 8.97e-01, 8.97e-01]    []  
                                                                                                                                                                                                                                                      10000     [7.38e-04, 1.09e-03, 4.76e-05, 1.90e+00, 1.90e+00]    [7.38e-04, 1.09e-03, 4.76e-05, 1.90e+00, 1.90e+00]    []  
12000     [3.50e-04, 1.33e-04, 5.79e-07, 1.28e+00, 1.28e+00]    [3.50e-04, 1.33e-04, 5.79e-07, 1.28e+00, 1.28e+00]    []  
                                                                                                                           11000     [5.32e-04, 1.18e-03, 1.08e-04, 1.72e+00, 1.72e+00]    [5.32e-04, 1.18e-03, 1.08e-04, 1.72e+00, 1.72e+00]    []  
                                                                                                                           13000     [2.66e-04, 8.10e-05, 2.28e-07, 7.26e-01, 7.26e-01]    [2.66e-04, 8.10e-05, 2.28e-07, 7.26e-01, 7.26e-01]    []  
                                                                                                                           12000     [4.54e-04, 1.02e-03, 3.10e-04, 2.56e-01, 2.56e-01]    [4.54e-04, 1.02e-03, 3.10e-04, 2.56e-01, 2.56e-01]    []  
14000     [2.27e-04, 6.94e-05, 5.72e-08, 6.53e-01, 6.53e-01]    [2.27e-04, 6.94e-05, 5.72e-08, 6.53e-01, 6.53e-01]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.

13000     [3.92e-04, 8.40e-04, 1.61e-04, 2.62e-01, 2.62e-01]    [3.92e-04, 8.40e-04, 1.61e-04, 2.62e-01, 2.62e-01]    []  
[I 2023-10-09 14:26:13,384] Using an existing study with name 'heartpinn' instead of creating a new one.
[I 2023-10-09 14:26:13,386] Using an existing study with name 'heartpinn' instead of creating a new one.
[I 2023-10-09 14:26:13,405] Using an existing study with name 'heartpinn' instead of creating a new one.
[I 2023-10-09 14:26:13,435] Using an existing study with name 'heartpinn' instead of creating a new one.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000224 s

Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000204 s

Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000371 s

Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000255 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.53e+01, 4.76e-05, 1.85e-07, 3.61e+03, 3.61e+03]    [3.53e+01, 4.76e-05, 1.85e-07, 3.61e+03, 3.61e+03]    []  
Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.95e+01, 4.19e-05, 7.77e-08, 3.61e+03, 3.61e+03]    [4.95e+01, 4.19e-05, 7.77e-08, 3.61e+03, 3.61e+03]    []  
Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.28e+01, 4.85e-05, 1.76e-07, 3.60e+03, 3.60e+03]    [3.28e+01, 4.85e-05, 1.76e-07, 3.60e+03, 3.60e+03]    []  
Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [8.56e+01, 5.13e-04, 2.55e-07, 3.61e+03, 3.61e+03]    [8.56e+01, 5.13e-04, 2.55e-07, 3.61e+03, 3.61e+03]    []  
34000     [1.68e-04, 6.76e-05, 7.16e-06, 6.36e-02, 6.36e-02]    [1.68e-04, 6.76e-05, 7.16e-06, 6.36e-02, 6.36e-02]    []  
                                                                                                                                                                                                                                                      35000     [2.64e-04, 5.72e-05, 3.47e-05, 2.79e+00, 2.79e+00]    [2.64e-04, 5.72e-05, 3.47e-05, 2.79e+00, 2.79e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                 36000     [2.85e-04, 6.83e-05, 2.29e-05, 4.64e-01, 4.64e-01]    [2.85e-04, 6.83e-05, 2.29e-05, 4.64e-01, 4.64e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                 37000     [1.61e-04, 7.34e-05, 9.16e-06, 5.40e-02, 5.40e-02]    [1.61e-04, 7.34e-05, 9.16e-06, 5.40e-02, 5.40e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            38000     [1.72e-04, 7.96e-05, 2.71e-05, 8.62e-02, 8.62e-02]    [1.72e-04, 7.96e-05, 2.71e-05, 8.62e-02, 8.62e-02]    []  
                                                                                                                           39000     [1.69e-04, 5.62e-05, 1.71e-05, 6.56e-01, 6.56e-01]    [1.69e-04, 5.62e-05, 1.71e-05, 6.56e-01, 6.56e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       40000     [1.77e-04, 4.80e-05, 2.99e-06, 4.87e-02, 4.87e-02]    [1.77e-04, 4.80e-05, 2.99e-06, 4.87e-02, 4.87e-02]    []  
                                                                                                                                                                                                                                                      41000     [1.51e-04, 4.53e-05, 1.06e-05, 7.18e-02, 7.18e-02]    [1.51e-04, 4.53e-05, 1.06e-05, 7.18e-02, 7.18e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            42000     [1.60e-04, 4.14e-05, 2.24e-06, 4.31e-02, 4.31e-02]    [1.60e-04, 4.14e-05, 2.24e-06, 4.31e-02, 4.31e-02]    []  
43000     [1.81e-04, 3.57e-05, 9.62e-06, 5.35e-02, 5.35e-02]    [1.81e-04, 3.57e-05, 9.62e-06, 5.35e-02, 5.35e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       44000     [1.27e-04, 3.62e-05, 1.02e-05, 3.91e-02, 3.91e-02]    [1.27e-04, 3.62e-05, 1.02e-05, 3.91e-02, 3.91e-02]    []  
                                                                                                                                                                                                                                                      45000     [1.24e-04, 4.37e-05, 2.02e-06, 3.75e-02, 3.75e-02]    [1.24e-04, 4.37e-05, 2.02e-06, 3.75e-02, 3.75e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            46000     [1.33e-04, 4.73e-05, 6.40e-06, 3.71e-02, 3.71e-02]    [1.33e-04, 4.73e-05, 6.40e-06, 3.71e-02, 3.71e-02]    []  
                                                                                                                                                                                                                                                      47000     [1.91e-04, 4.98e-05, 1.51e-05, 4.56e-02, 4.56e-02]    [1.91e-04, 4.98e-05, 1.51e-05, 4.56e-02, 4.56e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            48000     [1.23e-04, 5.63e-05, 1.19e-05, 3.43e-02, 3.43e-02]    [1.23e-04, 5.63e-05, 1.19e-05, 3.43e-02, 3.43e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       49000     [1.65e-04, 5.25e-05, 1.50e-05, 1.82e-01, 1.82e-01]    [1.65e-04, 5.25e-05, 1.50e-05, 1.82e-01, 1.82e-01]    []  
50000     [1.24e-04, 4.95e-05, 2.77e-05, 3.03e-02, 3.03e-02]    [1.24e-04, 4.95e-05, 2.77e-05, 3.03e-02, 3.03e-02]    []  

Best model at step 50000:
  train loss: 6.09e-02
  test loss: 6.09e-02
  test metric: []

'train' took 12575.460133 s

[I 2023-10-09 15:35:00,786] Trial 51 finished with value: 0.5055170448250386 and parameters: {'num_domain': 85916, 'num_boundary': 5098, 'resampling_period': 12747, 'lr': 0.0010609741113934228}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000285 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.54e+01, 1.07e-04, 7.26e-07, 3.61e+03, 3.61e+03]    [3.54e+01, 1.07e-04, 7.26e-07, 3.61e+03, 3.61e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1000      [9.35e-02, 1.02e-03, 3.37e-07, 5.81e+02, 5.81e+02]    [9.35e-02, 1.02e-03, 3.37e-07, 5.81e+02, 5.81e+02]    []  
                                                                                                                           2000      [2.57e-05, 2.47e-04, 5.21e-07, 6.55e+00, 6.55e+00]    [2.57e-05, 2.47e-04, 5.21e-07, 6.55e+00, 6.55e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       3000      [2.83e-05, 3.34e-05, 3.20e-08, 5.19e+00, 5.19e+00]    [2.83e-05, 3.34e-05, 3.20e-08, 5.19e+00, 5.19e+00]    []  
                                                                                                                                                                                                                                                      4000      [4.14e-05, 1.30e-05, 1.83e-08, 8.12e+00, 8.12e+00]    [4.14e-05, 1.30e-05, 1.83e-08, 8.12e+00, 8.12e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       5000      [1.04e-04, 1.26e-05, 3.85e-08, 1.86e+00, 1.86e+00]    [1.04e-04, 1.26e-05, 3.85e-08, 1.86e+00, 1.86e+00]    []  
                                                                                                                                                                                                                                                      6000      [1.19e-04, 2.51e-05, 2.10e-08, 1.17e+00, 1.17e+00]    [1.19e-04, 2.51e-05, 2.10e-08, 1.17e+00, 1.17e+00]    []  
12000     [8.89e-04, 7.44e-04, 1.63e-05, 3.28e+00, 3.28e+00]    [8.89e-04, 7.44e-04, 1.63e-05, 3.28e+00, 3.28e+00]    []  
12000     [5.17e-04, 5.66e-04, 1.89e-06, 4.50e-01, 4.50e-01]    [5.17e-04, 5.66e-04, 1.89e-06, 4.50e-01, 4.50e-01]    []  
11000     [5.60e-04, 9.00e-04, 1.58e-05, 3.40e-01, 3.40e-01]    [5.60e-04, 9.00e-04, 1.58e-05, 3.40e-01, 3.40e-01]    []  
10000     [5.69e-04, 4.57e-04, 1.95e-05, 5.19e+00, 5.19e+00]    [5.69e-04, 4.57e-04, 1.95e-05, 5.19e+00, 5.19e+00]    []  
28000     [1.25e-04, 4.25e-05, 8.74e-07, 3.33e-01, 3.33e-01]    [1.25e-04, 4.25e-05, 8.74e-07, 3.33e-01, 3.33e-01]    []  
30000     [1.56e-04, 3.98e-05, 2.68e-05, 3.73e+00, 3.73e+00]    [1.56e-04, 3.98e-05, 2.68e-05, 3.73e+00, 3.73e+00]    []  
                                                                                                                           13000     [7.46e-04, 9.15e-04, 2.20e-05, 2.42e+00, 2.42e+00]    [7.46e-04, 9.15e-04, 2.20e-05, 2.42e+00, 2.42e+00]    []  
13000     [4.77e-04, 4.26e-04, 1.23e-05, 2.57e-01, 2.57e-01]    [4.77e-04, 4.26e-04, 1.23e-05, 2.57e-01, 2.57e-01]    []  
31000     [7.06e-05, 3.53e-05, 8.11e-06, 4.20e-02, 4.20e-02]    [7.06e-05, 3.53e-05, 8.11e-06, 4.20e-02, 4.20e-02]    []  
29000     [1.42e-04, 6.23e-05, 2.66e-07, 1.01e-01, 1.01e-01]    [1.42e-04, 6.23e-05, 2.66e-07, 1.01e-01, 1.01e-01]    []  
12000     [4.78e-04, 8.71e-04, 2.32e-05, 2.28e-01, 2.28e-01]    [4.78e-04, 8.71e-04, 2.32e-05, 2.28e-01, 2.28e-01]    []  
                                                                                                                           11000     [4.57e-04, 3.33e-04, 2.93e-06, 4.49e+00, 4.49e+00]    [4.57e-04, 3.33e-04, 2.93e-06, 4.49e+00, 4.49e+00]    []  
                                                                                                                           32000     [5.83e-05, 3.48e-05, 7.55e-06, 3.42e-02, 3.42e-02]    [5.83e-05, 3.48e-05, 7.55e-06, 3.42e-02, 3.42e-02]    []  
14000     [5.81e-04, 1.09e-03, 2.63e-05, 1.86e+00, 1.86e+00]    [5.81e-04, 1.09e-03, 2.63e-05, 1.86e+00, 1.86e+00]    []  
14000     [3.20e-04, 3.14e-04, 3.49e-07, 2.06e-01, 2.06e-01]    [3.20e-04, 3.14e-04, 3.49e-07, 2.06e-01, 2.06e-01]    []  
30000     [1.11e-04, 4.40e-05, 5.10e-07, 3.27e-01, 3.27e-01]    [1.11e-04, 4.40e-05, 5.10e-07, 3.27e-01, 3.27e-01]    []  
13000     [4.88e-04, 7.92e-04, 1.18e-05, 3.40e-01, 3.40e-01]    [4.88e-04, 7.92e-04, 1.18e-05, 3.40e-01, 3.40e-01]    []  
                                                                                                                           33000     [7.87e-05, 3.53e-05, 1.21e-05, 4.61e-02, 4.61e-02]    [7.87e-05, 3.53e-05, 1.21e-05, 4.61e-02, 4.61e-02]    []  
12000     [2.61e-04, 8.71e-05, 2.58e-06, 3.95e+00, 3.95e+00]    [2.61e-04, 8.71e-05, 2.58e-06, 3.95e+00, 3.95e+00]    []  
15000     [6.84e-04, 1.04e-03, 1.52e-05, 1.51e+00, 1.51e+00]    [6.84e-04, 1.04e-03, 1.52e-05, 1.51e+00, 1.51e+00]    []  
15000     [3.41e-04, 1.95e-04, 2.14e-06, 2.82e+00, 2.82e+00]    [3.41e-04, 1.95e-04, 2.14e-06, 2.82e+00, 2.82e+00]    []  
31000     [1.09e-04, 4.97e-05, 1.12e-07, 9.56e-02, 9.56e-02]    [1.09e-04, 4.97e-05, 1.12e-07, 9.56e-02, 9.56e-02]    []  
34000     [1.14e-04, 3.08e-05, 6.17e-06, 3.69e-02, 3.69e-02]    [1.14e-04, 3.08e-05, 6.17e-06, 3.69e-02, 3.69e-02]    []  
14000     [6.53e-04, 3.61e-04, 1.92e-06, 6.87e-01, 6.87e-01]    [6.53e-04, 3.61e-04, 1.92e-06, 6.87e-01, 6.87e-01]    []  
                                                                                                                           32000     [1.10e-04, 5.90e-05, 1.33e-06, 3.32e-01, 3.32e-01]    [1.10e-04, 5.90e-05, 1.33e-06, 3.32e-01, 3.32e-01]    []  
16000     [6.77e-04, 9.55e-04, 2.05e-05, 1.27e+00, 1.27e+00]    [6.77e-04, 9.55e-04, 2.05e-05, 1.27e+00, 1.27e+00]    []  
16000     [3.24e-04, 1.14e-04, 1.94e-05, 1.60e-01, 1.60e-01]    [3.24e-04, 1.14e-04, 1.94e-05, 1.60e-01, 1.60e-01]    []  
                                                                                                                           35000     [1.87e-04, 2.93e-05, 2.62e-05, 2.17e+00, 2.17e+00]    [1.87e-04, 2.93e-05, 2.62e-05, 2.17e+00, 2.17e+00]    []  
13000     [9.32e-05, 3.92e-05, 3.70e-07, 3.43e+00, 3.43e+00]    [9.32e-05, 3.92e-05, 3.70e-07, 3.43e+00, 3.43e+00]    []  
15000     [2.67e-04, 1.06e-04, 5.06e-07, 3.58e-01, 3.58e-01]    [2.67e-04, 1.06e-04, 5.06e-07, 3.58e-01, 3.58e-01]    []  
15000     [4.90e-05, 8.65e-06, 2.22e-07, 3.62e-01, 3.62e-01]    [4.90e-05, 8.65e-06, 2.22e-07, 3.62e-01, 3.62e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            16000     [4.54e-05, 6.30e-06, 3.28e-07, 1.11e-01, 1.11e-01]    [4.54e-05, 6.30e-06, 3.28e-07, 1.11e-01, 1.11e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                 17000     [5.15e-05, 1.56e-05, 6.95e-06, 1.01e+00, 1.01e+00]    [5.15e-05, 1.56e-05, 6.95e-06, 1.01e+00, 1.01e+00]    []  
                                                                                                                                                                                                                                                      18000     [3.90e-05, 1.95e-05, 1.00e-05, 6.55e-01, 6.55e-01]    [3.90e-05, 1.95e-05, 1.00e-05, 6.55e-01, 6.55e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                 19000     [3.20e-05, 2.26e-05, 9.92e-07, 7.77e-02, 7.77e-02]    [3.20e-05, 2.26e-05, 9.92e-07, 7.77e-02, 7.77e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            20000     [2.81e-05, 2.89e-05, 3.62e-07, 6.81e-02, 6.81e-02]    [2.81e-05, 2.89e-05, 3.62e-07, 6.81e-02, 6.81e-02]    []  
                                                                                                                                                                                                                                                      21000     [2.95e-05, 3.63e-05, 1.35e-06, 1.93e-01, 1.93e-01]    [2.95e-05, 3.63e-05, 1.35e-06, 1.93e-01, 1.93e-01]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            22000     [3.73e-05, 4.62e-05, 4.43e-06, 7.69e-02, 7.69e-02]    [3.73e-05, 4.62e-05, 4.43e-06, 7.69e-02, 7.69e-02]    []  
                                                                                                                           23000     [3.76e-05, 4.48e-05, 1.43e-06, 6.14e-02, 6.14e-02]    [3.76e-05, 4.48e-05, 1.43e-06, 6.14e-02, 6.14e-02]    []  
21000     [7.16e-04, 1.61e-03, 1.44e-04, 8.63e-01, 8.63e-01]    [7.16e-04, 1.61e-03, 1.44e-04, 8.63e-01, 8.63e-01]    []  
38000     [9.12e-05, 4.62e-05, 3.65e-07, 1.14e+00, 1.14e+00]    [9.12e-05, 4.62e-05, 3.65e-07, 1.14e+00, 1.14e+00]    []  
19000     [1.06e-04, 2.36e-05, 1.16e-05, 1.25e+00, 1.25e+00]    [1.06e-04, 2.36e-05, 1.16e-05, 1.25e+00, 1.25e+00]    []  
42000     [4.56e-05, 3.25e-05, 2.29e-06, 2.23e-02, 2.23e-02]    [4.56e-05, 3.25e-05, 2.29e-06, 2.23e-02, 2.23e-02]    []  
17000     [8.57e-05, 1.40e-05, 8.86e-07, 1.62e+00, 1.62e+00]    [8.57e-05, 1.40e-05, 8.86e-07, 1.62e+00, 1.62e+00]    []  
                                                                                                                           22000     [3.85e-04, 4.74e-04, 3.29e-05, 6.99e-01, 6.99e-01]    [3.85e-04, 4.74e-04, 3.29e-05, 6.99e-01, 6.99e-01]    []  
22000     [6.16e-04, 9.54e-04, 1.14e-04, 9.30e-01, 9.30e-01]    [6.16e-04, 9.54e-04, 1.14e-04, 9.30e-01, 9.30e-01]    []  
39000     [8.45e-05, 3.80e-05, 4.92e-07, 7.14e-02, 7.14e-02]    [8.45e-05, 3.80e-05, 4.92e-07, 7.14e-02, 7.14e-02]    []  
43000     [5.18e-05, 3.20e-05, 1.72e-06, 2.10e-02, 2.10e-02]    [5.18e-05, 3.20e-05, 1.72e-06, 2.10e-02, 2.10e-02]    []  
20000     [6.83e-05, 1.09e-05, 4.98e-06, 2.30e-01, 2.30e-01]    [6.83e-05, 1.09e-05, 4.98e-06, 2.30e-01, 2.30e-01]    []  
                                                                                                                           18000     [1.90e-04, 4.34e-05, 7.66e-07, 1.45e+00, 1.45e+00]    [1.90e-04, 4.34e-05, 7.66e-07, 1.45e+00, 1.45e+00]    []  
44000     [5.35e-05, 3.05e-05, 2.34e-06, 1.99e-02, 1.99e-02]    [5.35e-05, 3.05e-05, 2.34e-06, 1.99e-02, 1.99e-02]    []  
40000     [7.54e-05, 2.56e-05, 1.18e-06, 1.47e-01, 1.47e-01]    [7.54e-05, 2.56e-05, 1.18e-06, 1.47e-01, 1.47e-01]    []  
23000     [2.64e-04, 2.64e-04, 4.96e-05, 5.84e-01, 5.84e-01]    [2.64e-04, 2.64e-04, 4.96e-05, 5.84e-01, 5.84e-01]    []  
23000     [4.28e-04, 7.33e-04, 3.16e-05, 3.45e-01, 3.45e-01]    [4.28e-04, 7.33e-04, 3.16e-05, 3.45e-01, 3.45e-01]    []  
                                                                                                                           45000     [8.01e-05, 3.94e-05, 1.81e-05, 1.62e-01, 1.62e-01]    [8.01e-05, 3.94e-05, 1.81e-05, 1.62e-01, 1.62e-01]    []  
21000     [5.68e-05, 1.11e-05, 3.27e-06, 2.75e-01, 2.75e-01]    [5.68e-05, 1.11e-05, 3.27e-06, 2.75e-01, 2.75e-01]    []  
41000     [6.61e-05, 1.53e-05, 7.36e-07, 8.49e-02, 8.49e-02]    [6.61e-05, 1.53e-05, 7.36e-07, 8.49e-02, 8.49e-02]    []  
24000     [2.43e-04, 1.80e-04, 4.31e-05, 5.17e-01, 5.17e-01]    [2.43e-04, 1.80e-04, 4.31e-05, 5.17e-01, 5.17e-01]    []  
19000     [2.96e-04, 9.72e-05, 5.67e-07, 1.28e+00, 1.28e+00]    [2.96e-04, 9.72e-05, 5.67e-07, 1.28e+00, 1.28e+00]    []  
24000     [3.98e-04, 5.20e-04, 4.21e-05, 4.57e-01, 4.57e-01]    [3.98e-04, 5.20e-04, 4.21e-05, 4.57e-01, 4.57e-01]    []  
                                                                                                                           46000     [9.34e-05, 1.20e-04, 9.26e-06, 2.91e-02, 2.91e-02]    [9.34e-05, 1.20e-04, 9.26e-06, 2.91e-02, 2.91e-02]    []  
42000     [7.12e-05, 2.01e-05, 3.16e-07, 4.12e-02, 4.12e-02]    [7.12e-05, 2.01e-05, 3.16e-07, 4.12e-02, 4.12e-02]    []  
                                                                                                                           22000     [4.78e-05, 9.62e-06, 6.04e-07, 1.22e-01, 1.22e-01]    [4.78e-05, 9.62e-06, 6.04e-07, 1.22e-01, 1.22e-01]    []  
25000     [2.03e-04, 1.46e-04, 2.34e-05, 4.39e-01, 4.39e-01]    [2.03e-04, 1.46e-04, 2.34e-05, 4.39e-01, 4.39e-01]    []  
25000     [3.34e-04, 3.06e-04, 4.16e-05, 3.15e-01, 3.15e-01]    [3.34e-04, 3.06e-04, 4.16e-05, 3.15e-01, 3.15e-01]    []  
47000     [5.08e-05, 6.05e-05, 4.75e-06, 1.63e-02, 1.63e-02]    [5.08e-05, 6.05e-05, 4.75e-06, 1.63e-02, 1.63e-02]    []  
                                                                                                                           20000     [3.16e-04, 1.15e-04, 2.44e-06, 1.27e+00, 1.27e+00]    [3.16e-04, 1.15e-04, 2.44e-06, 1.27e+00, 1.27e+00]    []  
43000     [5.66e-05, 1.87e-05, 1.71e-07, 1.67e-01, 1.67e-01]    [5.66e-05, 1.87e-05, 1.71e-07, 1.67e-01, 1.67e-01]    []  
32000     [1.06e-05, 8.86e-07, 1.83e-08, 2.77e-02, 2.77e-02]    [1.06e-05, 8.86e-07, 1.83e-08, 2.77e-02, 2.77e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            33000     [6.93e-06, 4.04e-07, 4.22e-07, 7.07e-02, 7.07e-02]    [6.93e-06, 4.04e-07, 4.22e-07, 7.07e-02, 7.07e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                 34000     [2.40e-05, 2.99e-06, 3.12e-07, 4.97e-02, 4.97e-02]    [2.40e-05, 2.99e-06, 3.12e-07, 4.97e-02, 4.97e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      35000     [8.46e-06, 6.62e-07, 1.96e-07, 2.49e-02, 2.49e-02]    [8.46e-06, 6.62e-07, 1.96e-07, 2.49e-02, 2.49e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          36000     [1.57e-05, 1.22e-06, 6.69e-07, 2.39e-02, 2.39e-02]    [1.57e-05, 1.22e-06, 6.69e-07, 2.39e-02, 2.39e-02]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       37000     [1.00e-05, 6.23e-07, 1.90e-07, 3.30e-02, 3.30e-02]    [1.00e-05, 6.23e-07, 1.90e-07, 3.30e-02, 3.30e-02]    []  
                                                                                                                           38000     [1.23e-05, 3.15e-07, 1.49e-07, 1.28e-01, 1.28e-01]    [1.23e-05, 3.15e-07, 1.49e-07, 1.28e-01, 1.28e-01]    []  
29000     [9.32e-05, 7.89e-05, 1.65e-05, 3.93e-01, 3.93e-01]    [9.32e-05, 7.89e-05, 1.65e-05, 3.93e-01, 3.93e-01]    []  
29000     [2.38e-04, 7.91e-05, 2.89e-05, 1.12e+00, 1.12e+00]    [2.38e-04, 7.91e-05, 2.89e-05, 1.12e+00, 1.12e+00]    []  
23000     [2.69e-04, 1.17e-04, 2.42e-06, 7.57e-01, 7.57e-01]    [2.69e-04, 1.17e-04, 2.42e-06, 7.57e-01, 7.57e-01]    []  
47000     [4.44e-05, 1.40e-05, 4.73e-07, 5.45e-02, 5.45e-02]    [4.44e-05, 1.40e-05, 4.73e-07, 5.45e-02, 5.45e-02]    []  
26000     [5.32e-05, 1.08e-05, 1.19e-06, 6.24e-02, 6.24e-02]    [5.32e-05, 1.08e-05, 1.19e-06, 6.24e-02, 6.24e-02]    []  
                                                                                                                           3000      [1.01e-03, 6.90e-04, 2.66e-06, 7.96e+00, 7.96e+00]    [1.01e-03, 6.90e-04, 2.66e-06, 7.96e+00, 7.96e+00]    []  
                                                                                                                           30000     [9.11e-05, 7.21e-05, 1.34e-05, 3.28e-01, 3.28e-01]    [9.11e-05, 7.21e-05, 1.34e-05, 3.28e-01, 3.28e-01]    []  
30000     [1.62e-04, 5.65e-05, 1.26e-05, 4.32e-01, 4.32e-01]    [1.62e-04, 5.65e-05, 1.26e-05, 4.32e-01, 4.32e-01]    []  
4000      [7.85e-04, 8.52e-04, 1.03e-05, 5.99e+00, 5.99e+00]    [7.85e-04, 8.52e-04, 1.03e-05, 5.99e+00, 5.99e+00]    []  
48000     [5.34e-05, 1.35e-05, 1.97e-07, 3.43e-02, 3.43e-02]    [5.34e-05, 1.35e-05, 1.97e-07, 3.43e-02, 3.43e-02]    []  
24000     [2.03e-04, 8.94e-05, 2.52e-06, 7.67e-01, 7.67e-01]    [2.03e-04, 8.94e-05, 2.52e-06, 7.67e-01, 7.67e-01]    []  
27000     [5.17e-05, 9.08e-06, 4.36e-06, 4.39e-01, 4.39e-01]    [5.17e-05, 9.08e-06, 4.36e-06, 4.39e-01, 4.39e-01]    []  
                                                                                                                           5000      [6.64e-04, 8.17e-04, 7.27e-07, 4.06e+00, 4.06e+00]    [6.64e-04, 8.17e-04, 7.27e-07, 4.06e+00, 4.06e+00]    []  
31000     [9.12e-05, 6.89e-05, 1.90e-05, 3.99e-01, 3.99e-01]    [9.12e-05, 6.89e-05, 1.90e-05, 3.99e-01, 3.99e-01]    []  
31000     [8.71e-05, 6.89e-05, 3.40e-06, 4.99e-02, 4.99e-02]    [8.71e-05, 6.89e-05, 3.40e-06, 4.99e-02, 4.99e-02]    []  
49000     [3.90e-05, 8.98e-06, 2.29e-07, 6.65e-02, 6.65e-02]    [3.90e-05, 8.98e-06, 2.29e-07, 6.65e-02, 6.65e-02]    []  
                                                                                                                           28000     [2.94e-05, 4.98e-06, 4.16e-07, 3.83e-02, 3.83e-02]    [2.94e-05, 4.98e-06, 4.16e-07, 3.83e-02, 3.83e-02]    []  
25000     [1.88e-04, 9.17e-05, 1.85e-06, 6.96e-01, 6.96e-01]    [1.88e-04, 9.17e-05, 1.85e-06, 6.96e-01, 6.96e-01]    []  
6000      [7.76e-04, 8.15e-04, 4.26e-06, 3.85e+00, 3.85e+00]    [7.76e-04, 8.15e-04, 4.26e-06, 3.85e+00, 3.85e+00]    []  
32000     [7.02e-05, 6.27e-05, 2.57e-05, 3.01e-01, 3.01e-01]    [7.02e-05, 6.27e-05, 2.57e-05, 3.01e-01, 3.01e-01]    []  
50000     [5.25e-05, 1.41e-05, 8.93e-08, 3.07e-02, 3.07e-02]    [5.25e-05, 1.41e-05, 8.93e-08, 3.07e-02, 3.07e-02]    []  

Best model at step 50000:
  train loss: 6.14e-02
  test loss: 6.14e-02
  test metric: []

'train' took 21887.047263 s

[I 2023-10-09 18:40:14,845] Trial 52 finished with value: 0.537370598207348 and parameters: {'num_domain': 87222, 'num_boundary': 5073, 'resampling_period': 12547, 'lr': 0.0010693025920854476}. Best is trial 0 with value: 0.4913720930752669.
32000     [6.82e-05, 4.65e-05, 1.55e-06, 4.49e-02, 4.49e-02]    [6.82e-05, 4.65e-05, 1.55e-06, 4.49e-02, 4.49e-02]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000204 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [5.21e+01, 6.65e-05, 3.51e-07, 3.60e+03, 3.60e+03]    [5.21e+01, 6.65e-05, 3.51e-07, 3.60e+03, 3.60e+03]    []  
7000      [1.10e-03, 9.44e-04, 6.26e-06, 5.94e+00, 5.94e+00]    [1.10e-03, 9.44e-04, 6.26e-06, 5.94e+00, 5.94e+00]    []  
                                                                                                                           29000     [5.21e-05, 5.78e-06, 2.26e-06, 8.40e-02, 8.40e-02]    [5.21e-05, 5.78e-06, 2.26e-06, 8.40e-02, 8.40e-02]    []  
26000     [1.93e-04, 9.76e-05, 2.08e-06, 5.43e-01, 5.43e-01]    [1.93e-04, 9.76e-05, 2.08e-06, 5.43e-01, 5.43e-01]    []  
33000     [7.95e-05, 6.21e-05, 1.29e-05, 3.65e-01, 3.65e-01]    [7.95e-05, 6.21e-05, 1.29e-05, 3.65e-01, 3.65e-01]    []  
8000      [8.91e-04, 9.87e-04, 3.72e-06, 1.18e+00, 1.18e+00]    [8.91e-04, 9.87e-04, 3.72e-06, 1.18e+00, 1.18e+00]    []  
1000      [3.51e-07, 1.07e-03, 6.03e-16, 2.10e+03, 2.10e+03]    [3.51e-07, 1.07e-03, 6.03e-16, 2.10e+03, 2.10e+03]    []  
33000     [7.36e-05, 3.51e-05, 5.44e-06, 5.36e-02, 5.36e-02]    [7.36e-05, 3.51e-05, 5.44e-06, 5.36e-02, 5.36e-02]    []  
                                                                                                                           9000      [7.90e-04, 9.21e-04, 5.77e-06, 1.11e+00, 1.11e+00]    [7.90e-04, 9.21e-04, 5.77e-06, 1.11e+00, 1.11e+00]    []  
30000     [1.52e-05, 2.88e-06, 1.29e-06, 3.41e-02, 3.41e-02]    [1.52e-05, 2.88e-06, 1.29e-06, 3.41e-02, 3.41e-02]    []  
27000     [2.08e-04, 1.16e-04, 1.95e-06, 7.00e-01, 7.00e-01]    [2.08e-04, 1.16e-04, 1.95e-06, 7.00e-01, 7.00e-01]    []  
34000     [6.89e-05, 5.80e-05, 1.38e-05, 2.75e-01, 2.75e-01]    [6.89e-05, 5.80e-05, 1.38e-05, 2.75e-01, 2.75e-01]    []  
2000      [1.51e-10, 1.05e-03, 1.19e-16, 2.09e+03, 2.09e+03]    [1.51e-10, 1.05e-03, 1.19e-16, 2.09e+03, 2.09e+03]    []  
34000     [6.03e-04, 5.33e-04, 5.01e-05, 7.60e-01, 7.60e-01]    [6.03e-04, 5.33e-04, 5.01e-05, 7.60e-01, 7.60e-01]    []  
                                                                                                                           10000     [7.12e-04, 6.73e-04, 2.30e-06, 3.52e+00, 3.52e+00]    [7.12e-04, 6.73e-04, 2.30e-06, 3.52e+00, 3.52e+00]    []  
                                                                                                                           31000     [3.56e-05, 5.37e-06, 2.49e-06, 7.48e-02, 7.48e-02]    [3.56e-05, 5.37e-06, 2.49e-06, 7.48e-02, 7.48e-02]    []  
35000     [6.50e-05, 5.54e-05, 1.22e-05, 2.19e-01, 2.19e-01]    [6.50e-05, 5.54e-05, 1.22e-05, 2.19e-01, 2.19e-01]    []  
3000      [8.78e-11, 1.05e-03, 3.64e-17, 2.09e+03, 2.09e+03]    [8.78e-11, 1.05e-03, 3.64e-17, 2.09e+03, 2.09e+03]    []  
35000     [5.24e-04, 2.62e-04, 4.79e-05, 5.98e-01, 5.98e-01]    [5.24e-04, 2.62e-04, 4.79e-05, 5.98e-01, 5.98e-01]    []  
11000     [5.07e-04, 4.64e-04, 3.10e-06, 1.11e+00, 1.11e+00]    [5.07e-04, 4.64e-04, 3.10e-06, 1.11e+00, 1.11e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              28000     [2.00e-04, 1.14e-04, 3.27e-06, 5.09e-01, 5.09e-01]    [2.00e-04, 1.14e-04, 3.27e-06, 5.09e-01, 5.09e-01]    []  
': 25913, 'lr': 0.0008640224264789008} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 100.00 MiB (GPU 1; 10.75 GiB total capacity; 9.65 GiB already allocated; 95.62 MiB free; 9.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 1; 10.75 GiB total capacity; 9.65 GiB already allocated; 95.62 MiB free; 9.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-09 19:05:25,536] Trial 61 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 80, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 66, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 338, in closure
    total_loss.backward()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 1; 10.75 GiB total capacity; 9.65 GiB already allocated; 95.62 MiB free; 9.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
12000     [4.06e-04, 3.36e-04, 4.56e-06, 6.07e-01, 6.07e-01]    [4.06e-04, 3.36e-04, 4.56e-06, 6.07e-01, 6.07e-01]    []  
32000     [2.30e-05, 3.80e-06, 7.20e-08, 3.05e-02, 3.05e-02]    [2.30e-05, 3.80e-06, 7.20e-08, 3.05e-02, 3.05e-02]    []  
4000      [7.21e-11, 1.05e-03, 4.32e-17, 2.09e+03, 2.09e+03]    [7.21e-11, 1.05e-03, 4.32e-17, 2.09e+03, 2.09e+03]    []  
36000     [6.35e-05, 5.27e-05, 1.12e-05, 2.16e-01, 2.16e-01]    [6.35e-05, 5.27e-05, 1.12e-05, 2.16e-01, 2.16e-01]    []  
36000     [3.41e-04, 2.39e-04, 6.62e-05, 1.34e-01, 1.34e-01]    [3.41e-04, 2.39e-04, 6.62e-05, 1.34e-01, 1.34e-01]    []  
29000     [1.93e-04, 1.15e-04, 2.39e-06, 8.40e-01, 8.40e-01]    [1.93e-04, 1.15e-04, 2.39e-06, 8.40e-01, 8.40e-01]    []  
13000     [3.32e-04, 2.21e-04, 3.73e-06, 8.16e-01, 8.16e-01]    [3.32e-04, 2.21e-04, 3.73e-06, 8.16e-01, 8.16e-01]    []  
5000      [8.45e-11, 1.05e-03, 4.16e-16, 2.09e+03, 2.09e+03]    [8.45e-11, 1.05e-03, 4.16e-16, 2.09e+03, 2.09e+03]    []  
37000     [5.53e-05, 4.88e-05, 9.93e-06, 2.19e-01, 2.19e-01]    [5.53e-05, 4.88e-05, 9.93e-06, 2.19e-01, 2.19e-01]    []  
33000     [1.75e-05, 2.08e-06, 2.06e-07, 2.70e-02, 2.70e-02]    [1.75e-05, 2.08e-06, 2.06e-07, 2.70e-02, 2.70e-02]    []  
37000     [2.34e-04, 1.75e-04, 5.55e-05, 1.94e-01, 1.94e-01]    [2.34e-04, 1.75e-04, 5.55e-05, 1.94e-01, 1.94e-01]    []  
14000     [2.53e-04, 1.60e-04, 5.53e-06, 5.55e-01, 5.55e-01]    [2.53e-04, 1.60e-04, 5.53e-06, 5.55e-01, 5.55e-01]    []  
30000     [1.89e-04, 1.02e-04, 4.45e-06, 8.09e-01, 8.09e-01]    [1.89e-04, 1.02e-04, 4.45e-06, 8.09e-01, 8.09e-01]    []  
6000      [1.15e-02, 7.49e-04, 8.87e-05, 6.07e+01, 6.07e+01]    [1.15e-02, 7.49e-04, 8.87e-05, 6.07e+01, 6.07e+01]    []  
15000     [2.32e-04, 1.17e-04, 5.20e-06, 1.25e+00, 1.25e+00]    [2.32e-04, 1.17e-04, 5.20e-06, 1.25e+00, 1.25e+00]    []  
38000     [4.96e-05, 4.67e-05, 8.42e-06, 2.03e-01, 2.03e-01]    [4.96e-05, 4.67e-05, 8.42e-06, 2.03e-01, 2.03e-01]    []  
38000     [1.92e-04, 1.29e-04, 3.12e-05, 7.91e-02, 7.91e-02]    [1.92e-04, 1.29e-04, 3.12e-05, 7.91e-02, 7.91e-02]    []  
34000     [1.15e-05, 1.05e-06, 8.26e-08, 2.51e-02, 2.51e-02]    [1.15e-05, 1.05e-06, 8.26e-08, 2.51e-02, 2.51e-02]    []  
16000     [2.29e-04, 1.20e-04, 6.53e-06, 9.90e-01, 9.90e-01]    [2.29e-04, 1.20e-04, 6.53e-06, 9.90e-01, 9.90e-01]    []  
7000      [2.62e-03, 2.24e-04, 7.47e-06, 1.20e+01, 1.20e+01]    [2.62e-03, 2.24e-04, 7.47e-06, 1.20e+01, 1.20e+01]    []  
31000     [1.82e-04, 9.59e-05, 4.84e-06, 3.79e-01, 3.79e-01]    [1.82e-04, 9.59e-05, 4.84e-06, 3.79e-01, 3.79e-01]    []  
39000     [5.23e-05, 4.38e-05, 9.89e-06, 2.71e-01, 2.71e-01]    [5.23e-05, 4.38e-05, 9.89e-06, 2.71e-01, 2.71e-01]    []  
39000     [3.10e-04, 4.57e-04, 3.13e-05, 1.20e-01, 1.20e-01]    [3.10e-04, 4.57e-04, 3.13e-05, 1.20e-01, 1.20e-01]    []  
35000     [5.08e-05, 9.02e-06, 1.34e-05, 2.92e+00, 2.92e+00]    [5.08e-05, 9.02e-06, 1.34e-05, 2.92e+00, 2.92e+00]    []  
17000     [2.35e-04, 1.29e-04, 4.94e-06, 6.78e-01, 6.78e-01]    [2.35e-04, 1.29e-04, 4.94e-06, 6.78e-01, 6.78e-01]    []  
8000      [1.57e-03, 3.05e-04, 1.49e-04, 9.72e+00, 9.72e+00]    [1.57e-03, 3.05e-04, 1.49e-04, 9.72e+00, 9.72e+00]    []  
40000     [4.28e-05, 4.25e-05, 7.13e-06, 1.67e-01, 1.67e-01]    [4.28e-05, 4.25e-05, 7.13e-06, 1.67e-01, 1.67e-01]    []  
40000     [3.92e-04, 1.12e-04, 1.36e-04, 1.34e-01, 1.34e-01]    [3.92e-04, 1.12e-04, 1.36e-04, 1.34e-01, 1.34e-01]    []  
18000     [2.34e-04, 1.48e-04, 7.82e-06, 5.33e-01, 5.33e-01]    [2.34e-04, 1.48e-04, 7.82e-06, 5.33e-01, 5.33e-01]    []  
32000     [1.81e-04, 9.18e-05, 1.22e-06, 4.65e-01, 4.65e-01]    [1.81e-04, 9.18e-05, 1.22e-06, 4.65e-01, 4.65e-01]    []  
36000     [1.36e-05, 2.04e-06, 1.62e-07, 2.31e-02, 2.31e-02]    [1.36e-05, 2.04e-06, 1.62e-07, 2.31e-02, 2.31e-02]    []  
9000      [8.10e-04, 4.47e-04, 9.73e-05, 7.59e+00, 7.59e+00]    [8.10e-04, 4.47e-04, 9.73e-05, 7.59e+00, 7.59e+00]    []  
19000     [2.13e-04, 1.43e-04, 3.09e-06, 2.16e-01, 2.16e-01]    [2.13e-04, 1.43e-04, 3.09e-06, 2.16e-01, 2.16e-01]    []  
41000     [4.58e-05, 4.15e-05, 2.67e-05, 2.17e-01, 2.17e-01]    [4.58e-05, 4.15e-05, 2.67e-05, 2.17e-01, 2.17e-01]    []  
41000     [2.50e-04, 9.08e-05, 1.74e-05, 5.88e-02, 5.88e-02]    [2.50e-04, 9.08e-05, 1.74e-05, 5.88e-02, 5.88e-02]    []  
33000     [1.83e-04, 8.44e-05, 1.53e-06, 3.61e-01, 3.61e-01]    [1.83e-04, 8.44e-05, 1.53e-06, 3.61e-01, 3.61e-01]    []  
37000     [1.56e-05, 1.20e-06, 2.97e-07, 2.23e-02, 2.23e-02]    [1.56e-05, 1.20e-06, 2.97e-07, 2.23e-02, 2.23e-02]    []  
20000     [2.28e-04, 1.40e-04, 5.75e-06, 4.72e-01, 4.72e-01]    [2.28e-04, 1.40e-04, 5.75e-06, 4.72e-01, 4.72e-01]    []  
10000     [1.05e-03, 5.94e-04, 6.12e-05, 6.70e+00, 6.70e+00]    [1.05e-03, 5.94e-04, 6.12e-05, 6.70e+00, 6.70e+00]    []  
42000     [4.46e-05, 3.85e-05, 2.77e-05, 2.78e-01, 2.78e-01]    [4.46e-05, 3.85e-05, 2.77e-05, 2.78e-01, 2.78e-01]    []  
42000     [1.66e-04, 5.78e-05, 4.72e-05, 4.90e-02, 4.90e-02]    [1.66e-04, 5.78e-05, 4.72e-05, 4.90e-02, 4.90e-02]    []  
21000     [1.97e-04, 1.28e-04, 7.11e-06, 4.67e-01, 4.67e-01]    [1.97e-04, 1.28e-04, 7.11e-06, 4.67e-01, 4.67e-01]    []  
11000     [7.04e-04, 5.99e-04, 3.31e-06, 5.71e+00, 5.71e+00]    [7.04e-04, 5.99e-04, 3.31e-06, 5.71e+00, 5.71e+00]    []  
38000     [1.06e-04, 1.52e-05, 6.73e-07, 1.68e+00, 1.68e+00]    [1.06e-04, 1.52e-05, 6.73e-07, 1.68e+00, 1.68e+00]    []  
34000     [1.90e-04, 7.88e-05, 1.89e-06, 4.96e-01, 4.96e-01]    [1.90e-04, 7.88e-05, 1.89e-06, 4.96e-01, 4.96e-01]    []  
22000     [1.93e-04, 1.14e-04, 5.00e-06, 1.87e-01, 1.87e-01]    [1.93e-04, 1.14e-04, 5.00e-06, 1.87e-01, 1.87e-01]    []  
43000     [4.25e-05, 3.65e-05, 3.86e-05, 4.13e-01, 4.13e-01]    [4.25e-05, 3.65e-05, 3.86e-05, 4.13e-01, 4.13e-01]    []  
43000     [1.89e-04, 4.80e-05, 3.58e-05, 4.91e-01, 4.91e-01]    [1.89e-04, 4.80e-05, 3.58e-05, 4.91e-01, 4.91e-01]    []  
12000     [4.58e-04, 5.04e-04, 2.09e-06, 5.03e+00, 5.03e+00]    [4.58e-04, 5.04e-04, 2.09e-06, 5.03e+00, 5.03e+00]    []  
23000     [1.91e-04, 1.02e-04, 4.68e-06, 2.45e-01, 2.45e-01]    [1.91e-04, 1.02e-04, 4.68e-06, 2.45e-01, 2.45e-01]    []  
39000     [7.01e-06, 7.53e-07, 2.75e-08, 2.24e-02, 2.24e-02]    [7.01e-06, 7.53e-07, 2.75e-08, 2.24e-02, 2.24e-02]    []  
35000     [1.84e-04, 8.12e-05, 1.21e-06, 3.63e-01, 3.63e-01]    [1.84e-04, 8.12e-05, 1.21e-06, 3.63e-01, 3.63e-01]    []  
44000     [3.43e-05, 3.67e-05, 1.62e-05, 1.42e-01, 1.42e-01]    [3.43e-05, 3.67e-05, 1.62e-05, 1.42e-01, 1.42e-01]    []  
44000     [1.24e-04, 7.05e-05, 7.83e-05, 1.41e-01, 1.41e-01]    [1.24e-04, 7.05e-05, 7.83e-05, 1.41e-01, 1.41e-01]    []  
24000     [1.73e-04, 7.68e-05, 1.10e-05, 3.92e-01, 3.92e-01]    [1.73e-04, 7.68e-05, 1.10e-05, 3.92e-01, 3.92e-01]    []  
13000     [4.14e-04, 3.13e-04, 1.16e-06, 3.81e+00, 3.81e+00]    [4.14e-04, 3.13e-04, 1.16e-06, 3.81e+00, 3.81e+00]    []  
40000     [1.83e-05, 2.57e-06, 2.08e-06, 1.48e+00, 1.48e+00]    [1.83e-05, 2.57e-06, 2.08e-06, 1.48e+00, 1.48e+00]    []  
45000     [3.42e-05, 3.53e-05, 1.37e-05, 1.33e-01, 1.33e-01]    [3.42e-05, 3.53e-05, 1.37e-05, 1.33e-01, 1.33e-01]    []  
25000     [1.56e-04, 6.37e-05, 2.39e-06, 1.13e-01, 1.13e-01]    [1.56e-04, 6.37e-05, 2.39e-06, 1.13e-01, 1.13e-01]    []  
45000     [1.71e-04, 9.91e-05, 5.48e-05, 3.35e-02, 3.35e-02]    [1.71e-04, 9.91e-05, 5.48e-05, 3.35e-02, 3.35e-02]    []  
36000     [1.89e-04, 8.10e-05, 1.83e-06, 3.20e-01, 3.20e-01]    [1.89e-04, 8.10e-05, 1.83e-06, 3.20e-01, 3.20e-01]    []  
14000     [4.84e-04, 2.66e-04, 4.44e-06, 3.46e+00, 3.46e+00]    [4.84e-04, 2.66e-04, 4.44e-06, 3.46e+00, 3.46e+00]    []  
26000     [1.77e-04, 4.76e-05, 7.19e-06, 3.26e+00, 3.26e+00]    [1.77e-04, 4.76e-05, 7.19e-06, 3.26e+00, 3.26e+00]    []  
41000     [1.33e-05, 2.20e-06, 3.13e-06, 6.22e-02, 6.22e-02]    [1.33e-05, 2.20e-06, 3.13e-06, 6.22e-02, 6.22e-02]    []  
46000     [3.47e-05, 3.44e-05, 1.44e-05, 1.62e-01, 1.62e-01]    [3.47e-05, 3.44e-05, 1.44e-05, 1.62e-01, 1.62e-01]    []  
46000     [1.81e-04, 7.84e-05, 6.53e-05, 3.65e-02, 3.65e-02]    [1.81e-04, 7.84e-05, 6.53e-05, 3.65e-02, 3.65e-02]    []  
15000     [7.66e-04, 5.67e-04, 3.75e-06, 2.91e+00, 2.91e+00]    [7.66e-04, 5.67e-04, 3.75e-06, 2.91e+00, 2.91e+00]    []  
37000     [1.87e-04, 7.84e-05, 2.02e-06, 3.29e-01, 3.29e-01]    [1.87e-04, 7.84e-05, 2.02e-06, 3.29e-01, 3.29e-01]    []  
27000     [1.43e-04, 3.64e-05, 2.69e-06, 1.31e-01, 1.31e-01]    [1.43e-04, 3.64e-05, 2.69e-06, 1.31e-01, 1.31e-01]    []  
47000     [4.14e-05, 3.27e-05, 1.65e-05, 1.26e-01, 1.26e-01]    [4.14e-05, 3.27e-05, 1.65e-05, 1.26e-01, 1.26e-01]    []  
42000     [2.23e-05, 8.85e-07, 1.68e-06, 8.65e-01, 8.65e-01]    [2.23e-05, 8.85e-07, 1.68e-06, 8.65e-01, 8.65e-01]    []  
47000     [5.42e-04, 1.12e-04, 1.48e-04, 6.75e-02, 6.75e-02]    [5.42e-04, 1.12e-04, 1.48e-04, 6.75e-02, 6.75e-02]    []  
28000     [1.46e-04, 3.89e-05, 3.72e-06, 1.86e-01, 1.86e-01]    [1.46e-04, 3.89e-05, 3.72e-06, 1.86e-01, 1.86e-01]    []  
16000     [7.45e-04, 7.72e-04, 3.68e-06, 2.24e+00, 2.24e+00]    [7.45e-04, 7.72e-04, 3.68e-06, 2.24e+00, 2.24e+00]    []  
38000     [1.88e-04, 7.40e-05, 1.59e-06, 2.95e-01, 2.95e-01]    [1.88e-04, 7.40e-05, 1.59e-06, 2.95e-01, 2.95e-01]    []  
29000     [1.39e-04, 3.47e-05, 1.79e-06, 1.10e-01, 1.10e-01]    [1.39e-04, 3.47e-05, 1.79e-06, 1.10e-01, 1.10e-01]    []  
48000     [3.18e-05, 3.17e-05, 1.33e-05, 1.19e-01, 1.19e-01]    [3.18e-05, 3.17e-05, 1.33e-05, 1.19e-01, 1.19e-01]    []  
48000     [1.20e-04, 1.59e-04, 1.34e-06, 2.50e-02, 2.50e-02]    [1.20e-04, 1.59e-04, 1.34e-06, 2.50e-02, 2.50e-02]    []  
43000     [6.42e-06, 6.39e-07, 3.80e-08, 1.67e-02, 1.67e-02]    [6.42e-06, 6.39e-07, 3.80e-08, 1.67e-02, 1.67e-02]    []  
17000     [6.96e-04, 7.11e-04, 8.62e-07, 1.77e+00, 1.77e+00]    [6.96e-04, 7.11e-04, 8.62e-07, 1.77e+00, 1.77e+00]    []  
30000     [1.12e-04, 3.39e-05, 1.98e-06, 1.70e-01, 1.70e-01]    [1.12e-04, 3.39e-05, 1.98e-06, 1.70e-01, 1.70e-01]    []  
39000     [1.85e-04, 7.08e-05, 1.23e-06, 2.69e-01, 2.69e-01]    [1.85e-04, 7.08e-05, 1.23e-06, 2.69e-01, 2.69e-01]    []  
49000     [5.84e-05, 3.13e-05, 4.00e-05, 1.01e+00, 1.01e+00]    [5.84e-05, 3.13e-05, 4.00e-05, 1.01e+00, 1.01e+00]    []  
18000     [6.25e-04, 5.52e-04, 1.18e-06, 1.51e+00, 1.51e+00]    [6.25e-04, 5.52e-04, 1.18e-06, 1.51e+00, 1.51e+00]    []  
49000     [1.77e-04, 1.41e-04, 4.05e-05, 2.58e-02, 2.58e-02]    [1.77e-04, 1.41e-04, 4.05e-05, 2.58e-02, 2.58e-02]    []  
44000     [1.07e-05, 1.27e-06, 7.66e-08, 1.64e-02, 1.64e-02]    [1.07e-05, 1.27e-06, 7.66e-08, 1.64e-02, 1.64e-02]    []  
31000     [9.57e-05, 3.03e-05, 2.17e-06, 7.63e-02, 7.63e-02]    [9.57e-05, 3.03e-05, 2.17e-06, 7.63e-02, 7.63e-02]    []  
19000     [5.57e-04, 4.77e-04, 1.09e-06, 1.37e+00, 1.37e+00]    [5.57e-04, 4.77e-04, 1.09e-06, 1.37e+00, 1.37e+00]    []  
32000     [9.54e-05, 3.20e-05, 3.30e-06, 9.61e-02, 9.61e-02]    [9.54e-05, 3.20e-05, 3.30e-06, 9.61e-02, 9.61e-02]    []  
50000     [3.70e-05, 2.89e-05, 1.15e-05, 1.24e-01, 1.24e-01]    [3.70e-05, 2.89e-05, 1.15e-05, 1.24e-01, 1.24e-01]    []  

Best model at step 48000:
  train loss: 2.39e-01
  test loss: 2.39e-01
  test metric: []

'train' took 23848.705370 s

40000     [1.84e-04, 6.97e-05, 1.06e-06, 2.68e-01, 2.68e-01]    [1.84e-04, 6.97e-05, 1.06e-06, 2.68e-01, 2.68e-01]    []  
[I 2023-10-09 21:04:17,000] Trial 54 finished with value: 0.5968434503957817 and parameters: {'num_domain': 84396, 'num_boundary': 5123, 'resampling_period': 11975, 'lr': 0.0003134654114074677}. Best is trial 0 with value: 0.4913720930752669.
50000     [1.52e-04, 1.07e-04, 9.19e-06, 8.07e-02, 8.07e-02]    [1.52e-04, 1.07e-04, 9.19e-06, 8.07e-02, 8.07e-02]    []  

Best model at step 48000:
  train loss: 5.03e-02
  test loss: 5.03e-02
  test metric: []

'train' took 23876.988632 s

[I 2023-10-09 21:04:45,700] Trial 55 finished with value: 0.6553924768134592 and parameters: {'num_domain': 86518, 'num_boundary': 5189, 'resampling_period': 3980, 'lr': 0.002341490232785622}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000267 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.97e+00, 2.56e-04, 2.97e-07, 3.61e+03, 3.61e+03]    [3.97e+00, 2.56e-04, 2.97e-07, 3.61e+03, 3.61e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000180 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.14e+01, 4.03e-05, 1.55e-07, 3.61e+03, 3.61e+03]    [4.14e+01, 4.03e-05, 1.55e-07, 3.61e+03, 3.61e+03]    []  
45000     [1.66e-05, 3.36e-06, 4.96e-06, 3.57e-01, 3.57e-01]    [1.66e-05, 3.36e-06, 4.96e-06, 3.57e-01, 3.57e-01]    []  
33000     [8.64e-05, 3.24e-05, 5.38e-06, 6.44e-02, 6.44e-02]    [8.64e-05, 3.24e-05, 5.38e-06, 6.44e-02, 6.44e-02]    []  
20000     [6.06e-04, 4.91e-04, 1.99e-06, 1.23e+00, 1.23e+00]    [6.06e-04, 4.91e-04, 1.99e-06, 1.23e+00, 1.23e+00]    []  
1000      [4.15e-10, 1.05e-03, 2.04e-15, 2.09e+03, 2.09e+03]    [4.15e-10, 1.05e-03, 2.04e-15, 2.09e+03, 2.09e+03]    []  
1000      [8.55e-02, 1.05e-03, 3.57e-09, 2.09e+03, 2.09e+03]    [8.55e-02, 1.05e-03, 3.57e-09, 2.09e+03, 2.09e+03]    []  
41000     [1.79e-04, 6.90e-05, 7.18e-07, 2.33e-01, 2.33e-01]    [1.79e-04, 6.90e-05, 7.18e-07, 2.33e-01, 2.33e-01]    []  
46000     [6.19e-06, 3.07e-07, 7.89e-08, 2.48e-01, 2.48e-01]    [6.19e-06, 3.07e-07, 7.89e-08, 2.48e-01, 2.48e-01]    []  
34000     [9.07e-05, 3.42e-05, 3.65e-06, 5.88e-02, 5.88e-02]    [9.07e-05, 3.42e-05, 3.65e-06, 5.88e-02, 5.88e-02]    []  
21000     [5.98e-04, 3.92e-04, 4.23e-06, 9.22e-01, 9.22e-01]    [5.98e-04, 3.92e-04, 4.23e-06, 9.22e-01, 9.22e-01]    []  
35000     [7.86e-05, 3.85e-05, 5.44e-06, 5.61e-02, 5.61e-02]    [7.86e-05, 3.85e-05, 5.44e-06, 5.61e-02, 5.61e-02]    []  
2000      [1.68e-02, 2.81e-04, 1.15e-05, 1.34e+01, 1.34e+01]    [1.68e-02, 2.81e-04, 1.15e-05, 1.34e+01, 1.34e+01]    []  
2000      [9.38e-03, 2.54e-04, 1.99e-05, 1.38e+01, 1.38e+01]    [9.38e-03, 2.54e-04, 1.99e-05, 1.38e+01, 1.38e+01]    []  
42000     [1.76e-04, 6.49e-05, 7.32e-07, 2.21e-01, 2.21e-01]    [1.76e-04, 6.49e-05, 7.32e-07, 2.21e-01, 2.21e-01]    []  
47000     [2.59e-05, 2.17e-07, 1.10e-05, 1.12e-01, 1.12e-01]    [2.59e-05, 2.17e-07, 1.10e-05, 1.12e-01, 1.12e-01]    []  
36000     [1.07e-04, 3.75e-05, 1.93e-05, 2.10e-01, 2.10e-01]    [1.07e-04, 3.75e-05, 1.93e-05, 2.10e-01, 2.10e-01]    []  
22000     [5.49e-04, 3.50e-04, 5.12e-06, 8.66e-01, 8.66e-01]    [5.49e-04, 3.50e-04, 5.12e-06, 8.66e-01, 8.66e-01]    []  
3000      [1.96e-03, 3.68e-04, 1.63e-05, 8.12e+00, 8.12e+00]    [1.96e-03, 3.68e-04, 1.63e-05, 8.12e+00, 8.12e+00]    []  
3000      [1.53e-03, 4.53e-04, 2.98e-05, 9.63e+00, 9.63e+00]    [1.53e-03, 4.53e-04, 2.98e-05, 9.63e+00, 9.63e+00]    []  
37000     [1.21e-04, 3.79e-05, 4.97e-06, 2.04e-01, 2.04e-01]    [1.21e-04, 3.79e-05, 4.97e-06, 2.04e-01, 2.04e-01]    []  
48000     [7.84e-06, 5.65e-07, 3.51e-07, 1.39e-02, 1.39e-02]    [7.84e-06, 5.65e-07, 3.51e-07, 1.39e-02, 1.39e-02]    []  
43000     [1.52e-04, 5.40e-05, 1.92e-06, 4.03e-01, 4.03e-01]    [1.52e-04, 5.40e-05, 1.92e-06, 4.03e-01, 4.03e-01]    []  
23000     [4.95e-04, 3.32e-04, 2.83e-06, 7.74e-01, 7.74e-01]    [4.95e-04, 3.32e-04, 2.83e-06, 7.74e-01, 7.74e-01]    []  
38000     [1.05e-04, 4.17e-05, 2.89e-06, 1.83e-01, 1.83e-01]    [1.05e-04, 4.17e-05, 2.89e-06, 1.83e-01, 1.83e-01]    []  
4000      [6.30e-04, 3.70e-04, 1.78e-06, 7.49e+00, 7.49e+00]    [6.30e-04, 3.70e-04, 1.78e-06, 7.49e+00, 7.49e+00]    []  
4000      [2.28e-03, 5.06e-04, 2.45e-06, 4.43e+00, 4.43e+00]    [2.28e-03, 5.06e-04, 2.45e-06, 4.43e+00, 4.43e+00]    []  
49000     [1.21e-05, 8.44e-07, 7.82e-07, 4.76e-02, 4.76e-02]    [1.21e-05, 8.44e-07, 7.82e-07, 4.76e-02, 4.76e-02]    []  
24000     [4.51e-04, 3.23e-04, 3.59e-06, 6.24e-01, 6.24e-01]    [4.51e-04, 3.23e-04, 3.59e-06, 6.24e-01, 6.24e-01]    []  
44000     [1.37e-04, 4.90e-05, 1.39e-06, 3.23e-01, 3.23e-01]    [1.37e-04, 4.90e-05, 1.39e-06, 3.23e-01, 3.23e-01]    []  
39000     [1.02e-04, 4.35e-05, 3.10e-06, 5.33e-02, 5.33e-02]    [1.02e-04, 4.35e-05, 3.10e-06, 5.33e-02, 5.33e-02]    []  
5000      [7.35e-04, 3.73e-04, 1.32e-06, 6.77e+00, 6.77e+00]    [7.35e-04, 3.73e-04, 1.32e-06, 6.77e+00, 6.77e+00]    []  
5000      [1.62e-03, 1.36e-03, 1.43e-05, 2.92e+00, 2.92e+00]    [1.62e-03, 1.36e-03, 1.43e-05, 2.92e+00, 2.92e+00]    []  
40000     [3.18e-04, 3.89e-05, 1.70e-05, 1.30e+00, 1.30e+00]    [3.18e-04, 3.89e-05, 1.70e-05, 1.30e+00, 1.30e+00]    []  
25000     [4.21e-04, 3.03e-04, 3.48e-06, 5.94e-01, 5.94e-01]    [4.21e-04, 3.03e-04, 3.48e-06, 5.94e-01, 5.94e-01]    []  
50000     [5.09e-06, 2.34e-07, 1.11e-07, 1.17e-02, 1.17e-02]    [5.09e-06, 2.34e-07, 1.11e-07, 1.17e-02, 1.17e-02]    []  

Best model at step 50000:
  train loss: 2.33e-02
  test loss: 2.33e-02
  test metric: []

'train' took 26676.707823 s

[I 2023-10-09 21:51:24,748] Trial 56 finished with value: 0.6099648544447722 and parameters: {'num_domain': 84116, 'num_boundary': 5274, 'resampling_period': 12681, 'lr': 0.0023648191255696404}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000297 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.82e+01, 4.53e-05, 2.76e-07, 3.61e+03, 3.61e+03]    [4.82e+01, 4.53e-05, 2.76e-07, 3.61e+03, 3.61e+03]    []  
45000     [1.29e-04, 4.84e-05, 5.60e-07, 2.18e-01, 2.18e-01]    [1.29e-04, 4.84e-05, 5.60e-07, 2.18e-01, 2.18e-01]    []  
6000      [7.49e-04, 3.91e-04, 3.34e-06, 4.50e+00, 4.50e+00]    [7.49e-04, 3.91e-04, 3.34e-06, 4.50e+00, 4.50e+00]    []  
41000     [9.76e-05, 4.39e-05, 4.54e-06, 8.28e-02, 8.28e-02]    [9.76e-05, 4.39e-05, 4.54e-06, 8.28e-02, 8.28e-02]    []  
6000      [1.03e-03, 1.52e-03, 7.63e-06, 1.74e+00, 1.74e+00]    [1.03e-03, 1.52e-03, 7.63e-06, 1.74e+00, 1.74e+00]    []  
26000     [4.21e-04, 2.90e-04, 2.84e-06, 5.04e-01, 5.04e-01]    [4.21e-04, 2.90e-04, 2.84e-06, 5.04e-01, 5.04e-01]    []  
1000      [9.33e-10, 1.05e-03, 1.42e-15, 2.09e+03, 2.09e+03]    [9.33e-10, 1.05e-03, 1.42e-15, 2.09e+03, 2.09e+03]    []  
42000     [1.70e-04, 5.58e-05, 2.07e-05, 2.28e+00, 2.28e+00]    [1.70e-04, 5.58e-05, 2.07e-05, 2.28e+00, 2.28e+00]    []  
46000     [1.27e-04, 4.17e-05, 2.41e-06, 5.30e-01, 5.30e-01]    [1.27e-04, 4.17e-05, 2.41e-06, 5.30e-01, 5.30e-01]    []  
7000      [1.28e-03, 5.08e-04, 1.19e-05, 9.62e+00, 9.62e+00]    [1.28e-03, 5.08e-04, 1.19e-05, 9.62e+00, 9.62e+00]    []  
7000      [1.13e-03, 1.23e-03, 7.81e-05, 1.95e+00, 1.95e+00]    [1.13e-03, 1.23e-03, 7.81e-05, 1.95e+00, 1.95e+00]    []  
27000     [4.08e-04, 2.62e-04, 4.24e-06, 4.71e-01, 4.71e-01]    [4.08e-04, 2.62e-04, 4.24e-06, 4.71e-01, 4.71e-01]    []  
43000     [1.10e-04, 4.83e-05, 1.84e-05, 7.89e-01, 7.89e-01]    [1.10e-04, 4.83e-05, 1.84e-05, 7.89e-01, 7.89e-01]    []  
2000      [1.91e-02, 1.54e-03, 1.83e-04, 2.91e+01, 2.91e+01]    [1.91e-02, 1.54e-03, 1.83e-04, 2.91e+01, 2.91e+01]    []  
8000      [1.50e-03, 1.05e-03, 2.21e-04, 2.13e+00, 2.13e+00]    [1.50e-03, 1.05e-03, 2.21e-04, 2.13e+00, 2.13e+00]    []  
28000     [4.44e-04, 2.39e-04, 6.92e-06, 1.19e+00, 1.19e+00]    [4.44e-04, 2.39e-04, 6.92e-06, 1.19e+00, 1.19e+00]    []  
44000     [1.06e-04, 5.66e-05, 6.44e-06, 6.01e-02, 6.01e-02]    [1.06e-04, 5.66e-05, 6.44e-06, 6.01e-02, 6.01e-02]    []  
8000      [8.74e-04, 1.23e-03, 4.33e-05, 1.57e+00, 1.57e+00]    [8.74e-04, 1.23e-03, 4.33e-05, 1.57e+00, 1.57e+00]    []  
47000     [1.22e-04, 4.32e-05, 5.19e-07, 1.92e-01, 1.92e-01]    [1.22e-04, 4.32e-05, 5.19e-07, 1.92e-01, 1.92e-01]    []  
45000     [8.58e-05, 5.09e-05, 6.02e-06, 3.03e-01, 3.03e-01]    [8.58e-05, 5.09e-05, 6.02e-06, 3.03e-01, 3.03e-01]    []  
3000      [3.33e-03, 5.93e-04, 3.08e-04, 1.67e+01, 1.67e+01]    [3.33e-03, 5.93e-04, 3.08e-04, 1.67e+01, 1.67e+01]    []  
29000     [4.02e-04, 2.10e-04, 3.46e-06, 4.18e-01, 4.18e-01]    [4.02e-04, 2.10e-04, 3.46e-06, 4.18e-01, 4.18e-01]    []  
9000      [8.63e-04, 1.33e-03, 2.80e-05, 1.19e+00, 1.19e+00]    [8.63e-04, 1.33e-03, 2.80e-05, 1.19e+00, 1.19e+00]    []  
9000      [8.67e-04, 1.17e-03, 2.64e-05, 1.62e+00, 1.62e+00]    [8.67e-04, 1.17e-03, 2.64e-05, 1.62e+00, 1.62e+00]    []  
48000     [1.18e-04, 4.00e-05, 4.80e-07, 2.16e-01, 2.16e-01]    [1.18e-04, 4.00e-05, 4.80e-07, 2.16e-01, 2.16e-01]    []  
46000     [7.49e-05, 5.53e-05, 4.20e-06, 5.57e-02, 5.57e-02]    [7.49e-05, 5.53e-05, 4.20e-06, 5.57e-02, 5.57e-02]    []  
30000     [3.96e-04, 1.91e-04, 2.95e-06, 5.33e-01, 5.33e-01]    [3.96e-04, 1.91e-04, 2.95e-06, 5.33e-01, 5.33e-01]    []  
4000      [3.82e-04, 7.40e-04, 3.55e-06, 1.22e+01, 1.22e+01]    [3.82e-04, 7.40e-04, 3.55e-06, 1.22e+01, 1.22e+01]    []  
10000     [7.47e-04, 1.35e-03, 7.36e-06, 1.10e+00, 1.10e+00]    [7.47e-04, 1.35e-03, 7.36e-06, 1.10e+00, 1.10e+00]    []  
47000     [7.97e-05, 6.27e-05, 6.93e-06, 3.95e-02, 3.95e-02]    [7.97e-05, 6.27e-05, 6.93e-06, 3.95e-02, 3.95e-02]    []  
10000     [7.58e-04, 1.10e-03, 2.23e-05, 9.62e-01, 9.62e-01]    [7.58e-04, 1.10e-03, 2.23e-05, 9.62e-01, 9.62e-01]    []  
49000     [1.15e-04, 3.80e-05, 4.19e-07, 2.39e-01, 2.39e-01]    [1.15e-04, 3.80e-05, 4.19e-07, 2.39e-01, 2.39e-01]    []  
31000     [3.52e-04, 1.70e-04, 5.93e-06, 4.23e-01, 4.23e-01]    [3.52e-04, 1.70e-04, 5.93e-06, 4.23e-01, 4.23e-01]    []  
48000     [9.81e-05, 6.40e-05, 9.48e-06, 1.40e-01, 1.40e-01]    [9.81e-05, 6.40e-05, 9.48e-06, 1.40e-01, 1.40e-01]    []  
11000     [7.30e-04, 1.33e-03, 5.87e-06, 1.31e+00, 1.31e+00]    [7.30e-04, 1.33e-03, 5.87e-06, 1.31e+00, 1.31e+00]    []  
5000      [5.04e-04, 5.29e-04, 4.00e-07, 6.19e+00, 6.19e+00]    [5.04e-04, 5.29e-04, 4.00e-07, 6.19e+00, 6.19e+00]    []  
11000     [7.44e-04, 9.75e-04, 2.92e-05, 7.30e-01, 7.30e-01]    [7.44e-04, 9.75e-04, 2.92e-05, 7.30e-01, 7.30e-01]    []  
49000     [6.56e-05, 6.55e-05, 1.56e-06, 3.65e-02, 3.65e-02]    [6.56e-05, 6.55e-05, 1.56e-06, 3.65e-02, 3.65e-02]    []  
32000     [3.32e-04, 1.54e-04, 5.65e-06, 3.16e-01, 3.16e-01]    [3.32e-04, 1.54e-04, 5.65e-06, 3.16e-01, 3.16e-01]    []  
50000     [1.10e-04, 3.28e-05, 7.56e-07, 1.99e-01, 1.99e-01]    [1.10e-04, 3.28e-05, 7.56e-07, 1.99e-01, 1.99e-01]    []  

Best model at step 47000:
  train loss: 3.83e-01
  test loss: 3.83e-01
  test metric: []

'train' took 29775.594425 s

[I 2023-10-09 22:43:03,809] Trial 57 finished with value: 0.6879855161350352 and parameters: {'num_domain': 85405, 'num_boundary': 5176, 'resampling_period': 11491, 'lr': 0.00031591030443979946}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000189 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [2.83e+01, 5.51e-05, 5.66e-08, 3.61e+03, 3.61e+03]    [2.83e+01, 5.51e-05, 5.66e-08, 3.61e+03, 3.61e+03]    []  
12000     [7.10e-04, 1.14e-03, 1.94e-06, 5.71e-01, 5.71e-01]    [7.10e-04, 1.14e-03, 1.94e-06, 5.71e-01, 5.71e-01]    []  
50000     [1.10e-04, 6.52e-05, 7.72e-06, 4.63e-02, 4.63e-02]    [1.10e-04, 6.52e-05, 7.72e-06, 4.63e-02, 4.63e-02]    []  

Best model at step 49000:
  train loss: 7.32e-02
  test loss: 7.32e-02
  test metric: []

'train' took 17062.949318 s

[I 2023-10-09 22:47:02,555] Trial 59 finished with value: 0.5313457917439086 and parameters: {'num_domain': 85728, 'num_boundary': 5089, 'resampling_period': 12430, 'lr': 0.0010479492246196253}. Best is trial 0 with value: 0.4913720930752669.
6000      [7.26e-04, 5.02e-04, 2.14e-07, 3.02e+00, 3.02e+00]    [7.26e-04, 5.02e-04, 2.14e-07, 3.02e+00, 3.02e+00]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000124 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.12e+02, 1.17e-03, 1.66e-07, 3.63e+03, 3.63e+03]    [1.12e+02, 1.17e-03, 1.66e-07, 3.63e+03, 3.63e+03]    []  
12000     [5.77e-04, 8.98e-04, 3.95e-05, 8.31e-01, 8.31e-01]    [5.77e-04, 8.98e-04, 3.95e-05, 8.31e-01, 8.31e-01]    []  
33000     [3.22e-04, 1.38e-04, 2.40e-06, 6.32e-01, 6.32e-01]    [3.22e-04, 1.38e-04, 2.40e-06, 6.32e-01, 6.32e-01]    []  
1000      [1.17e-09, 1.05e-03, 2.18e-14, 2.09e+03, 2.09e+03]    [1.17e-09, 1.05e-03, 2.18e-14, 2.09e+03, 2.09e+03]    []  
1000      [5.01e-10, 1.05e-03, 6.48e-16, 2.09e+03, 2.09e+03]    [5.01e-10, 1.05e-03, 6.48e-16, 2.09e+03, 2.09e+03]    []  
13000     [6.48e-04, 9.49e-04, 3.31e-06, 4.54e-01, 4.54e-01]    [6.48e-04, 9.49e-04, 3.31e-06, 4.54e-01, 4.54e-01]    []  
13000     [4.86e-04, 7.54e-04, 4.00e-05, 9.14e-01, 9.14e-01]    [4.86e-04, 7.54e-04, 4.00e-05, 9.14e-01, 9.14e-01]    []  
7000      [4.16e-04, 3.33e-04, 4.56e-06, 2.44e+00, 2.44e+00]    [4.16e-04, 3.33e-04, 4.56e-06, 2.44e+00, 2.44e+00]    []  
34000     [2.99e-04, 1.23e-04, 1.61e-06, 1.05e+00, 1.05e+00]    [2.99e-04, 1.23e-04, 1.61e-06, 1.05e+00, 1.05e+00]    []  
2000      [9.60e-03, 7.56e-04, 8.54e-05, 1.87e+01, 1.87e+01]    [9.60e-03, 7.56e-04, 8.54e-05, 1.87e+01, 1.87e+01]    []  
14000     [5.69e-04, 7.34e-04, 2.13e-06, 5.42e-01, 5.42e-01]    [5.69e-04, 7.34e-04, 2.13e-06, 5.42e-01, 5.42e-01]    []  
2000      [2.35e-02, 2.96e-04, 2.57e-05, 2.63e+01, 2.63e+01]    [2.35e-02, 2.96e-04, 2.57e-05, 2.63e+01, 2.63e+01]    []  
14000     [3.85e-04, 6.18e-04, 1.84e-05, 3.85e-01, 3.85e-01]    [3.85e-04, 6.18e-04, 1.84e-05, 3.85e-01, 3.85e-01]    []  
3000      [6.41e-04, 1.13e-03, 7.71e-06, 1.04e+01, 1.04e+01]    [6.41e-04, 1.13e-03, 7.71e-06, 1.04e+01, 1.04e+01]    []  
35000     [2.61e-04, 1.09e-04, 3.87e-06, 3.64e-01, 3.64e-01]    [2.61e-04, 1.09e-04, 3.87e-06, 3.64e-01, 3.64e-01]    []  
8000      [8.14e-04, 5.62e-04, 1.38e-05, 2.58e+00, 2.58e+00]    [8.14e-04, 5.62e-04, 1.38e-05, 2.58e+00, 2.58e+00]    []  
4000      [6.46e-04, 5.11e-04, 1.74e-06, 7.60e+00, 7.60e+00]    [6.46e-04, 5.11e-04, 1.74e-06, 7.60e+00, 7.60e+00]    []  
15000     [4.56e-04, 5.87e-04, 9.44e-06, 8.85e-01, 8.85e-01]    [4.56e-04, 5.87e-04, 9.44e-06, 8.85e-01, 8.85e-01]    []  
36000     [2.49e-04, 9.99e-05, 5.10e-06, 4.09e-01, 4.09e-01]    [2.49e-04, 9.99e-05, 5.10e-06, 4.09e-01, 4.09e-01]    []  
15000     [3.99e-04, 5.48e-04, 4.25e-05, 8.83e-01, 8.83e-01]    [3.99e-04, 5.48e-04, 4.25e-05, 8.83e-01, 8.83e-01]    []  
3000      [8.58e-04, 7.21e-04, 8.94e-06, 9.70e+00, 9.70e+00]    [8.58e-04, 7.21e-04, 8.94e-06, 9.70e+00, 9.70e+00]    []  
9000      [7.73e-04, 8.25e-04, 4.14e-06, 1.78e+00, 1.78e+00]    [7.73e-04, 8.25e-04, 4.14e-06, 1.78e+00, 1.78e+00]    []  
5000      [1.68e-03, 2.27e-04, 7.21e-05, 1.15e+01, 1.15e+01]    [1.68e-03, 2.27e-04, 7.21e-05, 1.15e+01, 1.15e+01]    []  
16000     [3.61e-04, 4.77e-04, 2.93e-06, 4.55e-01, 4.55e-01]    [3.61e-04, 4.77e-04, 2.93e-06, 4.55e-01, 4.55e-01]    []  
37000     [2.57e-04, 9.85e-05, 2.01e-06, 6.63e-01, 6.63e-01]    [2.57e-04, 9.85e-05, 2.01e-06, 6.63e-01, 6.63e-01]    []  
16000     [4.14e-04, 4.45e-04, 4.19e-05, 8.01e-01, 8.01e-01]    [4.14e-04, 4.45e-04, 4.19e-05, 8.01e-01, 8.01e-01]    []  
6000      [9.08e-04, 6.35e-04, 3.02e-05, 4.54e+00, 4.54e+00]    [9.08e-04, 6.35e-04, 3.02e-05, 4.54e+00, 4.54e+00]    []  
4000      [1.26e-03, 6.87e-04, 6.29e-06, 8.10e+00, 8.10e+00]    [1.26e-03, 6.87e-04, 6.29e-06, 8.10e+00, 8.10e+00]    []  
10000     [7.89e-04, 9.30e-04, 1.34e-06, 9.84e-01, 9.84e-01]    [7.89e-04, 9.30e-04, 1.34e-06, 9.84e-01, 9.84e-01]    []  
38000     [2.02e-04, 8.59e-05, 5.82e-06, 3.76e-01, 3.76e-01]    [2.02e-04, 8.59e-05, 5.82e-06, 3.76e-01, 3.76e-01]    []  
17000     [3.32e-04, 4.01e-04, 2.25e-06, 3.11e-01, 3.11e-01]    [3.32e-04, 4.01e-04, 2.25e-06, 3.11e-01, 3.11e-01]    []  
7000      [1.15e-03, 5.20e-04, 2.03e-06, 2.58e+00, 2.58e+00]    [1.15e-03, 5.20e-04, 2.03e-06, 2.58e+00, 2.58e+00]    []  
17000     [2.44e-04, 2.70e-04, 2.59e-05, 2.74e-01, 2.74e-01]    [2.44e-04, 2.70e-04, 2.59e-05, 2.74e-01, 2.74e-01]    []  
11000     [7.06e-04, 1.11e-03, 4.77e-06, 9.04e-01, 9.04e-01]    [7.06e-04, 1.11e-03, 4.77e-06, 9.04e-01, 9.04e-01]    []  
5000      [9.05e-04, 4.16e-04, 9.29e-06, 4.80e+00, 4.80e+00]    [9.05e-04, 4.16e-04, 9.29e-06, 4.80e+00, 4.80e+00]    []  
8000      [1.13e-03, 8.59e-04, 2.13e-06, 1.69e+00, 1.69e+00]    [1.13e-03, 8.59e-04, 2.13e-06, 1.69e+00, 1.69e+00]    []  
39000     [1.87e-04, 8.28e-05, 3.93e-06, 2.56e-01, 2.56e-01]    [1.87e-04, 8.28e-05, 3.93e-06, 2.56e-01, 2.56e-01]    []  
18000     [2.90e-04, 3.29e-04, 4.01e-06, 8.86e-01, 8.86e-01]    [2.90e-04, 3.29e-04, 4.01e-06, 8.86e-01, 8.86e-01]    []  
18000     [2.21e-04, 2.69e-04, 3.36e-05, 2.36e-01, 2.36e-01]    [2.21e-04, 2.69e-04, 3.36e-05, 2.36e-01, 2.36e-01]    []  
9000      [1.11e-03, 1.25e-03, 4.61e-05, 1.69e+00, 1.69e+00]    [1.11e-03, 1.25e-03, 4.61e-05, 1.69e+00, 1.69e+00]    []  
40000     [2.39e-04, 8.26e-05, 2.30e-06, 4.99e-01, 4.99e-01]    [2.39e-04, 8.26e-05, 2.30e-06, 4.99e-01, 4.99e-01]    []  
12000     [6.69e-04, 1.15e-03, 4.56e-06, 6.66e-01, 6.66e-01]    [6.69e-04, 1.15e-03, 4.56e-06, 6.66e-01, 6.66e-01]    []  
6000      [1.01e-03, 2.25e-04, 2.28e-05, 2.77e+00, 2.77e+00]    [1.01e-03, 2.25e-04, 2.28e-05, 2.77e+00, 2.77e+00]    []  
19000     [2.70e-04, 2.48e-04, 5.46e-06, 5.84e-01, 5.84e-01]    [2.70e-04, 2.48e-04, 5.46e-06, 5.84e-01, 5.84e-01]    []  
10000     [6.83e-04, 1.58e-03, 2.02e-05, 2.52e+00, 2.52e+00]    [6.83e-04, 1.58e-03, 2.02e-05, 2.52e+00, 2.52e+00]    []  
19000     [2.12e-04, 2.65e-04, 2.01e-05, 2.98e-01, 2.98e-01]    [2.12e-04, 2.65e-04, 2.01e-05, 2.98e-01, 2.98e-01]    []  
41000     [1.79e-04, 7.69e-05, 2.88e-06, 2.15e-01, 2.15e-01]    [1.79e-04, 7.69e-05, 2.88e-06, 2.15e-01, 2.15e-01]    []  
11000     [6.29e-04, 1.43e-03, 9.21e-06, 1.04e+00, 1.04e+00]    [6.29e-04, 1.43e-03, 9.21e-06, 1.04e+00, 1.04e+00]    []  
13000     [6.59e-04, 1.14e-03, 7.36e-06, 6.28e-01, 6.28e-01]    [6.59e-04, 1.14e-03, 7.36e-06, 6.28e-01, 6.28e-01]    []  
20000     [1.93e-04, 2.26e-04, 8.53e-06, 4.49e-01, 4.49e-01]    [1.93e-04, 2.26e-04, 8.53e-06, 4.49e-01, 4.49e-01]    []  
7000      [1.04e-03, 6.91e-04, 3.67e-06, 2.57e+00, 2.57e+00]    [1.04e-03, 6.91e-04, 3.67e-06, 2.57e+00, 2.57e+00]    []  
20000     [2.11e-04, 2.52e-04, 2.33e-05, 5.31e-01, 5.31e-01]    [2.11e-04, 2.52e-04, 2.33e-05, 5.31e-01, 5.31e-01]    []  
42000     [1.91e-04, 7.44e-05, 2.59e-06, 1.92e-01, 1.92e-01]    [1.91e-04, 7.44e-05, 2.59e-06, 1.92e-01, 1.92e-01]    []  
12000     [5.05e-04, 1.39e-03, 6.60e-06, 8.36e-01, 8.36e-01]    [5.05e-04, 1.39e-03, 6.60e-06, 8.36e-01, 8.36e-01]    []  
21000     [1.83e-04, 2.16e-04, 3.23e-05, 2.80e-01, 2.80e-01]    [1.83e-04, 2.16e-04, 3.23e-05, 2.80e-01, 2.80e-01]    []  
14000     [5.70e-04, 1.10e-03, 5.87e-07, 8.19e-01, 8.19e-01]    [5.70e-04, 1.10e-03, 5.87e-07, 8.19e-01, 8.19e-01]    []  
13000     [2.67e-04, 3.75e-04, 3.04e-05, 6.17e-01, 6.17e-01]    [2.67e-04, 3.75e-04, 3.04e-05, 6.17e-01, 6.17e-01]    []  
43000     [1.78e-04, 6.96e-05, 4.18e-06, 2.61e-01, 2.61e-01]    [1.78e-04, 6.96e-05, 4.18e-06, 2.61e-01, 2.61e-01]    []  
8000      [9.77e-04, 1.09e-03, 5.45e-06, 2.58e+00, 2.58e+00]    [9.77e-04, 1.09e-03, 5.45e-06, 2.58e+00, 2.58e+00]    []  
21000     [1.49e-04, 2.53e-04, 1.09e-05, 1.59e-01, 1.59e-01]    [1.49e-04, 2.53e-04, 1.09e-05, 1.59e-01, 1.59e-01]    []  
14000     [2.42e-04, 1.68e-04, 1.59e-05, 6.12e-01, 6.12e-01]    [2.42e-04, 1.68e-04, 1.59e-05, 6.12e-01, 6.12e-01]    []  
22000     [1.92e-04, 2.22e-04, 2.11e-06, 2.14e-01, 2.14e-01]    [1.92e-04, 2.22e-04, 2.11e-06, 2.14e-01, 2.14e-01]    []  
15000     [5.57e-04, 1.05e-03, 4.71e-06, 9.43e-01, 9.43e-01]    [5.57e-04, 1.05e-03, 4.71e-06, 9.43e-01, 9.43e-01]    []  
44000     [1.92e-04, 7.03e-05, 1.85e-06, 3.29e-01, 3.29e-01]    [1.92e-04, 7.03e-05, 1.85e-06, 3.29e-01, 3.29e-01]    []  
22000     [1.57e-04, 1.92e-04, 8.30e-06, 1.65e-01, 1.65e-01]    [1.57e-04, 1.92e-04, 8.30e-06, 1.65e-01, 1.65e-01]    []  
9000      [9.37e-04, 1.16e-03, 6.11e-06, 1.14e+00, 1.14e+00]    [9.37e-04, 1.16e-03, 6.11e-06, 1.14e+00, 1.14e+00]    []  
15000     [2.47e-04, 1.32e-04, 8.81e-06, 5.43e-01, 5.43e-01]    [2.47e-04, 1.32e-04, 8.81e-06, 5.43e-01, 5.43e-01]    []  
23000     [1.86e-04, 2.23e-04, 7.16e-05, 1.85e+00, 1.85e+00]    [1.86e-04, 2.23e-04, 7.16e-05, 1.85e+00, 1.85e+00]    []  
45000     [1.48e-04, 6.64e-05, 2.42e-06, 2.56e-01, 2.56e-01]    [1.48e-04, 6.64e-05, 2.42e-06, 2.56e-01, 2.56e-01]    []  
16000     [5.77e-04, 9.87e-04, 1.07e-05, 1.09e+00, 1.09e+00]    [5.77e-04, 9.87e-04, 1.07e-05, 1.09e+00, 1.09e+00]    []  
16000     [2.36e-04, 1.46e-04, 1.99e-05, 4.89e-01, 4.89e-01]    [2.36e-04, 1.46e-04, 1.99e-05, 4.89e-01, 4.89e-01]    []  
23000     [1.14e-04, 1.80e-04, 8.33e-06, 1.41e-01, 1.41e-01]    [1.14e-04, 1.80e-04, 8.33e-06, 1.41e-01, 1.41e-01]    []  
10000     [7.43e-04, 7.43e-04, 5.67e-06, 1.53e+00, 1.53e+00]    [7.43e-04, 7.43e-04, 5.67e-06, 1.53e+00, 1.53e+00]    []  
46000     [1.66e-04, 6.72e-05, 1.51e-06, 2.20e-01, 2.20e-01]    [1.66e-04, 6.72e-05, 1.51e-06, 2.20e-01, 2.20e-01]    []  
24000     [1.62e-04, 2.34e-04, 4.48e-06, 1.54e-01, 1.54e-01]    [1.62e-04, 2.34e-04, 4.48e-06, 1.54e-01, 1.54e-01]    []  
17000     [2.16e-04, 1.40e-04, 8.02e-06, 6.26e-01, 6.26e-01]    [2.16e-04, 1.40e-04, 8.02e-06, 6.26e-01, 6.26e-01]    []  
17000     [5.50e-04, 9.51e-04, 8.19e-06, 3.95e-01, 3.95e-01]    [5.50e-04, 9.51e-04, 8.19e-06, 3.95e-01, 3.95e-01]    []  
24000     [9.95e-05, 1.54e-04, 4.20e-06, 1.19e-01, 1.19e-01]    [9.95e-05, 1.54e-04, 4.20e-06, 1.19e-01, 1.19e-01]    []  
18000     [1.94e-04, 1.42e-04, 4.23e-06, 8.76e-01, 8.76e-01]    [1.94e-04, 1.42e-04, 4.23e-06, 8.76e-01, 8.76e-01]    []  
47000     [1.52e-04, 6.68e-05, 2.10e-06, 1.76e-01, 1.76e-01]    [1.52e-04, 6.68e-05, 2.10e-06, 1.76e-01, 1.76e-01]    []  
11000     [6.41e-04, 5.78e-04, 5.03e-06, 5.88e-01, 5.88e-01]    [6.41e-04, 5.78e-04, 5.03e-06, 5.88e-01, 5.88e-01]    []  
25000     [1.82e-04, 2.32e-04, 5.17e-05, 6.49e-01, 6.49e-01]    [1.82e-04, 2.32e-04, 5.17e-05, 6.49e-01, 6.49e-01]    []  
19000     [2.24e-04, 1.11e-04, 8.28e-06, 3.02e-01, 3.02e-01]    [2.24e-04, 1.11e-04, 8.28e-06, 3.02e-01, 3.02e-01]    []  
18000     [6.38e-04, 8.68e-04, 4.68e-06, 4.63e-01, 4.63e-01]    [6.38e-04, 8.68e-04, 4.68e-06, 4.63e-01, 4.63e-01]    []  
25000     [1.40e-04, 1.26e-04, 1.17e-05, 1.98e-01, 1.98e-01]    [1.40e-04, 1.26e-04, 1.17e-05, 1.98e-01, 1.98e-01]    []  
48000     [1.28e-04, 6.67e-05, 1.86e-06, 1.52e-01, 1.52e-01]    [1.28e-04, 6.67e-05, 1.86e-06, 1.52e-01, 1.52e-01]    []  
26000     [1.61e-04, 2.30e-04, 5.08e-07, 1.04e-01, 1.04e-01]    [1.61e-04, 2.30e-04, 5.08e-07, 1.04e-01, 1.04e-01]    []  
20000     [1.85e-04, 9.56e-05, 7.83e-06, 3.95e-01, 3.95e-01]    [1.85e-04, 9.56e-05, 7.83e-06, 3.95e-01, 3.95e-01]    []  
12000     [6.49e-04, 5.90e-04, 2.35e-06, 6.46e-01, 6.46e-01]    [6.49e-04, 5.90e-04, 2.35e-06, 6.46e-01, 6.46e-01]    []  
26000     [4.53e-04, 1.24e-04, 4.88e-05, 2.28e+00, 2.28e+00]    [4.53e-04, 1.24e-04, 4.88e-05, 2.28e+00, 2.28e+00]    []  
19000     [4.77e-04, 7.02e-04, 6.03e-06, 5.24e-01, 5.24e-01]    [4.77e-04, 7.02e-04, 6.03e-06, 5.24e-01, 5.24e-01]    []  
49000     [1.31e-04, 6.61e-05, 1.48e-05, 1.87e-01, 1.87e-01]    [1.31e-04, 6.61e-05, 1.48e-05, 1.87e-01, 1.87e-01]    []  
21000     [1.37e-04, 9.39e-05, 3.04e-06, 3.59e-01, 3.59e-01]    [1.37e-04, 9.39e-05, 3.04e-06, 3.59e-01, 3.59e-01]    []  
27000     [1.80e-04, 2.32e-04, 3.13e-06, 1.31e+00, 1.31e+00]    [1.80e-04, 2.32e-04, 3.13e-06, 1.31e+00, 1.31e+00]    []  
13000     [5.30e-04, 3.16e-04, 3.26e-06, 5.74e-01, 5.74e-01]    [5.30e-04, 3.16e-04, 3.26e-06, 5.74e-01, 5.74e-01]    []  
50000     [1.34e-04, 6.87e-05, 1.40e-06, 2.04e-01, 2.04e-01]    [1.34e-04, 6.87e-05, 1.40e-06, 2.04e-01, 2.04e-01]    []  

Best model at step 48000:
  train loss: 3.03e-01
  test loss: 3.03e-01
  test metric: []

'train' took 22433.906003 s

[I 2023-10-10 00:54:35,430] Trial 60 finished with value: 0.6649603737053403 and parameters: {'num_domain': 97598, 'num_boundary': 5393, 'resampling_period': 16111, 'lr': 0.00029592579733973986}. Best is trial 0 with value: 0.4913720930752669.
22000     [1.17e-04, 6.41e-05, 8.04e-06, 6.23e-01, 6.23e-01]    [1.17e-04, 6.41e-05, 8.04e-06, 6.23e-01, 6.23e-01]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000189 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [7.18e+01, 2.59e-04, 1.12e-07, 3.61e+03, 3.61e+03]    [7.18e+01, 2.59e-04, 1.12e-07, 3.61e+03, 3.61e+03]    []  
27000     [9.30e-05, 8.55e-05, 2.51e-05, 3.81e-01, 3.81e-01]    [9.30e-05, 8.55e-05, 2.51e-05, 3.81e-01, 3.81e-01]    []  
20000     [4.18e-04, 5.88e-04, 1.13e-05, 2.98e-01, 2.98e-01]    [4.18e-04, 5.88e-04, 1.13e-05, 2.98e-01, 2.98e-01]    []  
28000     [1.66e-04, 2.50e-04, 1.39e-05, 9.00e-02, 9.00e-02]    [1.66e-04, 2.50e-04, 1.39e-05, 9.00e-02, 9.00e-02]    []  
23000     [8.62e-05, 4.59e-05, 5.33e-06, 2.29e-01, 2.29e-01]    [8.62e-05, 4.59e-05, 5.33e-06, 2.29e-01, 2.29e-01]    []  
1000      [6.18e-09, 1.05e-03, 6.95e-14, 2.09e+03, 2.09e+03]    [6.18e-09, 1.05e-03, 6.95e-14, 2.09e+03, 2.09e+03]    []  
14000     [4.64e-04, 2.75e-04, 3.82e-06, 3.62e-01, 3.62e-01]    [4.64e-04, 2.75e-04, 3.82e-06, 3.62e-01, 3.62e-01]    []  
28000     [6.67e-05, 7.48e-05, 5.36e-06, 8.96e-02, 8.96e-02]    [6.67e-05, 7.48e-05, 5.36e-06, 8.96e-02, 8.96e-02]    []  
21000     [3.98e-04, 5.00e-04, 1.64e-05, 4.25e-01, 4.25e-01]    [3.98e-04, 5.00e-04, 1.64e-05, 4.25e-01, 4.25e-01]    []  
24000     [8.30e-05, 3.42e-05, 9.37e-06, 3.79e-01, 3.79e-01]    [8.30e-05, 3.42e-05, 9.37e-06, 3.79e-01, 3.79e-01]    []  
29000     [1.68e-04, 2.64e-04, 1.72e-06, 1.59e-01, 1.59e-01]    [1.68e-04, 2.64e-04, 1.72e-06, 1.59e-01, 1.59e-01]    []  
2000      [5.02e-03, 8.64e-04, 6.11e-04, 2.59e+01, 2.59e+01]    [5.02e-03, 8.64e-04, 6.11e-04, 2.59e+01, 2.59e+01]    []  
25000     [1.19e-04, 3.53e-05, 3.71e-06, 1.70e-01, 1.70e-01]    [1.19e-04, 3.53e-05, 3.71e-06, 1.70e-01, 1.70e-01]    []  
29000     [6.41e-05, 7.20e-05, 3.92e-06, 8.41e-02, 8.41e-02]    [6.41e-05, 7.20e-05, 3.92e-06, 8.41e-02, 8.41e-02]    []  
15000     [4.53e-04, 2.58e-04, 9.78e-06, 2.85e+00, 2.85e+00]    [4.53e-04, 2.58e-04, 9.78e-06, 2.85e+00, 2.85e+00]    []  
22000     [3.85e-04, 3.02e-04, 7.95e-06, 3.16e-01, 3.16e-01]    [3.85e-04, 3.02e-04, 7.95e-06, 3.16e-01, 3.16e-01]    []  
30000     [2.19e-04, 2.15e-04, 3.90e-06, 1.60e+00, 1.60e+00]    [2.19e-04, 2.15e-04, 3.90e-06, 1.60e+00, 1.60e+00]    []  
3000      [1.62e-03, 8.61e-04, 8.56e-05, 1.09e+01, 1.09e+01]    [1.62e-03, 8.61e-04, 8.56e-05, 1.09e+01, 1.09e+01]    []  
26000     [1.01e-04, 3.09e-05, 4.22e-06, 2.96e-01, 2.96e-01]    [1.01e-04, 3.09e-05, 4.22e-06, 2.96e-01, 2.96e-01]    []  
30000     [1.12e-04, 6.76e-05, 1.80e-05, 6.16e-01, 6.16e-01]    [1.12e-04, 6.76e-05, 1.80e-05, 6.16e-01, 6.16e-01]    []  
16000     [3.64e-04, 1.72e-04, 3.93e-06, 4.13e-01, 4.13e-01]    [3.64e-04, 1.72e-04, 3.93e-06, 4.13e-01, 4.13e-01]    []  
31000     [1.63e-04, 2.32e-04, 1.00e-06, 1.05e-01, 1.05e-01]    [1.63e-04, 2.32e-04, 1.00e-06, 1.05e-01, 1.05e-01]    []  
27000     [8.57e-05, 2.87e-05, 6.21e-06, 2.42e-01, 2.42e-01]    [8.57e-05, 2.87e-05, 6.21e-06, 2.42e-01, 2.42e-01]    []  
23000     [2.88e-04, 1.84e-04, 4.13e-06, 1.44e-01, 1.44e-01]    [2.88e-04, 1.84e-04, 4.13e-06, 1.44e-01, 1.44e-01]    []  
4000      [1.24e-03, 6.01e-04, 1.32e-05, 9.79e+00, 9.79e+00]    [1.24e-03, 6.01e-04, 1.32e-05, 9.79e+00, 9.79e+00]    []  
31000     [5.65e-05, 3.42e-05, 4.38e-06, 6.89e-02, 6.89e-02]    [5.65e-05, 3.42e-05, 4.38e-06, 6.89e-02, 6.89e-02]    []  
28000     [1.01e-04, 2.68e-05, 2.34e-06, 1.97e-01, 1.97e-01]    [1.01e-04, 2.68e-05, 2.34e-06, 1.97e-01, 1.97e-01]    []  
5000      [5.68e-04, 4.18e-04, 2.31e-06, 7.69e+00, 7.69e+00]    [5.68e-04, 4.18e-04, 2.31e-06, 7.69e+00, 7.69e+00]    []  
32000     [1.35e-04, 2.22e-04, 1.61e-06, 9.64e-02, 9.64e-02]    [1.35e-04, 2.22e-04, 1.61e-06, 9.64e-02, 9.64e-02]    []  
17000     [4.08e-04, 1.50e-04, 5.88e-06, 7.38e-01, 7.38e-01]    [4.08e-04, 1.50e-04, 5.88e-06, 7.38e-01, 7.38e-01]    []  
24000     [3.40e-04, 1.37e-04, 3.74e-06, 3.19e-01, 3.19e-01]    [3.40e-04, 1.37e-04, 3.74e-06, 3.19e-01, 3.19e-01]    []  
29000     [1.05e-04, 2.96e-05, 3.06e-06, 1.33e-01, 1.33e-01]    [1.05e-04, 2.96e-05, 3.06e-06, 1.33e-01, 1.33e-01]    []  
32000     [1.20e-04, 3.62e-05, 7.57e-06, 5.09e-01, 5.09e-01]    [1.20e-04, 3.62e-05, 7.57e-06, 5.09e-01, 5.09e-01]    []  
6000      [6.25e-04, 5.61e-04, 2.11e-06, 7.36e+00, 7.36e+00]    [6.25e-04, 5.61e-04, 2.11e-06, 7.36e+00, 7.36e+00]    []  
33000     [1.39e-04, 2.24e-04, 1.66e-06, 1.13e-01, 1.13e-01]    [1.39e-04, 2.24e-04, 1.66e-06, 1.13e-01, 1.13e-01]    []  
30000     [1.04e-04, 2.65e-05, 3.60e-06, 4.44e-01, 4.44e-01]    [1.04e-04, 2.65e-05, 3.60e-06, 4.44e-01, 4.44e-01]    []  
25000     [3.37e-04, 1.47e-04, 6.65e-06, 1.45e-01, 1.45e-01]    [3.37e-04, 1.47e-04, 6.65e-06, 1.45e-01, 1.45e-01]    []  
18000     [2.11e-04, 7.03e-05, 4.23e-06, 3.00e-01, 3.00e-01]    [2.11e-04, 7.03e-05, 4.23e-06, 3.00e-01, 3.00e-01]    []  
7000      [1.03e-03, 4.34e-04, 1.15e-04, 6.44e+00, 6.44e+00]    [1.03e-03, 4.34e-04, 1.15e-04, 6.44e+00, 6.44e+00]    []  
33000     [7.26e-05, 2.97e-05, 1.78e-05, 5.14e-01, 5.14e-01]    [7.26e-05, 2.97e-05, 1.78e-05, 5.14e-01, 5.14e-01]    []  
31000     [8.12e-05, 2.51e-05, 5.34e-06, 3.62e-01, 3.62e-01]    [8.12e-05, 2.51e-05, 5.34e-06, 3.62e-01, 3.62e-01]    []  
34000     [1.30e-04, 1.97e-04, 3.85e-06, 6.07e-02, 6.07e-02]    [1.30e-04, 1.97e-04, 3.85e-06, 6.07e-02, 6.07e-02]    []  
26000     [2.88e-04, 1.44e-04, 9.69e-06, 2.22e-01, 2.22e-01]    [2.88e-04, 1.44e-04, 9.69e-06, 2.22e-01, 2.22e-01]    []  
32000     [8.31e-05, 2.17e-05, 2.06e-06, 1.18e-01, 1.18e-01]    [8.31e-05, 2.17e-05, 2.06e-06, 1.18e-01, 1.18e-01]    []  
19000     [2.50e-04, 1.87e-04, 4.24e-05, 1.41e+00, 1.41e+00]    [2.50e-04, 1.87e-04, 4.24e-05, 1.41e+00, 1.41e+00]    []  
8000      [2.16e-03, 3.18e-04, 4.22e-06, 5.50e+00, 5.50e+00]    [2.16e-03, 3.18e-04, 4.22e-06, 5.50e+00, 5.50e+00]    []  
34000     [5.93e-05, 2.76e-05, 4.19e-06, 6.17e-02, 6.17e-02]    [5.93e-05, 2.76e-05, 4.19e-06, 6.17e-02, 6.17e-02]    []  
35000     [1.26e-04, 1.98e-04, 3.40e-06, 2.09e-01, 2.09e-01]    [1.26e-04, 1.98e-04, 3.40e-06, 2.09e-01, 2.09e-01]    []  
33000     [8.67e-05, 2.11e-05, 2.76e-06, 1.20e-01, 1.20e-01]    [8.67e-05, 2.11e-05, 2.76e-06, 1.20e-01, 1.20e-01]    []  
9000      [2.24e-03, 1.00e-03, 4.13e-05, 4.62e+00, 4.62e+00]    [2.24e-03, 1.00e-03, 4.13e-05, 4.62e+00, 4.62e+00]    []  
27000     [2.65e-04, 1.30e-04, 6.88e-06, 4.48e-01, 4.48e-01]    [2.65e-04, 1.30e-04, 6.88e-06, 4.48e-01, 4.48e-01]    []  
20000     [1.53e-04, 4.38e-05, 3.03e-06, 1.60e-01, 1.60e-01]    [1.53e-04, 4.38e-05, 3.03e-06, 1.60e-01, 1.60e-01]    []  
35000     [4.39e-05, 2.94e-05, 1.91e-06, 5.26e-02, 5.26e-02]    [4.39e-05, 2.94e-05, 1.91e-06, 5.26e-02, 5.26e-02]    []  
34000     [6.99e-05, 1.55e-05, 3.28e-06, 2.28e-01, 2.28e-01]    [6.99e-05, 1.55e-05, 3.28e-06, 2.28e-01, 2.28e-01]    []  
36000     [1.17e-04, 1.66e-04, 2.34e-06, 1.78e-01, 1.78e-01]    [1.17e-04, 1.66e-04, 2.34e-06, 1.78e-01, 1.78e-01]    []  
10000     [1.15e-03, 1.61e-03, 1.36e-05, 3.68e+00, 3.68e+00]    [1.15e-03, 1.61e-03, 1.36e-05, 3.68e+00, 3.68e+00]    []  
35000     [8.38e-05, 1.45e-05, 1.08e-05, 1.41e+00, 1.41e+00]    [8.38e-05, 1.45e-05, 1.08e-05, 1.41e+00, 1.41e+00]    []  
28000     [3.11e-04, 1.56e-04, 2.69e-05, 1.21e-01, 1.21e-01]    [3.11e-04, 1.56e-04, 2.69e-05, 1.21e-01, 1.21e-01]    []  
36000     [4.06e-05, 2.56e-05, 2.12e-06, 4.82e-02, 4.82e-02]    [4.06e-05, 2.56e-05, 2.12e-06, 4.82e-02, 4.82e-02]    []  
21000     [1.41e-04, 3.22e-05, 3.91e-06, 1.73e-01, 1.73e-01]    [1.41e-04, 3.22e-05, 3.91e-06, 1.73e-01, 1.73e-01]    []  
37000     [1.31e-04, 1.42e-04, 8.99e-06, 5.67e-02, 5.67e-02]    [1.31e-04, 1.42e-04, 8.99e-06, 5.67e-02, 5.67e-02]    []  
11000     [8.06e-04, 1.14e-03, 1.65e-04, 2.24e+00, 2.24e+00]    [8.06e-04, 1.14e-03, 1.65e-04, 2.24e+00, 2.24e+00]    []  
36000     [5.35e-05, 1.29e-05, 1.34e-06, 1.15e-01, 1.15e-01]    [5.35e-05, 1.29e-05, 1.34e-06, 1.15e-01, 1.15e-01]    []  
29000     [2.56e-04, 9.84e-05, 2.18e-06, 1.59e-01, 1.59e-01]    [2.56e-04, 9.84e-05, 2.18e-06, 1.59e-01, 1.59e-01]    []  
37000     [4.69e-05, 2.37e-05, 1.81e-06, 5.57e-02, 5.57e-02]    [4.69e-05, 2.37e-05, 1.81e-06, 5.57e-02, 5.57e-02]    []  
37000     [1.12e-04, 1.41e-05, 3.68e-06, 8.94e-02, 8.94e-02]    [1.12e-04, 1.41e-05, 3.68e-06, 8.94e-02, 8.94e-02]    []  
38000     [1.47e-04, 1.25e-04, 4.90e-06, 2.05e-01, 2.05e-01]    [1.47e-04, 1.25e-04, 4.90e-06, 2.05e-01, 2.05e-01]    []  
12000     [8.11e-04, 7.75e-04, 2.34e-04, 1.98e+00, 1.98e+00]    [8.11e-04, 7.75e-04, 2.34e-04, 1.98e+00, 1.98e+00]    []  
22000     [2.36e-04, 4.82e-05, 5.05e-06, 9.67e-01, 9.67e-01]    [2.36e-04, 4.82e-05, 5.05e-06, 9.67e-01, 9.67e-01]    []  
38000     [4.84e-05, 6.88e-06, 7.99e-06, 5.09e-01, 5.09e-01]    [4.84e-05, 6.88e-06, 7.99e-06, 5.09e-01, 5.09e-01]    []  
30000     [2.49e-04, 8.81e-05, 8.57e-07, 8.72e-02, 8.72e-02]    [2.49e-04, 8.81e-05, 8.57e-07, 8.72e-02, 8.72e-02]    []  
38000     [5.08e-05, 2.18e-05, 4.09e-06, 5.77e-02, 5.77e-02]    [5.08e-05, 2.18e-05, 4.09e-06, 5.77e-02, 5.77e-02]    []  
13000     [9.65e-04, 6.04e-04, 7.14e-05, 1.28e+00, 1.28e+00]    [9.65e-04, 6.04e-04, 7.14e-05, 1.28e+00, 1.28e+00]    []  
39000     [1.32e-04, 1.23e-04, 5.46e-06, 1.35e-01, 1.35e-01]    [1.32e-04, 1.23e-04, 5.46e-06, 1.35e-01, 1.35e-01]    []  
23000     [1.15e-04, 3.04e-05, 1.80e-06, 1.52e-01, 1.52e-01]    [1.15e-04, 3.04e-05, 1.80e-06, 1.52e-01, 1.52e-01]    []  
39000     [4.68e-05, 9.31e-06, 3.91e-06, 7.04e-02, 7.04e-02]    [4.68e-05, 9.31e-06, 3.91e-06, 7.04e-02, 7.04e-02]    []  
14000     [4.35e-04, 4.89e-04, 4.41e-05, 6.15e-01, 6.15e-01]    [4.35e-04, 4.89e-04, 4.41e-05, 6.15e-01, 6.15e-01]    []  
39000     [1.37e-03, 4.40e-05, 8.35e-06, 1.29e+00, 1.29e+00]    [1.37e-03, 4.40e-05, 8.35e-06, 1.29e+00, 1.29e+00]    []  
31000     [4.70e-04, 9.88e-05, 5.78e-05, 1.27e+00, 1.27e+00]    [4.70e-04, 9.88e-05, 5.78e-05, 1.27e+00, 1.27e+00]    []  
40000     [1.12e-04, 1.20e-04, 2.43e-06, 4.47e-02, 4.47e-02]    [1.12e-04, 1.20e-04, 2.43e-06, 4.47e-02, 4.47e-02]    []  
40000     [4.14e-05, 7.23e-06, 2.03e-06, 5.53e-02, 5.53e-02]    [4.14e-05, 7.23e-06, 2.03e-06, 5.53e-02, 5.53e-02]    []  
24000     [1.20e-04, 3.71e-05, 2.03e-06, 3.16e-01, 3.16e-01]    [1.20e-04, 3.71e-05, 2.03e-06, 3.16e-01, 3.16e-01]    []  
41000     [1.77e-04, 1.08e-05, 8.61e-06, 1.88e-01, 1.88e-01]    [1.77e-04, 1.08e-05, 8.61e-06, 1.88e-01, 1.88e-01]    []  
15000     [3.81e-04, 4.23e-04, 1.75e-05, 7.69e-01, 7.69e-01]    [3.81e-04, 4.23e-04, 1.75e-05, 7.69e-01, 7.69e-01]    []  
40000     [3.03e-05, 1.99e-05, 3.10e-06, 3.85e-02, 3.85e-02]    [3.03e-05, 1.99e-05, 3.10e-06, 3.85e-02, 3.85e-02]    []  
41000     [1.41e-04, 1.23e-04, 1.36e-05, 1.97e-01, 1.97e-01]    [1.41e-04, 1.23e-04, 1.36e-05, 1.97e-01, 1.97e-01]    []  
32000     [2.87e-04, 9.90e-05, 8.92e-06, 9.80e-02, 9.80e-02]    [2.87e-04, 9.90e-05, 8.92e-06, 9.80e-02, 9.80e-02]    []  
42000     [3.64e-05, 5.15e-06, 1.62e-06, 1.03e-01, 1.03e-01]    [3.64e-05, 5.15e-06, 1.62e-06, 1.03e-01, 1.03e-01]    []  
16000     [2.63e-04, 2.02e-04, 2.94e-05, 6.39e-01, 6.39e-01]    [2.63e-04, 2.02e-04, 2.94e-05, 6.39e-01, 6.39e-01]    []  
25000     [1.12e-04, 2.88e-05, 3.56e-07, 9.64e-02, 9.64e-02]    [1.12e-04, 2.88e-05, 3.56e-07, 9.64e-02, 9.64e-02]    []  
41000     [4.09e-05, 2.33e-05, 2.92e-06, 3.84e-02, 3.84e-02]    [4.09e-05, 2.33e-05, 2.92e-06, 3.84e-02, 3.84e-02]    []  
42000     [1.79e-04, 7.89e-05, 4.26e-05, 1.08e+00, 1.08e+00]    [1.79e-04, 7.89e-05, 4.26e-05, 1.08e+00, 1.08e+00]    []  
43000     [3.46e-05, 5.67e-06, 1.89e-06, 1.09e-01, 1.09e-01]    [3.46e-05, 5.67e-06, 1.89e-06, 1.09e-01, 1.09e-01]    []  
33000     [3.09e-04, 8.19e-05, 9.13e-06, 5.57e-01, 5.57e-01]    [3.09e-04, 8.19e-05, 9.13e-06, 5.57e-01, 5.57e-01]    []  
17000     [4.10e-04, 1.27e-04, 2.43e-05, 1.60e+00, 1.60e+00]    [4.10e-04, 1.27e-04, 2.43e-05, 1.60e+00, 1.60e+00]    []  
44000     [3.97e-05, 6.02e-06, 9.94e-07, 4.78e-02, 4.78e-02]    [3.97e-05, 6.02e-06, 9.94e-07, 4.78e-02, 4.78e-02]    []  
26000     [1.06e-04, 2.32e-05, 1.39e-06, 3.18e-01, 3.18e-01]    [1.06e-04, 2.32e-05, 1.39e-06, 3.18e-01, 3.18e-01]    []  
42000     [3.56e-05, 1.79e-05, 2.13e-06, 3.50e-02, 3.50e-02]    [3.56e-05, 1.79e-05, 2.13e-06, 3.50e-02, 3.50e-02]    []  
43000     [1.22e-04, 8.40e-05, 4.54e-05, 1.18e+00, 1.18e+00]    [1.22e-04, 8.40e-05, 4.54e-05, 1.18e+00, 1.18e+00]    []  
34000     [2.34e-04, 1.03e-04, 4.75e-05, 1.75e+00, 1.75e+00]    [2.34e-04, 1.03e-04, 4.75e-05, 1.75e+00, 1.75e+00]    []  
18000     [1.92e-04, 1.00e-04, 1.76e-05, 1.14e+00, 1.14e+00]    [1.92e-04, 1.00e-04, 1.76e-05, 1.14e+00, 1.14e+00]    []  
45000     [3.91e-05, 6.40e-06, 1.80e-06, 4.17e-02, 4.17e-02]    [3.91e-05, 6.40e-06, 1.80e-06, 4.17e-02, 4.17e-02]    []  
43000     [3.98e-05, 1.78e-05, 3.07e-06, 3.34e-02, 3.34e-02]    [3.98e-05, 1.78e-05, 3.07e-06, 3.34e-02, 3.34e-02]    []  
44000     [8.15e-05, 9.03e-05, 1.43e-06, 3.77e-02, 3.77e-02]    [8.15e-05, 9.03e-05, 1.43e-06, 3.77e-02, 3.77e-02]    []  
27000     [1.04e-04, 2.32e-05, 3.03e-07, 2.46e-01, 2.46e-01]    [1.04e-04, 2.32e-05, 3.03e-07, 2.46e-01, 2.46e-01]    []  
46000     [6.44e-05, 5.34e-06, 7.64e-06, 5.17e-02, 5.17e-02]    [6.44e-05, 5.34e-06, 7.64e-06, 5.17e-02, 5.17e-02]    []  
19000     [2.27e-04, 9.68e-05, 1.80e-05, 4.04e-01, 4.04e-01]    [2.27e-04, 9.68e-05, 1.80e-05, 4.04e-01, 4.04e-01]    []  
35000     [2.19e-04, 9.65e-05, 1.82e-05, 3.68e-01, 3.68e-01]    [2.19e-04, 9.65e-05, 1.82e-05, 3.68e-01, 3.68e-01]    []  
47000     [4.34e-05, 5.11e-06, 1.40e-06, 3.77e-02, 3.77e-02]    [4.34e-05, 5.11e-06, 1.40e-06, 3.77e-02, 3.77e-02]    []  
45000     [1.52e-04, 7.15e-05, 1.80e-05, 4.39e-01, 4.39e-01]    [1.52e-04, 7.15e-05, 1.80e-05, 4.39e-01, 4.39e-01]    []  
44000     [3.35e-05, 1.63e-05, 7.09e-06, 5.66e-02, 5.66e-02]    [3.35e-05, 1.63e-05, 7.09e-06, 5.66e-02, 5.66e-02]    []  
20000     [1.52e-04, 7.05e-05, 9.44e-06, 3.07e-01, 3.07e-01]    [1.52e-04, 7.05e-05, 9.44e-06, 3.07e-01, 3.07e-01]    []  
28000     [9.89e-05, 2.04e-05, 6.58e-07, 1.28e-01, 1.28e-01]    [9.89e-05, 2.04e-05, 6.58e-07, 1.28e-01, 1.28e-01]    []  
36000     [2.43e-04, 9.54e-05, 4.65e-06, 3.52e-01, 3.52e-01]    [2.43e-04, 9.54e-05, 4.65e-06, 3.52e-01, 3.52e-01]    []  
48000     [5.27e-05, 6.02e-06, 1.83e-06, 4.37e-02, 4.37e-02]    [5.27e-05, 6.02e-06, 1.83e-06, 4.37e-02, 4.37e-02]    []  
21000     [1.03e-04, 5.69e-05, 5.25e-06, 4.67e-01, 4.67e-01]    [1.03e-04, 5.69e-05, 5.25e-06, 4.67e-01, 4.67e-01]    []  
46000     [7.81e-05, 8.04e-05, 1.53e-06, 3.45e-02, 3.45e-02]    [7.81e-05, 8.04e-05, 1.53e-06, 3.45e-02, 3.45e-02]    []  
45000     [3.20e-05, 1.44e-05, 5.08e-06, 3.48e-02, 3.48e-02]    [3.20e-05, 1.44e-05, 5.08e-06, 3.48e-02, 3.48e-02]    []  
49000     [1.01e-03, 2.95e-05, 3.34e-05, 2.56e+00, 2.56e+00]    [1.01e-03, 2.95e-05, 3.34e-05, 2.56e+00, 2.56e+00]    []  
29000     [1.01e-04, 1.77e-05, 1.85e-06, 1.10e+00, 1.10e+00]    [1.01e-04, 1.77e-05, 1.85e-06, 1.10e+00, 1.10e+00]    []  
37000     [2.41e-04, 9.35e-05, 3.38e-06, 1.78e-01, 1.78e-01]    [2.41e-04, 9.35e-05, 3.38e-06, 1.78e-01, 1.78e-01]    []  
22000     [9.11e-05, 4.81e-05, 1.07e-05, 2.65e-01, 2.65e-01]    [9.11e-05, 4.81e-05, 1.07e-05, 2.65e-01, 2.65e-01]    []  
50000     [4.71e-05, 3.41e-06, 4.26e-06, 8.20e-01, 8.20e-01]    [4.71e-05, 3.41e-06, 4.26e-06, 8.20e-01, 8.20e-01]    []  

Best model at step 47000:
  train loss: 7.54e-02
  test loss: 7.54e-02
  test metric: []

'train' took 17406.568066 s

[I 2023-10-10 03:37:34,206] Trial 66 finished with value: 0.9924019285706173 and parameters: {'num_domain': 96365, 'num_boundary': 3831, 'resampling_period': 15931, 'lr': 0.0009457139234908324}. Best is trial 0 with value: 0.4913720930752669.
47000     [7.46e-05, 7.27e-05, 1.38e-06, 3.50e-02, 3.50e-02]    [7.46e-05, 7.27e-05, 1.38e-06, 3.50e-02, 3.50e-02]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000189 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.29e+01, 1.34e-04, 2.12e-07, 3.63e+03, 3.63e+03]    [4.29e+01, 1.34e-04, 2.12e-07, 3.63e+03, 3.63e+03]    []  
46000     [5.08e-05, 2.00e-05, 1.07e-05, 4.22e-01, 4.22e-01]    [5.08e-05, 2.00e-05, 1.07e-05, 4.22e-01, 4.22e-01]    []  
1000      [2.72e-10, 1.05e-03, 3.51e-16, 2.09e+03, 2.09e+03]    [2.72e-10, 1.05e-03, 3.51e-16, 2.09e+03, 2.09e+03]    []  
30000     [1.08e-04, 6.31e-05, 1.50e-06, 6.15e-01, 6.15e-01]    [1.08e-04, 6.31e-05, 1.50e-06, 6.15e-01, 6.15e-01]    []  
38000     [1.98e-04, 1.01e-04, 1.10e-05, 6.99e-02, 6.99e-02]    [1.98e-04, 1.01e-04, 1.10e-05, 6.99e-02, 6.99e-02]    []  
23000     [8.33e-05, 4.51e-05, 3.16e-06, 2.87e-01, 2.87e-01]    [8.33e-05, 4.51e-05, 3.16e-06, 2.87e-01, 2.87e-01]    []  
2000      [2.35e-01, 4.29e-04, 8.51e-04, 1.69e+02, 1.69e+02]    [2.35e-01, 4.29e-04, 8.51e-04, 1.69e+02, 1.69e+02]    []  
48000     [7.81e-05, 7.02e-05, 1.50e-06, 3.63e-02, 3.63e-02]    [7.81e-05, 7.02e-05, 1.50e-06, 3.63e-02, 3.63e-02]    []  
47000     [2.84e-05, 1.23e-05, 3.92e-06, 2.78e-02, 2.78e-02]    [2.84e-05, 1.23e-05, 3.92e-06, 2.78e-02, 2.78e-02]    []  
3000      [1.57e-02, 5.43e-04, 3.49e-03, 1.08e+01, 1.08e+01]    [1.57e-02, 5.43e-04, 3.49e-03, 1.08e+01, 1.08e+01]    []  
24000     [1.03e-04, 4.69e-05, 1.28e-05, 2.06e-01, 2.06e-01]    [1.03e-04, 4.69e-05, 1.28e-05, 2.06e-01, 2.06e-01]    []  
4000      [7.67e-03, 7.30e-04, 9.40e-04, 1.02e+01, 1.02e+01]    [7.67e-03, 7.30e-04, 9.40e-04, 1.02e+01, 1.02e+01]    []  
39000     [2.07e-04, 1.01e-04, 4.89e-06, 8.80e-02, 8.80e-02]    [2.07e-04, 1.01e-04, 4.89e-06, 8.80e-02, 8.80e-02]    []  
31000     [9.17e-05, 2.32e-05, 4.92e-07, 5.82e-02, 5.82e-02]    [9.17e-05, 2.32e-05, 4.92e-07, 5.82e-02, 5.82e-02]    []  
49000     [9.10e-05, 6.65e-05, 9.83e-06, 2.43e-01, 2.43e-01]    [9.10e-05, 6.65e-05, 9.83e-06, 2.43e-01, 2.43e-01]    []  
5000      [2.81e-03, 3.81e-04, 1.39e-04, 6.64e+00, 6.64e+00]    [2.81e-03, 3.81e-04, 1.39e-04, 6.64e+00, 6.64e+00]    []  
48000     [4.69e-05, 1.37e-05, 8.17e-06, 3.79e-02, 3.79e-02]    [4.69e-05, 1.37e-05, 8.17e-06, 3.79e-02, 3.79e-02]    []  
25000     [6.80e-05, 4.52e-05, 5.14e-06, 1.73e-01, 1.73e-01]    [6.80e-05, 4.52e-05, 5.14e-06, 1.73e-01, 1.73e-01]    []  
6000      [2.21e-03, 5.65e-04, 7.31e-04, 5.08e+00, 5.08e+00]    [2.21e-03, 5.65e-04, 7.31e-04, 5.08e+00, 5.08e+00]    []  
40000     [2.04e-04, 9.29e-05, 1.46e-05, 1.84e-01, 1.84e-01]    [2.04e-04, 9.29e-05, 1.46e-05, 1.84e-01, 1.84e-01]    []  
7000      [4.05e-03, 3.57e-04, 8.96e-05, 2.47e+00, 2.47e+00]    [4.05e-03, 3.57e-04, 8.96e-05, 2.47e+00, 2.47e+00]    []  
50000     [1.78e-04, 8.96e-05, 4.37e-05, 2.24e-01, 2.24e-01]    [1.78e-04, 8.96e-05, 4.37e-05, 2.24e-01, 2.24e-01]    []  

Best model at step 46000:
  train loss: 6.92e-02
  test loss: 6.92e-02
  test metric: []

'train' took 25064.666581 s

[I 2023-10-10 04:02:55,840] Trial 63 finished with value: 0.6755882539504213 and parameters: {'num_domain': 95298, 'num_boundary': 3818, 'resampling_period': 15736, 'lr': 0.0009485078937254368}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000173 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.02e+01, 6.99e-05, 2.69e-07, 3.60e+03, 3.60e+03]    [4.02e+01, 6.99e-05, 2.69e-07, 3.60e+03, 3.60e+03]    []  
32000     [9.44e-05, 2.30e-05, 6.67e-07, 5.54e-02, 5.54e-02]    [9.44e-05, 2.30e-05, 6.67e-07, 5.54e-02, 5.54e-02]    []  
49000     [2.93e-05, 1.22e-05, 4.31e-06, 2.62e-02, 2.62e-02]    [2.93e-05, 1.22e-05, 4.31e-06, 2.62e-02, 2.62e-02]    []  
8000      [2.23e-03, 2.42e-04, 3.17e-04, 1.68e+00, 1.68e+00]    [2.23e-03, 2.42e-04, 3.17e-04, 1.68e+00, 1.68e+00]    []  
26000     [8.16e-05, 3.31e-05, 5.24e-06, 2.13e-01, 2.13e-01]    [8.16e-05, 3.31e-05, 5.24e-06, 2.13e-01, 2.13e-01]    []  
9000      [2.39e-03, 2.28e-04, 5.31e-04, 4.30e+00, 4.30e+00]    [2.39e-03, 2.28e-04, 5.31e-04, 4.30e+00, 4.30e+00]    []  
41000     [1.51e-04, 6.77e-05, 9.89e-06, 1.33e-01, 1.33e-01]    [1.51e-04, 6.77e-05, 9.89e-06, 1.33e-01, 1.33e-01]    []  
1000      [1.28e-09, 1.05e-03, 3.23e-15, 2.09e+03, 2.09e+03]    [1.28e-09, 1.05e-03, 3.23e-15, 2.09e+03, 2.09e+03]    []  
50000     [3.22e-05, 1.04e-05, 2.21e-06, 2.50e-02, 2.50e-02]    [3.22e-05, 1.04e-05, 2.21e-06, 2.50e-02, 2.50e-02]    []  

Best model at step 50000:
  train loss: 5.00e-02
  test loss: 5.00e-02
  test metric: []

'train' took 25663.705438 s

[I 2023-10-10 04:12:30,709] Trial 62 finished with value: 0.5162037283109533 and parameters: {'num_domain': 96961, 'num_boundary': 5637, 'resampling_period': 15455, 'lr': 0.0010588479710360607}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000150 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [2.29e+01, 1.12e-04, 2.76e-07, 3.60e+03, 3.60e+03]    [2.29e+01, 1.12e-04, 2.76e-07, 3.60e+03, 3.60e+03]    []  
10000     [3.18e-03, 3.24e-04, 5.51e-05, 1.43e+00, 1.43e+00]    [3.18e-03, 3.24e-04, 5.51e-05, 1.43e+00, 1.43e+00]    []  
33000     [9.24e-05, 2.07e-05, 4.38e-07, 5.86e-02, 5.86e-02]    [9.24e-05, 2.07e-05, 4.38e-07, 5.86e-02, 5.86e-02]    []  
27000     [6.29e-05, 2.84e-05, 3.40e-06, 1.24e-01, 1.24e-01]    [6.29e-05, 2.84e-05, 3.40e-06, 1.24e-01, 1.24e-01]    []  
11000     [2.56e-03, 2.36e-04, 7.59e-04, 9.82e-01, 9.82e-01]    [2.56e-03, 2.36e-04, 7.59e-04, 9.82e-01, 9.82e-01]    []  
2000      [1.78e-02, 2.92e-04, 1.73e-05, 3.05e+01, 3.05e+01]    [1.78e-02, 2.92e-04, 1.73e-05, 3.05e+01, 3.05e+01]    []  
12000     [1.97e-03, 2.03e-04, 5.87e-04, 1.01e+00, 1.01e+00]    [1.97e-03, 2.03e-04, 5.87e-04, 1.01e+00, 1.01e+00]    []  
42000     [1.31e-04, 6.51e-05, 9.62e-06, 5.24e-02, 5.24e-02]    [1.31e-04, 6.51e-05, 9.62e-06, 5.24e-02, 5.24e-02]    []  
1000      [3.08e-10, 1.05e-03, 6.48e-16, 2.09e+03, 2.09e+03]    [3.08e-10, 1.05e-03, 6.48e-16, 2.09e+03, 2.09e+03]    []  
28000     [5.11e-05, 1.71e-05, 5.77e-06, 1.12e-01, 1.12e-01]    [5.11e-05, 1.71e-05, 5.77e-06, 1.12e-01, 1.12e-01]    []  
34000     [9.22e-05, 2.15e-05, 3.49e-07, 7.98e-02, 7.98e-02]    [9.22e-05, 2.15e-05, 3.49e-07, 7.98e-02, 7.98e-02]    []  
13000     [1.25e-03, 2.20e-04, 3.53e-05, 8.08e-01, 8.08e-01]    [1.25e-03, 2.20e-04, 3.53e-05, 8.08e-01, 8.08e-01]    []  
14000     [8.07e-04, 2.29e-04, 4.73e-05, 1.41e+00, 1.41e+00]    [8.07e-04, 2.29e-04, 4.73e-05, 1.41e+00, 1.41e+00]    []  
3000      [1.24e-04, 6.64e-04, 5.14e-07, 9.68e+00, 9.68e+00]    [1.24e-04, 6.64e-04, 5.14e-07, 9.68e+00, 9.68e+00]    []  
29000     [3.78e-05, 1.27e-05, 2.66e-06, 1.23e-01, 1.23e-01]    [3.78e-05, 1.27e-05, 2.66e-06, 1.23e-01, 1.23e-01]    []  
2000      [1.57e-10, 1.05e-03, 1.78e-15, 2.09e+03, 2.09e+03]    [1.57e-10, 1.05e-03, 1.78e-15, 2.09e+03, 2.09e+03]    []  
43000     [1.38e-04, 6.38e-05, 1.68e-06, 6.20e-02, 6.20e-02]    [1.38e-04, 6.38e-05, 1.68e-06, 6.20e-02, 6.20e-02]    []  
15000     [1.05e-03, 2.04e-04, 3.53e-05, 5.42e-01, 5.42e-01]    [1.05e-03, 2.04e-04, 3.53e-05, 5.42e-01, 5.42e-01]    []  
35000     [8.85e-05, 2.04e-05, 5.36e-07, 4.58e-02, 4.58e-02]    [8.85e-05, 2.04e-05, 5.36e-07, 4.58e-02, 4.58e-02]    []  
16000     [8.25e-04, 2.09e-04, 3.50e-05, 1.51e+00, 1.51e+00]    [8.25e-04, 2.09e-04, 3.50e-05, 1.51e+00, 1.51e+00]    []  
30000     [3.67e-05, 1.02e-05, 3.26e-06, 9.73e-02, 9.73e-02]    [3.67e-05, 1.02e-05, 3.26e-06, 9.73e-02, 9.73e-02]    []  
4000      [4.99e-05, 1.41e-04, 5.72e-07, 7.68e+00, 7.68e+00]    [4.99e-05, 1.41e-04, 5.72e-07, 7.68e+00, 7.68e+00]    []  
17000     [1.08e-03, 2.20e-04, 2.08e-04, 5.12e-01, 5.12e-01]    [1.08e-03, 2.20e-04, 2.08e-04, 5.12e-01, 5.12e-01]    []  
3000      [9.26e-03, 4.98e-04, 9.85e-05, 1.69e+01, 1.69e+01]    [9.26e-03, 4.98e-04, 9.85e-05, 1.69e+01, 1.69e+01]    []  
44000     [1.76e-04, 6.03e-05, 4.20e-06, 3.17e-01, 3.17e-01]    [1.76e-04, 6.03e-05, 4.20e-06, 3.17e-01, 3.17e-01]    []  
18000     [8.32e-04, 2.44e-04, 1.14e-05, 9.10e-01, 9.10e-01]    [8.32e-04, 2.44e-04, 1.14e-05, 9.10e-01, 9.10e-01]    []  
31000     [8.38e-05, 8.45e-06, 4.01e-06, 1.96e-01, 1.96e-01]    [8.38e-05, 8.45e-06, 4.01e-06, 1.96e-01, 1.96e-01]    []  
36000     [9.56e-05, 1.99e-05, 1.99e-07, 4.30e-02, 4.30e-02]    [9.56e-05, 1.99e-05, 1.99e-07, 4.30e-02, 4.30e-02]    []  
19000     [8.62e-04, 2.47e-04, 3.62e-05, 3.29e-01, 3.29e-01]    [8.62e-04, 2.47e-04, 3.62e-05, 3.29e-01, 3.29e-01]    []  
5000      [7.19e-04, 1.03e-04, 3.15e-06, 4.89e+00, 4.89e+00]    [7.19e-04, 1.03e-04, 3.15e-06, 4.89e+00, 4.89e+00]    []  
4000      [1.33e-03, 7.33e-04, 5.93e-05, 8.70e+00, 8.70e+00]    [1.33e-03, 7.33e-04, 5.93e-05, 8.70e+00, 8.70e+00]    []  
20000     [5.96e-03, 2.19e-04, 3.58e-04, 4.21e-01, 4.21e-01]    [5.96e-03, 2.19e-04, 3.58e-04, 4.21e-01, 4.21e-01]    []  
45000     [1.42e-04, 5.89e-05, 4.23e-06, 5.74e-02, 5.74e-02]    [1.42e-04, 5.89e-05, 4.23e-06, 5.74e-02, 5.74e-02]    []  
32000     [5.64e-05, 9.33e-06, 2.74e-06, 8.45e-02, 8.45e-02]    [5.64e-05, 9.33e-06, 2.74e-06, 8.45e-02, 8.45e-02]    []  
21000     [7.55e-04, 1.94e-04, 2.80e-04, 7.68e-01, 7.68e-01]    [7.55e-04, 1.94e-04, 2.80e-04, 7.68e-01, 7.68e-01]    []  
6000      [8.90e-04, 1.76e-04, 7.08e-06, 3.27e+00, 3.27e+00]    [8.90e-04, 1.76e-04, 7.08e-06, 3.27e+00, 3.27e+00]    []  
37000     [8.31e-05, 1.48e-05, 4.05e-07, 5.47e-02, 5.47e-02]    [8.31e-05, 1.48e-05, 4.05e-07, 5.47e-02, 5.47e-02]    []  
5000      [1.11e-03, 5.42e-04, 1.82e-05, 5.29e+00, 5.29e+00]    [1.11e-03, 5.42e-04, 1.82e-05, 5.29e+00, 5.29e+00]    []  
22000     [9.88e-04, 1.90e-04, 1.84e-05, 4.94e-01, 4.94e-01]    [9.88e-04, 1.90e-04, 1.84e-05, 4.94e-01, 4.94e-01]    []  
46000     [1.27e-04, 5.52e-05, 3.66e-06, 3.95e-02, 3.95e-02]    [1.27e-04, 5.52e-05, 3.66e-06, 3.95e-02, 3.95e-02]    []  
33000     [6.72e-05, 9.69e-06, 4.66e-06, 2.17e-01, 2.17e-01]    [6.72e-05, 9.69e-06, 4.66e-06, 2.17e-01, 2.17e-01]    []  
23000     [6.51e-04, 1.75e-04, 1.19e-04, 2.05e-01, 2.05e-01]    [6.51e-04, 1.75e-04, 1.19e-04, 2.05e-01, 2.05e-01]    []  
7000      [1.18e-03, 8.29e-05, 5.99e-06, 2.17e+00, 2.17e+00]    [1.18e-03, 8.29e-05, 5.99e-06, 2.17e+00, 2.17e+00]    []  
24000     [3.74e-03, 1.89e-04, 2.55e-05, 6.85e-01, 6.85e-01]    [3.74e-03, 1.89e-04, 2.55e-05, 6.85e-01, 6.85e-01]    []  
6000      [1.25e-03, 8.63e-04, 1.80e-05, 4.07e+00, 4.07e+00]    [1.25e-03, 8.63e-04, 1.80e-05, 4.07e+00, 4.07e+00]    []  
38000     [7.63e-05, 1.21e-05, 3.39e-07, 3.86e-02, 3.86e-02]    [7.63e-05, 1.21e-05, 3.39e-07, 3.86e-02, 3.86e-02]    []  
25000     [7.68e-04, 1.61e-04, 6.00e-05, 2.07e+00, 2.07e+00]    [7.68e-04, 1.61e-04, 6.00e-05, 2.07e+00, 2.07e+00]    []  
34000     [4.17e-05, 9.07e-06, 5.15e-06, 7.77e-02, 7.77e-02]    [4.17e-05, 9.07e-06, 5.15e-06, 7.77e-02, 7.77e-02]    []  
47000     [1.28e-04, 5.52e-05, 3.57e-06, 3.92e-02, 3.92e-02]    [1.28e-04, 5.52e-05, 3.57e-06, 3.92e-02, 3.92e-02]    []  
26000     [5.31e-04, 1.68e-04, 2.05e-05, 1.54e-01, 1.54e-01]    [5.31e-04, 1.68e-04, 2.05e-05, 1.54e-01, 1.54e-01]    []  
8000      [7.58e-04, 3.00e-04, 4.56e-06, 1.63e+00, 1.63e+00]    [7.58e-04, 3.00e-04, 4.56e-06, 1.63e+00, 1.63e+00]    []  
7000      [9.37e-04, 1.11e-03, 1.03e-05, 2.76e+00, 2.76e+00]    [9.37e-04, 1.11e-03, 1.03e-05, 2.76e+00, 2.76e+00]    []  
27000     [7.96e-03, 1.71e-04, 9.59e-05, 3.56e-01, 3.56e-01]    [7.96e-03, 1.71e-04, 9.59e-05, 3.56e-01, 3.56e-01]    []  
35000     [4.25e-05, 4.06e-06, 5.96e-06, 2.51e-01, 2.51e-01]    [4.25e-05, 4.06e-06, 5.96e-06, 2.51e-01, 2.51e-01]    []  
39000     [7.11e-05, 1.09e-05, 8.83e-07, 3.78e-02, 3.78e-02]    [7.11e-05, 1.09e-05, 8.83e-07, 3.78e-02, 3.78e-02]    []  
28000     [1.07e-03, 1.81e-04, 6.60e-05, 5.39e-01, 5.39e-01]    [1.07e-03, 1.81e-04, 6.60e-05, 5.39e-01, 5.39e-01]    []  
48000     [1.64e-04, 5.34e-05, 6.69e-05, 1.14e+00, 1.14e+00]    [1.64e-04, 5.34e-05, 6.69e-05, 1.14e+00, 1.14e+00]    []  
9000      [6.32e-04, 4.04e-04, 2.05e-06, 1.91e+00, 1.91e+00]    [6.32e-04, 4.04e-04, 2.05e-06, 1.91e+00, 1.91e+00]    []  
8000      [1.51e-03, 9.28e-04, 4.60e-05, 2.18e+00, 2.18e+00]    [1.51e-03, 9.28e-04, 4.60e-05, 2.18e+00, 2.18e+00]    []  
29000     [6.50e-04, 1.73e-04, 3.60e-05, 1.99e-01, 1.99e-01]    [6.50e-04, 1.73e-04, 3.60e-05, 1.99e-01, 1.99e-01]    []  
36000     [5.10e-05, 4.06e-06, 2.18e-06, 8.10e-02, 8.10e-02]    [5.10e-05, 4.06e-06, 2.18e-06, 8.10e-02, 8.10e-02]    []  
30000     [4.71e-04, 1.57e-04, 6.22e-05, 1.66e-01, 1.66e-01]    [4.71e-04, 1.57e-04, 6.22e-05, 1.66e-01, 1.66e-01]    []  
40000     [7.10e-05, 1.12e-05, 4.12e-07, 3.46e-02, 3.46e-02]    [7.10e-05, 1.12e-05, 4.12e-07, 3.46e-02, 3.46e-02]    []  
49000     [1.47e-04, 5.13e-05, 5.29e-07, 4.59e-02, 4.59e-02]    [1.47e-04, 5.13e-05, 5.29e-07, 4.59e-02, 4.59e-02]    []  
10000     [6.37e-04, 4.88e-04, 1.17e-06, 9.59e-01, 9.59e-01]    [6.37e-04, 4.88e-04, 1.17e-06, 9.59e-01, 9.59e-01]    []  
31000     [2.17e-03, 1.50e-04, 9.71e-06, 5.32e-01, 5.32e-01]    [2.17e-03, 1.50e-04, 9.71e-06, 5.32e-01, 5.32e-01]    []  
9000      [1.34e-03, 1.06e-03, 3.12e-05, 1.55e+00, 1.55e+00]    [1.34e-03, 1.06e-03, 3.12e-05, 1.55e+00, 1.55e+00]    []  
37000     [4.09e-05, 3.22e-06, 1.62e-06, 1.77e-01, 1.77e-01]    [4.09e-05, 3.22e-06, 1.62e-06, 1.77e-01, 1.77e-01]    []  
32000     [5.29e-04, 1.54e-04, 4.15e-06, 1.45e-01, 1.45e-01]    [5.29e-04, 1.54e-04, 4.15e-06, 1.45e-01, 1.45e-01]    []  
33000     [3.72e-04, 1.57e-04, 1.79e-05, 1.85e-01, 1.85e-01]    [3.72e-04, 1.57e-04, 1.79e-05, 1.85e-01, 1.85e-01]    []  
41000     [7.40e-05, 1.17e-05, 8.19e-07, 3.43e-02, 3.43e-02]    [7.40e-05, 1.17e-05, 8.19e-07, 3.43e-02, 3.43e-02]    []  
11000     [6.97e-04, 4.26e-04, 1.67e-06, 1.89e+00, 1.89e+00]    [6.97e-04, 4.26e-04, 1.67e-06, 1.89e+00, 1.89e+00]    []  
50000     [1.43e-04, 4.75e-05, 1.65e-05, 3.27e-01, 3.27e-01]    [1.43e-04, 4.75e-05, 1.65e-05, 3.27e-01, 3.27e-01]    []  

Best model at step 47000:
  train loss: 7.86e-02
  test loss: 7.86e-02
  test metric: []

'train' took 27835.432874 s

[I 2023-10-10 05:35:46,312] Trial 64 finished with value: 0.7837283381916321 and parameters: {'num_domain': 96706, 'num_boundary': 3812, 'resampling_period': 14688, 'lr': 0.0009827221719584972}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000182 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.18e+01, 1.44e-04, 7.93e-08, 3.62e+03, 3.62e+03]    [1.18e+01, 1.44e-04, 7.93e-08, 3.62e+03, 3.62e+03]    []  
38000     [3.59e-05, 3.51e-06, 1.71e-06, 5.69e-02, 5.69e-02]    [3.59e-05, 3.51e-06, 1.71e-06, 5.69e-02, 5.69e-02]    []  
10000     [7.68e-04, 3.16e-04, 3.26e-05, 1.28e+00, 1.28e+00]    [7.68e-04, 3.16e-04, 3.26e-05, 1.28e+00, 1.28e+00]    []  
34000     [3.73e-04, 1.52e-04, 2.20e-05, 2.30e-01, 2.30e-01]    [3.73e-04, 1.52e-04, 2.20e-05, 2.30e-01, 2.30e-01]    []  
35000     [7.84e-04, 1.54e-04, 8.80e-06, 8.65e-02, 8.65e-02]    [7.84e-04, 1.54e-04, 8.80e-06, 8.65e-02, 8.65e-02]    []  
12000     [5.89e-04, 3.79e-04, 1.93e-06, 5.75e-01, 5.75e-01]    [5.89e-04, 3.79e-04, 1.93e-06, 5.75e-01, 5.75e-01]    []  
39000     [3.69e-05, 3.30e-06, 1.55e-06, 1.37e-01, 1.37e-01]    [3.69e-05, 3.30e-06, 1.55e-06, 1.37e-01, 1.37e-01]    []  
42000     [8.02e-05, 1.39e-05, 9.66e-07, 3.10e-02, 3.10e-02]    [8.02e-05, 1.39e-05, 9.66e-07, 3.10e-02, 3.10e-02]    []  
36000     [6.03e-04, 1.60e-04, 2.36e-05, 3.45e-01, 3.45e-01]    [6.03e-04, 1.60e-04, 2.36e-05, 3.45e-01, 3.45e-01]    []  
11000     [4.62e-04, 3.95e-04, 5.01e-05, 1.09e+00, 1.09e+00]    [4.62e-04, 3.95e-04, 5.01e-05, 1.09e+00, 1.09e+00]    []  
1000      [2.48e-03, 2.11e-04, 2.58e-06, 3.57e+01, 3.57e+01]    [2.48e-03, 2.11e-04, 2.58e-06, 3.57e+01, 3.57e+01]    []  
37000     [4.45e-04, 1.49e-04, 9.85e-06, 1.13e-01, 1.13e-01]    [4.45e-04, 1.49e-04, 9.85e-06, 1.13e-01, 1.13e-01]    []  
40000     [4.29e-05, 2.97e-06, 4.31e-06, 7.59e-02, 7.59e-02]    [4.29e-05, 2.97e-06, 4.31e-06, 7.59e-02, 7.59e-02]    []  
13000     [5.61e-04, 3.02e-04, 1.25e-06, 9.87e-01, 9.87e-01]    [5.61e-04, 3.02e-04, 1.25e-06, 9.87e-01, 9.87e-01]    []  
38000     [4.38e-04, 1.55e-04, 1.58e-06, 7.77e-02, 7.77e-02]    [4.38e-04, 1.55e-04, 1.58e-06, 7.77e-02, 7.77e-02]    []  
12000     [3.46e-04, 2.38e-04, 6.29e-05, 1.38e+00, 1.38e+00]    [3.46e-04, 2.38e-04, 6.29e-05, 1.38e+00, 1.38e+00]    []  
2000      [8.15e-04, 3.57e-04, 1.12e-05, 5.14e+00, 5.14e+00]    [8.15e-04, 3.57e-04, 1.12e-05, 5.14e+00, 5.14e+00]    []  
43000     [1.12e-04, 2.46e-05, 1.06e-06, 2.10e-01, 2.10e-01]    [1.12e-04, 2.46e-05, 1.06e-06, 2.10e-01, 2.10e-01]    []  
39000     [9.38e-04, 1.39e-04, 2.47e-06, 8.01e-02, 8.01e-02]    [9.38e-04, 1.39e-04, 2.47e-06, 8.01e-02, 8.01e-02]    []  
41000     [2.91e-05, 3.28e-06, 2.13e-06, 8.66e-02, 8.66e-02]    [2.91e-05, 3.28e-06, 2.13e-06, 8.66e-02, 8.66e-02]    []  
40000     [3.69e-04, 1.58e-04, 4.86e-06, 6.19e-02, 6.19e-02]    [3.69e-04, 1.58e-04, 4.86e-06, 6.19e-02, 6.19e-02]    []  
14000     [4.83e-04, 2.07e-04, 3.22e-07, 1.05e+00, 1.05e+00]    [4.83e-04, 2.07e-04, 3.22e-07, 1.05e+00, 1.05e+00]    []  
13000     [5.01e-04, 1.22e-04, 3.83e-05, 5.78e-01, 5.78e-01]    [5.01e-04, 1.22e-04, 3.83e-05, 5.78e-01, 5.78e-01]    []  
41000     [3.78e-04, 1.45e-04, 2.55e-05, 5.82e-02, 5.82e-02]    [3.78e-04, 1.45e-04, 2.55e-05, 5.82e-02, 5.82e-02]    []  
3000      [3.32e-04, 9.61e-05, 4.93e-06, 5.90e+00, 5.90e+00]    [3.32e-04, 9.61e-05, 4.93e-06, 5.90e+00, 5.90e+00]    []  
44000     [8.39e-05, 1.52e-05, 3.75e-06, 3.55e-02, 3.55e-02]    [8.39e-05, 1.52e-05, 3.75e-06, 3.55e-02, 3.55e-02]    []  
42000     [4.13e-04, 1.26e-04, 5.11e-06, 2.11e-01, 2.11e-01]    [4.13e-04, 1.26e-04, 5.11e-06, 2.11e-01, 2.11e-01]    []  
42000     [3.21e-05, 3.44e-06, 6.00e-06, 8.53e-02, 8.53e-02]    [3.21e-05, 3.44e-06, 6.00e-06, 8.53e-02, 8.53e-02]    []  
15000     [4.60e-04, 1.38e-04, 5.91e-07, 2.45e+00, 2.45e+00]    [4.60e-04, 1.38e-04, 5.91e-07, 2.45e+00, 2.45e+00]    []  
43000     [3.80e-04, 1.48e-04, 7.21e-05, 1.68e-01, 1.68e-01]    [3.80e-04, 1.48e-04, 7.21e-05, 1.68e-01, 1.68e-01]    []  
14000     [3.74e-04, 8.05e-05, 3.60e-05, 1.23e+00, 1.23e+00]    [3.74e-04, 8.05e-05, 3.60e-05, 1.23e+00, 1.23e+00]    []  
4000      [4.47e-04, 5.05e-05, 6.50e-06, 4.52e+00, 4.52e+00]    [4.47e-04, 5.05e-05, 6.50e-06, 4.52e+00, 4.52e+00]    []  
44000     [3.34e-04, 1.31e-04, 2.11e-04, 6.34e-02, 6.34e-02]    [3.34e-04, 1.31e-04, 2.11e-04, 6.34e-02, 6.34e-02]    []  
43000     [2.71e-05, 3.53e-06, 5.38e-06, 1.84e-01, 1.84e-01]    [2.71e-05, 3.53e-06, 5.38e-06, 1.84e-01, 1.84e-01]    []  
45000     [8.35e-05, 1.43e-05, 5.30e-07, 2.99e-02, 2.99e-02]    [8.35e-05, 1.43e-05, 5.30e-07, 2.99e-02, 2.99e-02]    []  
45000     [3.76e-04, 1.36e-04, 2.06e-04, 3.42e-01, 3.42e-01]    [3.76e-04, 1.36e-04, 2.06e-04, 3.42e-01, 3.42e-01]    []  
16000     [4.08e-04, 1.20e-04, 4.90e-07, 7.71e-01, 7.71e-01]    [4.08e-04, 1.20e-04, 4.90e-07, 7.71e-01, 7.71e-01]    []  
15000     [3.90e-04, 6.72e-05, 7.16e-05, 2.41e+00, 2.41e+00]    [3.90e-04, 6.72e-05, 7.16e-05, 2.41e+00, 2.41e+00]    []  
46000     [4.62e-04, 1.34e-04, 4.06e-06, 5.01e-02, 5.01e-02]    [4.62e-04, 1.34e-04, 4.06e-06, 5.01e-02, 5.01e-02]    []  
44000     [2.39e-05, 3.46e-06, 2.18e-06, 9.49e-02, 9.49e-02]    [2.39e-05, 3.46e-06, 2.18e-06, 9.49e-02, 9.49e-02]    []  
5000      [2.82e-04, 6.65e-05, 1.54e-06, 1.24e+00, 1.24e+00]    [2.82e-04, 6.65e-05, 1.54e-06, 1.24e+00, 1.24e+00]    []  
47000     [2.64e-04, 1.31e-04, 1.13e-05, 7.04e-02, 7.04e-02]    [2.64e-04, 1.31e-04, 1.13e-05, 7.04e-02, 7.04e-02]    []  
46000     [7.94e-05, 1.32e-05, 7.34e-07, 3.11e-02, 3.11e-02]    [7.94e-05, 1.32e-05, 7.34e-07, 3.11e-02, 3.11e-02]    []  
17000     [3.66e-04, 1.03e-04, 2.64e-07, 1.51e+00, 1.51e+00]    [3.66e-04, 1.03e-04, 2.64e-07, 1.51e+00, 1.51e+00]    []  
16000     [3.02e-04, 5.84e-05, 3.17e-05, 1.01e+00, 1.01e+00]    [3.02e-04, 5.84e-05, 3.17e-05, 1.01e+00, 1.01e+00]    []  
48000     [4.10e-04, 1.19e-04, 2.41e-04, 4.89e-02, 4.89e-02]    [4.10e-04, 1.19e-04, 2.41e-04, 4.89e-02, 4.89e-02]    []  
45000     [2.86e-05, 3.79e-06, 2.89e-06, 6.37e-02, 6.37e-02]    [2.86e-05, 3.79e-06, 2.89e-06, 6.37e-02, 6.37e-02]    []  
49000     [4.99e-04, 1.49e-04, 8.55e-06, 7.52e-02, 7.52e-02]    [4.99e-04, 1.49e-04, 8.55e-06, 7.52e-02, 7.52e-02]    []  
6000      [3.49e-04, 5.78e-05, 4.46e-06, 1.07e+00, 1.07e+00]    [3.49e-04, 5.78e-05, 4.46e-06, 1.07e+00, 1.07e+00]    []  
18000     [2.82e-04, 6.22e-05, 8.30e-08, 4.66e-01, 4.66e-01]    [2.82e-04, 6.22e-05, 8.30e-08, 4.66e-01, 4.66e-01]    []  
50000     [4.05e-04, 1.50e-04, 7.74e-06, 4.12e-02, 4.12e-02]    [4.05e-04, 1.50e-04, 7.74e-06, 4.12e-02, 4.12e-02]    []  

Best model at step 50000:
  train loss: 8.30e-02
  test loss: 8.30e-02
  test metric: []

'train' took 10612.390219 s

[I 2023-10-10 06:34:44,301] Trial 68 finished with value: 0.5309671359213213 and parameters: {'num_domain': 44933, 'num_boundary': 4695, 'resampling_period': 29964, 'lr': 0.0009104081358776624}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000130 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.93e+01, 1.72e-04, 7.37e-07, 3.60e+03, 3.60e+03]    [3.93e+01, 1.72e-04, 7.37e-07, 3.60e+03, 3.60e+03]    []  
47000     [2.63e-04, 6.15e-05, 2.25e-06, 7.04e-01, 7.04e-01]    [2.63e-04, 6.15e-05, 2.25e-06, 7.04e-01, 7.04e-01]    []  
17000     [2.76e-04, 4.68e-05, 1.58e-05, 4.19e-01, 4.19e-01]    [2.76e-04, 4.68e-05, 1.58e-05, 4.19e-01, 4.19e-01]    []  
46000     [3.19e-05, 3.80e-06, 1.68e-06, 5.28e-02, 5.28e-02]    [3.19e-05, 3.80e-06, 1.68e-06, 5.28e-02, 5.28e-02]    []  
1000      [1.67e-03, 8.33e-04, 1.64e-04, 3.57e+01, 3.57e+01]    [1.67e-03, 8.33e-04, 1.64e-04, 3.57e+01, 3.57e+01]    []  
7000      [2.85e-04, 5.85e-05, 2.34e-06, 1.44e+00, 1.44e+00]    [2.85e-04, 5.85e-05, 2.34e-06, 1.44e+00, 1.44e+00]    []  
19000     [2.18e-04, 5.66e-05, 2.54e-07, 4.69e-01, 4.69e-01]    [2.18e-04, 5.66e-05, 2.54e-07, 4.69e-01, 4.69e-01]    []  
47000     [2.12e-05, 3.86e-06, 1.40e-06, 3.60e-02, 3.60e-02]    [2.12e-05, 3.86e-06, 1.40e-06, 3.60e-02, 3.60e-02]    []  
18000     [1.85e-04, 3.81e-05, 2.45e-05, 2.89e-01, 2.89e-01]    [1.85e-04, 3.81e-05, 2.45e-05, 2.89e-01, 2.89e-01]    []  
48000     [1.04e-04, 1.70e-05, 1.36e-06, 3.39e-02, 3.39e-02]    [1.04e-04, 1.70e-05, 1.36e-06, 3.39e-02, 3.39e-02]    []  
2000      [6.29e-04, 1.26e-03, 8.12e-06, 5.33e+00, 5.33e+00]    [6.29e-04, 1.26e-03, 8.12e-06, 5.33e+00, 5.33e+00]    []  
8000      [2.89e-04, 7.15e-05, 7.35e-06, 8.68e-01, 8.68e-01]    [2.89e-04, 7.15e-05, 7.35e-06, 8.68e-01, 8.68e-01]    []  
20000     [2.04e-04, 4.78e-05, 7.96e-08, 2.49e-01, 2.49e-01]    [2.04e-04, 4.78e-05, 7.96e-08, 2.49e-01, 2.49e-01]    []  
48000     [4.46e-05, 3.94e-06, 1.80e-06, 3.75e-02, 3.75e-02]    [4.46e-05, 3.94e-06, 1.80e-06, 3.75e-02, 3.75e-02]    []  
3000      [9.54e-04, 6.54e-04, 1.14e-05, 2.15e+00, 2.15e+00]    [9.54e-04, 6.54e-04, 1.14e-05, 2.15e+00, 2.15e+00]    []  
19000     [1.40e-04, 2.99e-05, 2.10e-05, 6.24e-01, 6.24e-01]    [1.40e-04, 2.99e-05, 2.10e-05, 6.24e-01, 6.24e-01]    []  
49000     [7.12e-05, 9.85e-06, 5.86e-07, 6.76e-02, 6.76e-02]    [7.12e-05, 9.85e-06, 5.86e-07, 6.76e-02, 6.76e-02]    []  
4000      [8.29e-04, 6.77e-04, 2.69e-06, 2.45e+00, 2.45e+00]    [8.29e-04, 6.77e-04, 2.69e-06, 2.45e+00, 2.45e+00]    []  
49000     [4.37e-05, 5.11e-06, 2.44e-06, 9.02e-02, 9.02e-02]    [4.37e-05, 5.11e-06, 2.44e-06, 9.02e-02, 9.02e-02]    []  
21000     [2.39e-04, 6.07e-05, 3.69e-07, 5.13e-01, 5.13e-01]    [2.39e-04, 6.07e-05, 3.69e-07, 5.13e-01, 5.13e-01]    []  
9000      [2.87e-04, 9.87e-05, 3.84e-06, 2.61e-01, 2.61e-01]    [2.87e-04, 9.87e-05, 3.84e-06, 2.61e-01, 2.61e-01]    []  
20000     [1.52e-04, 2.80e-05, 8.66e-06, 3.43e-01, 3.43e-01]    [1.52e-04, 2.80e-05, 8.66e-06, 3.43e-01, 3.43e-01]    []  
5000      [9.04e-04, 6.56e-04, 5.68e-06, 2.17e+00, 2.17e+00]    [9.04e-04, 6.56e-04, 5.68e-06, 2.17e+00, 2.17e+00]    []  
50000     [5.18e-05, 4.33e-06, 2.46e-06, 6.00e-02, 6.00e-02]    [5.18e-05, 4.33e-06, 2.46e-06, 6.00e-02, 6.00e-02]    []  

Best model at step 47000:
  train loss: 7.21e-02
  test loss: 7.21e-02
  test metric: []

'train' took 22242.888104 s

[I 2023-10-10 07:05:40,283] Trial 67 finished with value: 0.5342625090037505 and parameters: {'num_domain': 92569, 'num_boundary': 5981, 'resampling_period': 26975, 'lr': 0.0008701499316365489}. Best is trial 0 with value: 0.4913720930752669.
50000     [6.50e-05, 9.05e-06, 1.06e-06, 2.31e-02, 2.31e-02]    [6.50e-05, 9.05e-06, 1.06e-06, 2.31e-02, 2.31e-02]    []  

Best model at step 50000:
  train loss: 4.62e-02
  test loss: 4.62e-02
  test metric: []

'train' took 30135.552851 s

[I 2023-10-10 07:05:42,857] Trial 65 finished with value: 0.49600698130016113 and parameters: {'num_domain': 96399, 'num_boundary': 3614, 'resampling_period': 16419, 'lr': 0.0010754373653956743}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000470 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [6.85e+01, 2.15e-04, 2.23e-07, 3.61e+03, 3.61e+03]    [6.85e+01, 2.15e-04, 2.23e-07, 3.61e+03, 3.61e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000135 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [6.53e+01, 2.71e-04, 2.50e-07, 3.60e+03, 3.60e+03]    [6.53e+01, 2.71e-04, 2.50e-07, 3.60e+03, 3.60e+03]    []  
22000     [1.52e-04, 3.87e-05, 2.22e-08, 2.75e-01, 2.75e-01]    [1.52e-04, 3.87e-05, 2.22e-08, 2.75e-01, 2.75e-01]    []  
10000     [3.70e-04, 9.54e-05, 9.00e-06, 3.36e-01, 3.36e-01]    [3.70e-04, 9.54e-05, 9.00e-06, 3.36e-01, 3.36e-01]    []  
21000     [1.16e-04, 2.34e-05, 1.25e-05, 6.89e-01, 6.89e-01]    [1.16e-04, 2.34e-05, 1.25e-05, 6.89e-01, 6.89e-01]    []  
6000      [8.77e-04, 9.60e-04, 9.28e-06, 1.43e+00, 1.43e+00]    [8.77e-04, 9.60e-04, 9.28e-06, 1.43e+00, 1.43e+00]    []  
1000      [5.67e-02, 8.76e-04, 1.46e-04, 7.27e+02, 7.27e+02]    [5.67e-02, 8.76e-04, 1.46e-04, 7.27e+02, 7.27e+02]    []  
1000      [2.82e-02, 2.88e-04, 1.70e-02, 1.15e+02, 1.15e+02]    [2.82e-02, 2.88e-04, 1.70e-02, 1.15e+02, 1.15e+02]    []  
2000      [5.09e-04, 1.84e-05, 1.49e-05, 1.04e+01, 1.04e+01]    [5.09e-04, 1.84e-05, 1.49e-05, 1.04e+01, 1.04e+01]    []  
7000      [6.92e-04, 8.30e-04, 7.16e-06, 1.56e+00, 1.56e+00]    [6.92e-04, 8.30e-04, 7.16e-06, 1.56e+00, 1.56e+00]    []  
23000     [1.25e-04, 3.79e-05, 4.52e-08, 1.78e-01, 1.78e-01]    [1.25e-04, 3.79e-05, 4.52e-08, 1.78e-01, 1.78e-01]    []  
22000     [1.38e-04, 2.14e-05, 5.04e-05, 2.65e-01, 2.65e-01]    [1.38e-04, 2.14e-05, 5.04e-05, 2.65e-01, 2.65e-01]    []  
11000     [3.93e-04, 9.59e-05, 1.49e-05, 3.17e-01, 3.17e-01]    [3.93e-04, 9.59e-05, 1.49e-05, 3.17e-01, 3.17e-01]    []  
3000      [1.13e-04, 1.98e-05, 2.10e-08, 5.00e+00, 5.00e+00]    [1.13e-04, 1.98e-05, 2.10e-08, 5.00e+00, 5.00e+00]    []  
8000      [6.21e-04, 7.47e-04, 2.05e-06, 4.35e-01, 4.35e-01]    [6.21e-04, 7.47e-04, 2.05e-06, 4.35e-01, 4.35e-01]    []  
2000      [6.90e-03, 3.17e-04, 2.41e-04, 9.90e+00, 9.90e+00]    [6.90e-03, 3.17e-04, 2.41e-04, 9.90e+00, 9.90e+00]    []  
24000     [1.22e-04, 4.76e-05, 1.41e-07, 3.77e-01, 3.77e-01]    [1.22e-04, 4.76e-05, 1.41e-07, 3.77e-01, 3.77e-01]    []  
4000      [1.55e-04, 1.37e-05, 8.05e-08, 6.14e+00, 6.14e+00]    [1.55e-04, 1.37e-05, 8.05e-08, 6.14e+00, 6.14e+00]    []  
23000     [9.59e-05, 2.07e-05, 3.73e-06, 2.35e-01, 2.35e-01]    [9.59e-05, 2.07e-05, 3.73e-06, 2.35e-01, 2.35e-01]    []  
12000     [2.62e-04, 9.96e-05, 3.24e-06, 3.35e+00, 3.35e+00]    [2.62e-04, 9.96e-05, 3.24e-06, 3.35e+00, 3.35e+00]    []  
9000      [5.67e-04, 6.91e-04, 2.52e-06, 3.45e-01, 3.45e-01]    [5.67e-04, 6.91e-04, 2.52e-06, 3.45e-01, 3.45e-01]    []  
5000      [1.23e-04, 8.17e-06, 2.38e-08, 3.64e+00, 3.64e+00]    [1.23e-04, 8.17e-06, 2.38e-08, 3.64e+00, 3.64e+00]    []  
3000      [4.38e-03, 2.09e-04, 1.45e-04, 4.99e+00, 4.99e+00]    [4.38e-03, 2.09e-04, 1.45e-04, 4.99e+00, 4.99e+00]    []  
25000     [7.65e-05, 2.59e-05, 4.85e-08, 1.65e-01, 1.65e-01]    [7.65e-05, 2.59e-05, 4.85e-08, 1.65e-01, 1.65e-01]    []  
10000     [5.92e-04, 5.01e-04, 1.89e-06, 4.22e-01, 4.22e-01]    [5.92e-04, 5.01e-04, 1.89e-06, 4.22e-01, 4.22e-01]    []  
24000     [9.54e-05, 2.00e-05, 1.11e-05, 3.91e-01, 3.91e-01]    [9.54e-05, 2.00e-05, 1.11e-05, 3.91e-01, 3.91e-01]    []  
6000      [9.89e-05, 1.01e-05, 1.94e-07, 1.46e+00, 1.46e+00]    [9.89e-05, 1.01e-05, 1.94e-07, 1.46e+00, 1.46e+00]    []  
13000     [1.75e-04, 7.02e-05, 6.44e-07, 7.53e-01, 7.53e-01]    [1.75e-04, 7.02e-05, 6.44e-07, 7.53e-01, 7.53e-01]    []  
11000     [5.00e-04, 3.35e-04, 3.89e-06, 6.76e-01, 6.76e-01]    [5.00e-04, 3.35e-04, 3.89e-06, 6.76e-01, 6.76e-01]    []  
4000      [3.03e-03, 4.54e-04, 6.17e-04, 2.59e+00, 2.59e+00]    [3.03e-03, 4.54e-04, 6.17e-04, 2.59e+00, 2.59e+00]    []  
7000      [1.16e-04, 2.19e-05, 5.09e-07, 1.58e+00, 1.58e+00]    [1.16e-04, 2.19e-05, 5.09e-07, 1.58e+00, 1.58e+00]    []  
26000     [7.47e-05, 2.46e-05, 4.76e-08, 3.97e-01, 3.97e-01]    [7.47e-05, 2.46e-05, 4.76e-08, 3.97e-01, 3.97e-01]    []  
25000     [9.48e-05, 1.91e-05, 8.08e-06, 1.78e-01, 1.78e-01]    [9.48e-05, 1.91e-05, 8.08e-06, 1.78e-01, 1.78e-01]    []  
8000      [1.82e-04, 4.91e-05, 1.80e-06, 2.18e+00, 2.18e+00]    [1.82e-04, 4.91e-05, 1.80e-06, 2.18e+00, 2.18e+00]    []  
12000     [4.67e-04, 2.21e-04, 3.12e-06, 3.52e-01, 3.52e-01]    [4.67e-04, 2.21e-04, 3.12e-06, 3.52e-01, 3.52e-01]    []  
14000     [8.40e-05, 2.34e-05, 2.87e-06, 1.53e-01, 1.53e-01]    [8.40e-05, 2.34e-05, 2.87e-06, 1.53e-01, 1.53e-01]    []  
5000      [1.95e-03, 7.58e-04, 1.55e-04, 1.46e+00, 1.46e+00]    [1.95e-03, 7.58e-04, 1.55e-04, 1.46e+00, 1.46e+00]    []  
9000      [8.11e-05, 1.36e-05, 2.45e-07, 2.04e+00, 2.04e+00]    [8.11e-05, 1.36e-05, 2.45e-07, 2.04e+00, 2.04e+00]    []  
27000     [6.82e-05, 2.06e-05, 3.42e-07, 1.21e-01, 1.21e-01]    [6.82e-05, 2.06e-05, 3.42e-07, 1.21e-01, 1.21e-01]    []  
13000     [3.62e-04, 2.02e-04, 8.82e-06, 2.77e-01, 2.77e-01]    [3.62e-04, 2.02e-04, 8.82e-06, 2.77e-01, 2.77e-01]    []  
26000     [7.53e-05, 1.93e-05, 4.88e-06, 1.50e-01, 1.50e-01]    [7.53e-05, 1.93e-05, 4.88e-06, 1.50e-01, 1.50e-01]    []  
10000     [5.75e-05, 9.34e-06, 2.86e-07, 6.10e-01, 6.10e-01]    [5.75e-05, 9.34e-06, 2.86e-07, 6.10e-01, 6.10e-01]    []  
15000     [4.94e-05, 2.02e-05, 2.95e-06, 1.75e-01, 1.75e-01]    [4.94e-05, 2.02e-05, 2.95e-06, 1.75e-01, 1.75e-01]    []  
6000      [3.26e-03, 7.15e-04, 4.43e-05, 2.68e+00, 2.68e+00]    [3.26e-03, 7.15e-04, 4.43e-05, 2.68e+00, 2.68e+00]    []  
14000     [3.45e-04, 1.67e-04, 3.95e-06, 4.08e-01, 4.08e-01]    [3.45e-04, 1.67e-04, 3.95e-06, 4.08e-01, 4.08e-01]    []  
28000     [5.64e-05, 1.28e-05, 2.28e-08, 1.06e-01, 1.06e-01]    [5.64e-05, 1.28e-05, 2.28e-08, 1.06e-01, 1.06e-01]    []  
11000     [6.89e-05, 7.46e-06, 1.79e-07, 3.54e+00, 3.54e+00]    [6.89e-05, 7.46e-06, 1.79e-07, 3.54e+00, 3.54e+00]    []  
27000     [8.47e-05, 1.92e-05, 1.70e-05, 2.57e-01, 2.57e-01]    [8.47e-05, 1.92e-05, 1.70e-05, 2.57e-01, 2.57e-01]    []  
15000     [3.55e-04, 1.67e-04, 1.41e-06, 1.20e+00, 1.20e+00]    [3.55e-04, 1.67e-04, 1.41e-06, 1.20e+00, 1.20e+00]    []  
16000     [5.44e-05, 1.92e-05, 3.30e-07, 8.64e-02, 8.64e-02]    [5.44e-05, 1.92e-05, 3.30e-07, 8.64e-02, 8.64e-02]    []  
7000      [2.65e-03, 7.24e-04, 9.60e-04, 1.67e+00, 1.67e+00]    [2.65e-03, 7.24e-04, 9.60e-04, 1.67e+00, 1.67e+00]    []  
12000     [6.27e-05, 1.15e-05, 5.37e-08, 4.52e-01, 4.52e-01]    [6.27e-05, 1.15e-05, 5.37e-08, 4.52e-01, 4.52e-01]    []  
29000     [5.57e-05, 1.05e-05, 1.02e-07, 9.84e-02, 9.84e-02]    [5.57e-05, 1.05e-05, 1.02e-07, 9.84e-02, 9.84e-02]    []  
28000     [7.67e-05, 1.80e-05, 7.26e-05, 1.93e-01, 1.93e-01]    [7.67e-05, 1.80e-05, 7.26e-05, 1.93e-01, 1.93e-01]    []  
16000     [2.39e-04, 9.24e-05, 8.86e-06, 9.07e-01, 9.07e-01]    [2.39e-04, 9.24e-05, 8.86e-06, 9.07e-01, 9.07e-01]    []  
13000     [5.02e-05, 8.29e-06, 1.28e-07, 2.31e-01, 2.31e-01]    [5.02e-05, 8.29e-06, 1.28e-07, 2.31e-01, 2.31e-01]    []  
8000      [3.92e-03, 6.82e-04, 3.29e-04, 1.30e+00, 1.30e+00]    [3.92e-03, 6.82e-04, 3.29e-04, 1.30e+00, 1.30e+00]    []  
17000     [4.35e-05, 1.44e-05, 6.48e-07, 1.46e-01, 1.46e-01]    [4.35e-05, 1.44e-05, 6.48e-07, 1.46e-01, 1.46e-01]    []  
14000     [1.30e-04, 1.09e-05, 2.88e-07, 1.47e+00, 1.47e+00]    [1.30e-04, 1.09e-05, 2.88e-07, 1.47e+00, 1.47e+00]    []  
30000     [5.43e-05, 1.01e-05, 3.63e-08, 9.09e-02, 9.09e-02]    [5.43e-05, 1.01e-05, 3.63e-08, 9.09e-02, 9.09e-02]    []  
17000     [2.37e-04, 7.47e-05, 3.05e-06, 4.95e-01, 4.95e-01]    [2.37e-04, 7.47e-05, 3.05e-06, 4.95e-01, 4.95e-01]    []  
29000     [6.77e-05, 1.79e-05, 4.79e-06, 3.67e-01, 3.67e-01]    [6.77e-05, 1.79e-05, 4.79e-06, 3.67e-01, 3.67e-01]    []  
15000     [5.83e-05, 5.94e-06, 1.84e-07, 1.19e-01, 1.19e-01]    [5.83e-05, 5.94e-06, 1.84e-07, 1.19e-01, 1.19e-01]    []  
18000     [1.64e-04, 3.44e-05, 1.79e-06, 1.95e-01, 1.95e-01]    [1.64e-04, 3.44e-05, 1.79e-06, 1.95e-01, 1.95e-01]    []  
9000      [2.27e-03, 5.74e-04, 3.76e-04, 1.58e+00, 1.58e+00]    [2.27e-03, 5.74e-04, 3.76e-04, 1.58e+00, 1.58e+00]    []  
18000     [3.07e-05, 1.34e-05, 7.14e-08, 6.40e-02, 6.40e-02]    [3.07e-05, 1.34e-05, 7.14e-08, 6.40e-02, 6.40e-02]    []  
31000     [5.65e-05, 1.19e-05, 8.50e-08, 8.30e-02, 8.30e-02]    [5.65e-05, 1.19e-05, 8.50e-08, 8.30e-02, 8.30e-02]    []  
16000     [3.02e-05, 4.85e-06, 2.28e-07, 7.11e-01, 7.11e-01]    [3.02e-05, 4.85e-06, 2.28e-07, 7.11e-01, 7.11e-01]    []  
30000     [7.76e-05, 1.70e-05, 6.92e-06, 1.29e-01, 1.29e-01]    [7.76e-05, 1.70e-05, 6.92e-06, 1.29e-01, 1.29e-01]    []  
19000     [1.56e-04, 3.38e-05, 1.44e-07, 6.45e-02, 6.45e-02]    [1.56e-04, 3.38e-05, 1.44e-07, 6.45e-02, 6.45e-02]    []  
10000     [2.27e-02, 5.60e-04, 6.87e-05, 6.36e-01, 6.36e-01]    [2.27e-02, 5.60e-04, 6.87e-05, 6.36e-01, 6.36e-01]    []  
17000     [2.54e-05, 2.08e-06, 3.09e-08, 8.55e-02, 8.55e-02]    [2.54e-05, 2.08e-06, 3.09e-08, 8.55e-02, 8.55e-02]    []  
32000     [5.42e-05, 1.41e-05, 3.50e-07, 4.71e-01, 4.71e-01]    [5.42e-05, 1.41e-05, 3.50e-07, 4.71e-01, 4.71e-01]    []  
19000     [3.53e-05, 6.02e-06, 1.34e-06, 7.32e-02, 7.32e-02]    [3.53e-05, 6.02e-06, 1.34e-06, 7.32e-02, 7.32e-02]    []  
20000     [9.97e-05, 6.70e-06, 1.33e-07, 2.17e-01, 2.17e-01]    [9.97e-05, 6.70e-06, 1.33e-07, 2.17e-01, 2.17e-01]    []  
31000     [7.24e-05, 1.66e-05, 2.54e-06, 1.46e-01, 1.46e-01]    [7.24e-05, 1.66e-05, 2.54e-06, 1.46e-01, 1.46e-01]    []  
18000     [2.93e-05, 8.00e-06, 1.01e-07, 1.05e+00, 1.05e+00]    [2.93e-05, 8.00e-06, 1.01e-07, 1.05e+00, 1.05e+00]    []  
11000     [1.92e-03, 5.07e-04, 1.76e-04, 4.22e-01, 4.22e-01]    [1.92e-03, 5.07e-04, 1.76e-04, 4.22e-01, 4.22e-01]    []  
21000     [1.06e-04, 1.82e-05, 1.31e-07, 5.83e-02, 5.83e-02]    [1.06e-04, 1.82e-05, 1.31e-07, 5.83e-02, 5.83e-02]    []  
33000     [4.90e-05, 1.53e-05, 8.63e-08, 7.15e-02, 7.15e-02]    [4.90e-05, 1.53e-05, 8.63e-08, 7.15e-02, 7.15e-02]    []  
19000     [1.85e-05, 1.74e-06, 2.29e-08, 7.70e-02, 7.70e-02]    [1.85e-05, 1.74e-06, 2.29e-08, 7.70e-02, 7.70e-02]    []  
20000     [3.16e-05, 5.57e-06, 3.09e-07, 4.95e-02, 4.95e-02]    [3.16e-05, 5.57e-06, 3.09e-07, 4.95e-02, 4.95e-02]    []  
32000     [7.29e-05, 1.58e-05, 1.56e-05, 1.75e-01, 1.75e-01]    [7.29e-05, 1.58e-05, 1.56e-05, 1.75e-01, 1.75e-01]    []  
22000     [1.05e-04, 1.88e-05, 3.24e-08, 4.48e-02, 4.48e-02]    [1.05e-04, 1.88e-05, 3.24e-08, 4.48e-02, 4.48e-02]    []  
20000     [6.06e-06, 2.88e-07, 4.15e-09, 6.79e-02, 6.79e-02]    [6.06e-06, 2.88e-07, 4.15e-09, 6.79e-02, 6.79e-02]    []  
12000     [1.99e-03, 4.89e-04, 3.70e-05, 4.35e-01, 4.35e-01]    [1.99e-03, 4.89e-04, 3.70e-05, 4.35e-01, 4.35e-01]    []  
34000     [4.55e-05, 7.67e-06, 4.76e-08, 7.05e-02, 7.05e-02]    [4.55e-05, 7.67e-06, 4.76e-08, 7.05e-02, 7.05e-02]    []  
21000     [2.50e-05, 5.20e-06, 8.31e-07, 7.02e-02, 7.02e-02]    [2.50e-05, 5.20e-06, 8.31e-07, 7.02e-02, 7.02e-02]    []  
33000     [1.02e-04, 1.48e-05, 1.82e-05, 5.54e-01, 5.54e-01]    [1.02e-04, 1.48e-05, 1.82e-05, 5.54e-01, 5.54e-01]    []  
21000     [3.87e-06, 1.50e-07, 7.66e-09, 4.93e-01, 4.93e-01]    [3.87e-06, 1.50e-07, 7.66e-09, 4.93e-01, 4.93e-01]    []  
23000     [1.05e-04, 2.13e-05, 1.27e-07, 4.40e-02, 4.40e-02]    [1.05e-04, 2.13e-05, 1.27e-07, 4.40e-02, 4.40e-02]    []  
13000     [5.05e-03, 5.03e-04, 8.08e-05, 1.69e+00, 1.69e+00]    [5.05e-03, 5.03e-04, 8.08e-05, 1.69e+00, 1.69e+00]    []  
22000     [5.36e-06, 3.27e-07, 9.53e-09, 5.93e-02, 5.93e-02]    [5.36e-06, 3.27e-07, 9.53e-09, 5.93e-02, 5.93e-02]    []  
35000     [4.89e-05, 8.66e-06, 1.19e-07, 6.29e-02, 6.29e-02]    [4.89e-05, 8.66e-06, 1.19e-07, 6.29e-02, 6.29e-02]    []  
24000     [8.79e-05, 1.86e-05, 4.05e-08, 4.09e-02, 4.09e-02]    [8.79e-05, 1.86e-05, 4.05e-08, 4.09e-02, 4.09e-02]    []  
34000     [4.92e-05, 1.44e-05, 5.38e-06, 1.19e-01, 1.19e-01]    [4.92e-05, 1.44e-05, 5.38e-06, 1.19e-01, 1.19e-01]    []  
22000     [1.95e-05, 3.89e-06, 3.71e-08, 4.08e-02, 4.08e-02]    [1.95e-05, 3.89e-06, 3.71e-08, 4.08e-02, 4.08e-02]    []  
23000     [3.59e-05, 2.10e-07, 5.55e-08, 3.36e+00, 3.36e+00]    [3.59e-05, 2.10e-07, 5.55e-08, 3.36e+00, 3.36e+00]    []  
25000     [1.71e-04, 5.02e-05, 7.76e-08, 9.15e+00, 9.15e+00]    [1.71e-04, 5.02e-05, 7.76e-08, 9.15e+00, 9.15e+00]    []  
14000     [2.08e-03, 5.20e-04, 3.44e-05, 8.28e-01, 8.28e-01]    [2.08e-03, 5.20e-04, 3.44e-05, 8.28e-01, 8.28e-01]    []  
36000     [4.92e-05, 8.73e-06, 1.38e-07, 6.91e-02, 6.91e-02]    [4.92e-05, 8.73e-06, 1.38e-07, 6.91e-02, 6.91e-02]    []  
24000     [1.87e-05, 8.11e-06, 9.43e-09, 7.14e-01, 7.14e-01]    [1.87e-05, 8.11e-06, 9.43e-09, 7.14e-01, 7.14e-01]    []  
35000     [5.06e-05, 1.38e-05, 2.19e-06, 9.71e-02, 9.71e-02]    [5.06e-05, 1.38e-05, 2.19e-06, 9.71e-02, 9.71e-02]    []  
23000     [1.84e-05, 4.41e-06, 1.29e-07, 3.80e-02, 3.80e-02]    [1.84e-05, 4.41e-06, 1.29e-07, 3.80e-02, 3.80e-02]    []  
26000     [1.96e-04, 4.55e-05, 2.13e-07, 6.87e-01, 6.87e-01]    [1.96e-04, 4.55e-05, 2.13e-07, 6.87e-01, 6.87e-01]    []  
15000     [1.57e-03, 4.89e-04, 5.85e-05, 2.67e-01, 2.67e-01]    [1.57e-03, 4.89e-04, 5.85e-05, 2.67e-01, 2.67e-01]    []  
25000     [5.73e-06, 2.56e-07, 3.85e-09, 4.75e-01, 4.75e-01]    [5.73e-06, 2.56e-07, 3.85e-09, 4.75e-01, 4.75e-01]    []  
37000     [4.23e-05, 5.59e-06, 1.31e-07, 5.62e-02, 5.62e-02]    [4.23e-05, 5.59e-06, 1.31e-07, 5.62e-02, 5.62e-02]    []  
27000     [9.11e-05, 2.07e-05, 1.61e-07, 6.47e-02, 6.47e-02]    [9.11e-05, 2.07e-05, 1.61e-07, 6.47e-02, 6.47e-02]    []  
36000     [4.05e-05, 1.37e-05, 6.22e-06, 1.88e-01, 1.88e-01]    [4.05e-05, 1.37e-05, 6.22e-06, 1.88e-01, 1.88e-01]    []  
26000     [1.54e-06, 1.66e-08, 1.43e-08, 5.47e-02, 5.47e-02]    [1.54e-06, 1.66e-08, 1.43e-08, 5.47e-02, 5.47e-02]    []  
24000     [5.88e-05, 3.76e-06, 3.57e-07, 3.84e+00, 3.84e+00]    [5.88e-05, 3.76e-06, 3.57e-07, 3.84e+00, 3.84e+00]    []  
16000     [3.71e-02, 4.36e-04, 1.07e-04, 4.99e-01, 4.99e-01]    [3.71e-02, 4.36e-04, 1.07e-04, 4.99e-01, 4.99e-01]    []  
28000     [6.84e-05, 1.57e-05, 7.11e-08, 3.12e-02, 3.12e-02]    [6.84e-05, 1.57e-05, 7.11e-08, 3.12e-02, 3.12e-02]    []  
27000     [1.55e-06, 3.17e-08, 8.91e-09, 4.14e-02, 4.14e-02]    [1.55e-06, 3.17e-08, 8.91e-09, 4.14e-02, 4.14e-02]    []  
38000     [4.23e-05, 5.35e-06, 3.65e-08, 5.31e-02, 5.31e-02]    [4.23e-05, 5.35e-06, 3.65e-08, 5.31e-02, 5.31e-02]    []  
37000     [4.46e-05, 1.32e-05, 3.94e-06, 8.18e-02, 8.18e-02]    [4.46e-05, 1.32e-05, 3.94e-06, 8.18e-02, 8.18e-02]    []  
28000     [4.88e-07, 7.52e-09, 4.54e-09, 3.89e-02, 3.89e-02]    [4.88e-07, 7.52e-09, 4.54e-09, 3.89e-02, 3.89e-02]    []  
29000     [6.13e-05, 6.95e-06, 1.16e-07, 6.33e-01, 6.33e-01]    [6.13e-05, 6.95e-06, 1.16e-07, 6.33e-01, 6.33e-01]    []  
25000     [1.36e-05, 1.36e-06, 8.62e-08, 1.04e-01, 1.04e-01]    [1.36e-05, 1.36e-06, 8.62e-08, 1.04e-01, 1.04e-01]    []  
17000     [6.10e-03, 5.33e-04, 7.95e-05, 5.35e-01, 5.35e-01]    [6.10e-03, 5.33e-04, 7.95e-05, 5.35e-01, 5.35e-01]    []  
39000     [4.59e-05, 5.51e-06, 1.23e-07, 2.47e-01, 2.47e-01]    [4.59e-05, 5.51e-06, 1.23e-07, 2.47e-01, 2.47e-01]    []  
29000     [5.47e-07, 7.23e-09, 6.27e-09, 4.40e-02, 4.40e-02]    [5.47e-07, 7.23e-09, 6.27e-09, 4.40e-02, 4.40e-02]    []  
38000     [4.35e-05, 1.35e-05, 6.27e-06, 1.42e-01, 1.42e-01]    [4.35e-05, 1.35e-05, 6.27e-06, 1.42e-01, 1.42e-01]    []  
30000     [6.32e-05, 1.63e-05, 2.35e-08, 2.53e-01, 2.53e-01]    [6.32e-05, 1.63e-05, 2.35e-08, 2.53e-01, 2.53e-01]    []  
26000     [2.90e-05, 2.82e-06, 1.43e-07, 8.02e-02, 8.02e-02]    [2.90e-05, 2.82e-06, 1.43e-07, 8.02e-02, 8.02e-02]    []  
18000     [1.71e-03, 4.29e-04, 4.57e-04, 1.74e-01, 1.74e-01]    [1.71e-03, 4.29e-04, 4.57e-04, 1.74e-01, 1.74e-01]    []  
30000     [6.75e-06, 1.25e-07, 9.49e-08, 8.99e-02, 8.99e-02]    [6.75e-06, 1.25e-07, 9.49e-08, 8.99e-02, 8.99e-02]    []  
31000     [5.78e-05, 1.29e-05, 3.49e-08, 2.43e-02, 2.43e-02]    [5.78e-05, 1.29e-05, 3.49e-08, 2.43e-02, 2.43e-02]    []  
40000     [5.65e-05, 1.01e-05, 9.80e-08, 3.16e-01, 3.16e-01]    [5.65e-05, 1.01e-05, 9.80e-08, 3.16e-01, 3.16e-01]    []  
39000     [4.31e-05, 1.37e-05, 4.03e-06, 1.42e-01, 1.42e-01]    [4.31e-05, 1.37e-05, 4.03e-06, 1.42e-01, 1.42e-01]    []  
31000     [1.57e-05, 4.89e-06, 3.37e-08, 3.96e-01, 3.96e-01]    [1.57e-05, 4.89e-06, 3.37e-08, 3.96e-01, 3.96e-01]    []  
19000     [1.58e-03, 4.27e-04, 1.05e-04, 2.33e-01, 2.33e-01]    [1.58e-03, 4.27e-04, 1.05e-04, 2.33e-01, 2.33e-01]    []  
32000     [4.72e-05, 9.45e-06, 1.49e-08, 2.23e-02, 2.23e-02]    [4.72e-05, 9.45e-06, 1.49e-08, 2.23e-02, 2.23e-02]    []  
27000     [5.38e-05, 1.45e-06, 2.43e-07, 9.52e-01, 9.52e-01]    [5.38e-05, 1.45e-06, 2.43e-07, 9.52e-01, 9.52e-01]    []  
41000     [4.65e-05, 7.85e-06, 7.00e-08, 4.52e-02, 4.52e-02]    [4.65e-05, 7.85e-06, 7.00e-08, 4.52e-02, 4.52e-02]    []  
32000     [3.66e-06, 4.04e-08, 4.58e-08, 9.40e-02, 9.40e-02]    [3.66e-06, 4.04e-08, 4.58e-08, 9.40e-02, 9.40e-02]    []  
40000     [4.40e-05, 1.42e-05, 9.51e-06, 7.52e-02, 7.52e-02]    [4.40e-05, 1.42e-05, 9.51e-06, 7.52e-02, 7.52e-02]    []  
33000     [5.10e-05, 1.18e-05, 2.15e-07, 2.57e-02, 2.57e-02]    [5.10e-05, 1.18e-05, 2.15e-07, 2.57e-02, 2.57e-02]    []  
20000     [2.42e-03, 4.59e-04, 1.22e-03, 2.63e-01, 2.63e-01]    [2.42e-03, 4.59e-04, 1.22e-03, 2.63e-01, 2.63e-01]    []  
33000     [3.49e-06, 4.36e-08, 6.03e-09, 3.03e-02, 3.03e-02]    [3.49e-06, 4.36e-08, 6.03e-09, 3.03e-02, 3.03e-02]    []  
28000     [1.31e-05, 1.25e-06, 3.91e-08, 2.11e-01, 2.11e-01]    [1.31e-05, 1.25e-06, 3.91e-08, 2.11e-01, 2.11e-01]    []  
42000     [4.51e-05, 7.48e-06, 6.88e-08, 9.62e-02, 9.62e-02]    [4.51e-05, 7.48e-06, 6.88e-08, 9.62e-02, 9.62e-02]    []  
34000     [4.24e-05, 8.04e-06, 6.38e-07, 2.18e-02, 2.18e-02]    [4.24e-05, 8.04e-06, 6.38e-07, 2.18e-02, 2.18e-02]    []  
41000     [3.75e-05, 1.46e-05, 2.85e-06, 7.84e-02, 7.84e-02]    [3.75e-05, 1.46e-05, 2.85e-06, 7.84e-02, 7.84e-02]    []  
34000     [2.38e-06, 7.19e-08, 5.72e-09, 2.82e-02, 2.82e-02]    [2.38e-06, 7.19e-08, 5.72e-09, 2.82e-02, 2.82e-02]    []  
21000     [1.80e-02, 5.64e-04, 2.61e-04, 5.38e-01, 5.38e-01]    [1.80e-02, 5.64e-04, 2.61e-04, 5.38e-01, 5.38e-01]    []  
35000     [9.11e-05, 1.46e-05, 2.94e-07, 2.44e-01, 2.44e-01]    [9.11e-05, 1.46e-05, 2.94e-07, 2.44e-01, 2.44e-01]    []  
29000     [1.90e-05, 2.22e-06, 4.58e-07, 2.74e-01, 2.74e-01]    [1.90e-05, 2.22e-06, 4.58e-07, 2.74e-01, 2.74e-01]    []  
43000     [5.12e-05, 8.03e-06, 3.86e-08, 1.35e-01, 1.35e-01]    [5.12e-05, 8.03e-06, 3.86e-08, 1.35e-01, 1.35e-01]    []  
35000     [2.81e-06, 7.66e-08, 6.89e-09, 2.91e-02, 2.91e-02]    [2.81e-06, 7.66e-08, 6.89e-09, 2.91e-02, 2.91e-02]    []  
42000     [4.35e-05, 1.60e-05, 1.08e-05, 1.34e-01, 1.34e-01]    [4.35e-05, 1.60e-05, 1.08e-05, 1.34e-01, 1.34e-01]    []  
36000     [3.95e-05, 7.54e-06, 1.94e-07, 1.88e-02, 1.88e-02]    [3.95e-05, 7.54e-06, 1.94e-07, 1.88e-02, 1.88e-02]    []  
22000     [1.25e-03, 6.53e-04, 2.02e-04, 1.08e-01, 1.08e-01]    [1.25e-03, 6.53e-04, 2.02e-04, 1.08e-01, 1.08e-01]    []  
36000     [3.59e-05, 6.18e-08, 1.62e-08, 2.88e-02, 2.88e-02]    [3.59e-05, 6.18e-08, 1.62e-08, 2.88e-02, 2.88e-02]    []  
44000     [3.92e-05, 7.31e-06, 7.53e-08, 5.94e-02, 5.94e-02]    [3.92e-05, 7.31e-06, 7.53e-08, 5.94e-02, 5.94e-02]    []  
30000     [2.01e-05, 1.52e-06, 1.03e-08, 3.79e-02, 3.79e-02]    [2.01e-05, 1.52e-06, 1.03e-08, 3.79e-02, 3.79e-02]    []  
37000     [8.43e-05, 1.13e-05, 5.73e-07, 2.33e+00, 2.33e+00]    [8.43e-05, 1.13e-05, 5.73e-07, 2.33e+00, 2.33e+00]    []  
43000     [5.12e-05, 1.65e-05, 2.16e-05, 1.32e-01, 1.32e-01]    [5.12e-05, 1.65e-05, 2.16e-05, 1.32e-01, 1.32e-01]    []  
37000     [1.51e-04, 2.07e-06, 1.54e-07, 1.77e+00, 1.77e+00]    [1.51e-04, 2.07e-06, 1.54e-07, 1.77e+00, 1.77e+00]    []  
23000     [7.55e-03, 6.16e-04, 1.86e-04, 1.69e-01, 1.69e-01]    [7.55e-03, 6.16e-04, 1.86e-04, 1.69e-01, 1.69e-01]    []  
38000     [2.87e-05, 4.17e-06, 5.91e-08, 1.32e-01, 1.32e-01]    [2.87e-05, 4.17e-06, 5.91e-08, 1.32e-01, 1.32e-01]    []  
38000     [1.38e-05, 2.05e-06, 1.97e-08, 2.94e-02, 2.94e-02]    [1.38e-05, 2.05e-06, 1.97e-08, 2.94e-02, 2.94e-02]    []  
45000     [3.54e-05, 5.69e-06, 9.02e-08, 5.69e-02, 5.69e-02]    [3.54e-05, 5.69e-06, 9.02e-08, 5.69e-02, 5.69e-02]    []  
31000     [1.52e-05, 1.03e-06, 1.73e-07, 4.36e-02, 4.36e-02]    [1.52e-05, 1.03e-06, 1.73e-07, 4.36e-02, 4.36e-02]    []  
44000     [4.32e-05, 1.68e-05, 5.11e-06, 8.54e-02, 8.54e-02]    [4.32e-05, 1.68e-05, 5.11e-06, 8.54e-02, 8.54e-02]    []  
24000     [2.17e-03, 5.79e-04, 5.65e-04, 1.33e+00, 1.33e+00]    [2.17e-03, 5.79e-04, 5.65e-04, 1.33e+00, 1.33e+00]    []  
39000     [3.42e-05, 1.67e-06, 2.06e-07, 9.91e-01, 9.91e-01]    [3.42e-05, 1.67e-06, 2.06e-07, 9.91e-01, 9.91e-01]    []  
39000     [8.74e-05, 6.89e-06, 2.57e-07, 2.27e+00, 2.27e+00]    [8.74e-05, 6.89e-06, 2.57e-07, 2.27e+00, 2.27e+00]    []  
46000     [3.46e-05, 5.53e-06, 6.69e-08, 3.60e-02, 3.60e-02]    [3.46e-05, 5.53e-06, 6.69e-08, 3.60e-02, 3.60e-02]    []  
32000     [8.18e-06, 4.39e-07, 1.15e-08, 3.10e-02, 3.10e-02]    [8.18e-06, 4.39e-07, 1.15e-08, 3.10e-02, 3.10e-02]    []  
40000     [8.98e-06, 9.76e-07, 2.69e-08, 2.76e-02, 2.76e-02]    [8.98e-06, 9.76e-07, 2.69e-08, 2.76e-02, 2.76e-02]    []  
45000     [4.71e-05, 1.65e-05, 2.23e-05, 1.45e-01, 1.45e-01]    [4.71e-05, 1.65e-05, 2.23e-05, 1.45e-01, 1.45e-01]    []  
40000     [3.11e-05, 4.22e-06, 3.98e-08, 1.48e-02, 1.48e-02]    [3.11e-05, 4.22e-06, 3.98e-08, 1.48e-02, 1.48e-02]    []  
25000     [2.36e-03, 5.29e-04, 2.79e-04, 8.23e-01, 8.23e-01]    [2.36e-03, 5.29e-04, 2.79e-04, 8.23e-01, 8.23e-01]    []  
41000     [1.49e-05, 1.40e-06, 2.63e-08, 4.46e-01, 4.46e-01]    [1.49e-05, 1.40e-06, 2.63e-08, 4.46e-01, 4.46e-01]    []  
47000     [3.73e-05, 6.09e-06, 5.07e-08, 3.42e-02, 3.42e-02]    [3.73e-05, 6.09e-06, 5.07e-08, 3.42e-02, 3.42e-02]    []  
41000     [2.66e-05, 4.43e-06, 3.19e-08, 3.99e-02, 3.99e-02]    [2.66e-05, 4.43e-06, 3.19e-08, 3.99e-02, 3.99e-02]    []  
33000     [9.67e-06, 5.22e-07, 3.43e-08, 3.03e-02, 3.03e-02]    [9.67e-06, 5.22e-07, 3.43e-08, 3.03e-02, 3.03e-02]    []  
46000     [4.27e-05, 1.69e-05, 6.69e-06, 9.05e-02, 9.05e-02]    [4.27e-05, 1.69e-05, 6.69e-06, 9.05e-02, 9.05e-02]    []  
42000     [8.98e-06, 3.31e-07, 2.67e-08, 2.08e-02, 2.08e-02]    [8.98e-06, 3.31e-07, 2.67e-08, 2.08e-02, 2.08e-02]    []  
26000     [9.33e-04, 4.83e-04, 6.12e-05, 1.47e-01, 1.47e-01]    [9.33e-04, 4.83e-04, 6.12e-05, 1.47e-01, 1.47e-01]    []  
42000     [1.67e-03, 1.56e-03, 7.06e-05, 5.07e-01, 5.07e-01]    [1.67e-03, 1.56e-03, 7.06e-05, 5.07e-01, 5.07e-01]    []  
48000     [3.56e-05, 6.72e-06, 8.20e-08, 3.38e-02, 3.38e-02]    [3.56e-05, 6.72e-06, 8.20e-08, 3.38e-02, 3.38e-02]    []  
43000     [7.13e-06, 3.47e-07, 2.74e-09, 1.96e-02, 1.96e-02]    [7.13e-06, 3.47e-07, 2.74e-09, 1.96e-02, 1.96e-02]    []  
47000     [1.25e-04, 1.89e-05, 2.09e-05, 7.17e-02, 7.17e-02]    [1.25e-04, 1.89e-05, 2.09e-05, 7.17e-02, 7.17e-02]    []  
34000     [6.16e-06, 3.41e-07, 3.56e-08, 4.82e-02, 4.82e-02]    [6.16e-06, 3.41e-07, 3.56e-08, 4.82e-02, 4.82e-02]    []  
43000     [5.80e-04, 5.50e-04, 5.49e-06, 4.91e-01, 4.91e-01]    [5.80e-04, 5.50e-04, 5.49e-06, 4.91e-01, 4.91e-01]    []  
27000     [1.23e-03, 4.19e-04, 1.37e-04, 1.38e-01, 1.38e-01]    [1.23e-03, 4.19e-04, 1.37e-04, 1.38e-01, 1.38e-01]    []  
44000     [4.25e-06, 1.44e-07, 8.53e-09, 1.88e-02, 1.88e-02]    [4.25e-06, 1.44e-07, 8.53e-09, 1.88e-02, 1.88e-02]    []  
49000     [3.29e-05, 5.22e-06, 1.00e-07, 3.23e-02, 3.23e-02]    [3.29e-05, 5.22e-06, 1.00e-07, 3.23e-02, 3.23e-02]    []  
44000     [4.40e-04, 3.25e-04, 2.70e-06, 1.14e+00, 1.14e+00]    [4.40e-04, 3.25e-04, 2.70e-06, 1.14e+00, 1.14e+00]    []  
48000     [4.20e-05, 1.77e-05, 3.89e-06, 5.54e-02, 5.54e-02]    [4.20e-05, 1.77e-05, 3.89e-06, 5.54e-02, 5.54e-02]    []  
35000     [1.31e-05, 4.32e-07, 4.59e-08, 2.79e-02, 2.79e-02]    [1.31e-05, 4.32e-07, 4.59e-08, 2.79e-02, 2.79e-02]    []  
45000     [1.15e-05, 3.99e-07, 5.69e-08, 1.31e-01, 1.31e-01]    [1.15e-05, 3.99e-07, 5.69e-08, 1.31e-01, 1.31e-01]    []  
28000     [5.76e-04, 3.65e-04, 3.10e-05, 7.31e-02, 7.31e-02]    [5.76e-04, 3.65e-04, 3.10e-05, 7.31e-02, 7.31e-02]    []  
45000     [4.96e-04, 1.31e-04, 6.78e-06, 6.73e-01, 6.73e-01]    [4.96e-04, 1.31e-04, 6.78e-06, 6.73e-01, 6.73e-01]    []  
50000     [3.49e-05, 5.06e-06, 1.90e-07, 5.83e-02, 5.83e-02]    [3.49e-05, 5.06e-06, 1.90e-07, 5.83e-02, 5.83e-02]    []  

Best model at step 49000:
  train loss: 6.47e-02
  test loss: 6.47e-02
  test metric: []

'train' took 25357.034166 s

[I 2023-10-10 11:05:57,054] Trial 69 finished with value: 0.5518866395768051 and parameters: {'num_domain': 90959, 'num_boundary': 5928, 'resampling_period': 31386, 'lr': 0.0010046626769936622}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000115 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.34e+01, 9.13e-05, 1.95e-07, 3.60e+03, 3.60e+03]    [3.34e+01, 9.13e-05, 1.95e-07, 3.60e+03, 3.60e+03]    []  
46000     [4.87e-06, 1.75e-07, 2.58e-08, 1.97e-02, 1.97e-02]    [4.87e-06, 1.75e-07, 2.58e-08, 1.97e-02, 1.97e-02]    []  
49000     [3.93e-05, 1.88e-05, 7.40e-06, 4.48e-02, 4.48e-02]    [3.93e-05, 1.88e-05, 7.40e-06, 4.48e-02, 4.48e-02]    []  
36000     [1.15e-05, 2.83e-07, 9.99e-09, 3.35e-02, 3.35e-02]    [1.15e-05, 2.83e-07, 9.99e-09, 3.35e-02, 3.35e-02]    []  
29000     [6.60e-04, 3.13e-04, 1.62e-05, 6.40e-02, 6.40e-02]    [6.60e-04, 3.13e-04, 1.62e-05, 6.40e-02, 6.40e-02]    []  
46000     [1.82e-04, 1.30e-04, 6.44e-06, 3.06e-01, 3.06e-01]    [1.82e-04, 1.30e-04, 6.44e-06, 3.06e-01, 3.06e-01]    []  
47000     [3.35e-06, 7.80e-08, 8.68e-09, 1.83e-02, 1.83e-02]    [3.35e-06, 7.80e-08, 8.68e-09, 1.83e-02, 1.83e-02]    []  
1000      [8.80e-03, 2.29e-06, 3.06e-07, 9.08e+01, 9.08e+01]    [8.80e-03, 2.29e-06, 3.06e-07, 9.08e+01, 9.08e+01]    []  
50000     [3.85e-05, 1.91e-05, 4.87e-06, 4.28e-02, 4.28e-02]    [3.85e-05, 1.91e-05, 4.87e-06, 4.28e-02, 4.28e-02]    []  

Best model at step 50000:
  train loss: 8.57e-02
  test loss: 8.57e-02
  test metric: []

'train' took 25435.822521 s

[I 2023-10-10 11:16:49,526] Trial 70 finished with value: 0.5070033792142399 and parameters: {'num_domain': 91613, 'num_boundary': 6057, 'resampling_period': 26264, 'lr': 0.0007433458604911725}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000101 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [7.12e+01, 1.92e-04, 1.74e-07, 3.60e+03, 3.60e+03]    [7.12e+01, 1.92e-04, 1.74e-07, 3.60e+03, 3.60e+03]    []  
47000     [1.33e-04, 1.22e-04, 3.14e-06, 1.00e-01, 1.00e-01]    [1.33e-04, 1.22e-04, 3.14e-06, 1.00e-01, 1.00e-01]    []  
48000     [2.23e-06, 4.76e-08, 3.40e-08, 1.89e-02, 1.89e-02]    [2.23e-06, 4.76e-08, 3.40e-08, 1.89e-02, 1.89e-02]    []  
2000      [8.93e-04, 1.51e-03, 2.72e-05, 5.44e+00, 5.44e+00]    [8.93e-04, 1.51e-03, 2.72e-05, 5.44e+00, 5.44e+00]    []  
30000     [5.96e-04, 2.90e-04, 1.81e-05, 1.73e-01, 1.73e-01]    [5.96e-04, 2.90e-04, 1.81e-05, 1.73e-01, 1.73e-01]    []  
37000     [1.21e-05, 1.93e-06, 1.47e-07, 2.48e-02, 2.48e-02]    [1.21e-05, 1.93e-06, 1.47e-07, 2.48e-02, 2.48e-02]    []  
48000     [9.84e-05, 1.13e-04, 6.74e-07, 5.13e-02, 5.13e-02]    [9.84e-05, 1.13e-04, 6.74e-07, 5.13e-02, 5.13e-02]    []  
49000     [1.94e-05, 9.47e-07, 3.31e-08, 7.10e-01, 7.10e-01]    [1.94e-05, 9.47e-07, 3.31e-08, 7.10e-01, 7.10e-01]    []  
1000      [9.72e-10, 1.05e-03, 6.38e-15, 2.09e+03, 2.09e+03]    [9.72e-10, 1.05e-03, 6.38e-15, 2.09e+03, 2.09e+03]    []  
3000      [1.05e-03, 1.19e-03, 1.62e-05, 3.71e+00, 3.71e+00]    [1.05e-03, 1.19e-03, 1.62e-05, 3.71e+00, 3.71e+00]    []  
31000     [7.81e-03, 3.00e-04, 2.94e-05, 9.63e-01, 9.63e-01]    [7.81e-03, 3.00e-04, 2.94e-05, 9.63e-01, 9.63e-01]    []  
38000     [6.89e-06, 2.40e-07, 4.29e-08, 2.70e-02, 2.70e-02]    [6.89e-06, 2.40e-07, 4.29e-08, 2.70e-02, 2.70e-02]    []  
50000     [8.50e-06, 3.93e-07, 8.42e-09, 1.64e-02, 1.64e-02]    [8.50e-06, 3.93e-07, 8.42e-09, 1.64e-02, 1.64e-02]    []  

Best model at step 50000:
  train loss: 3.28e-02
  test loss: 3.28e-02
  test metric: []

'train' took 15802.450411 s

[I 2023-10-10 11:29:16,808] Trial 73 finished with value: 0.6834015536017097 and parameters: {'num_domain': 44469, 'num_boundary': 2470, 'resampling_period': 33718, 'lr': 0.003150504855986661}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000110 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.80e+01, 5.12e-05, 1.29e-07, 3.60e+03, 3.60e+03]    [3.80e+01, 5.12e-05, 1.29e-07, 3.60e+03, 3.60e+03]    []  
49000     [6.13e-05, 9.89e-05, 3.51e-07, 4.42e-02, 4.42e-02]    [6.13e-05, 9.89e-05, 3.51e-07, 4.42e-02, 4.42e-02]    []  
4000      [9.96e-04, 1.26e-03, 4.38e-05, 1.24e+00, 1.24e+00]    [9.96e-04, 1.26e-03, 4.38e-05, 1.24e+00, 1.24e+00]    []  
2000      [1.92e-03, 7.84e-04, 9.00e-06, 1.06e+01, 1.06e+01]    [1.92e-03, 7.84e-04, 9.00e-06, 1.06e+01, 1.06e+01]    []  
50000     [6.89e-05, 1.34e-04, 4.60e-07, 9.07e-02, 9.07e-02]    [6.89e-05, 1.34e-04, 4.60e-07, 9.07e-02, 9.07e-02]    []  

Best model at step 40000:
  train loss: 2.97e-02
  test loss: 2.97e-02
  test metric: []

'train' took 18048.088743 s

[I 2023-10-10 11:35:56,148] Trial 72 finished with value: 0.6188958040452078 and parameters: {'num_domain': 93495, 'num_boundary': 6180, 'resampling_period': 28548, 'lr': 0.0030001279228941157}. Best is trial 0 with value: 0.4913720930752669.
32000     [4.66e-04, 2.39e-04, 2.46e-05, 8.23e-02, 8.23e-02]    [4.66e-04, 2.39e-04, 2.46e-05, 8.23e-02, 8.23e-02]    []  
1000      [2.01e-10, 1.05e-03, 1.14e-16, 2.09e+03, 2.09e+03]    [2.01e-10, 1.05e-03, 1.14e-16, 2.09e+03, 2.09e+03]    []  
39000     [4.28e-06, 7.32e-08, 2.11e-08, 1.98e-02, 1.98e-02]    [4.28e-06, 7.32e-08, 2.11e-08, 1.98e-02, 1.98e-02]    []  
5000      [9.12e-04, 1.04e-03, 3.82e-05, 5.94e-01, 5.94e-01]    [9.12e-04, 1.04e-03, 3.82e-05, 5.94e-01, 5.94e-01]    []  
3000      [1.88e-03, 5.76e-04, 2.62e-05, 1.05e+01, 1.05e+01]    [1.88e-03, 5.76e-04, 2.62e-05, 1.05e+01, 1.05e+01]    []  
6000      [7.41e-04, 9.21e-04, 3.24e-05, 7.24e-01, 7.24e-01]    [7.41e-04, 9.21e-04, 3.24e-05, 7.24e-01, 7.24e-01]    []  
33000     [2.83e-04, 2.04e-04, 1.02e-05, 4.38e-02, 4.38e-02]    [2.83e-04, 2.04e-04, 1.02e-05, 4.38e-02, 4.38e-02]    []  
2000      [1.18e-10, 1.05e-03, 5.41e-17, 2.09e+03, 2.09e+03]    [1.18e-10, 1.05e-03, 5.41e-17, 2.09e+03, 2.09e+03]    []  
40000     [4.11e-06, 1.64e-07, 9.47e-09, 2.07e-02, 2.07e-02]    [4.11e-06, 1.64e-07, 9.47e-09, 2.07e-02, 2.07e-02]    []  
4000      [2.16e-03, 9.63e-04, 5.03e-05, 1.77e+00, 1.77e+00]    [2.16e-03, 9.63e-04, 5.03e-05, 1.77e+00, 1.77e+00]    []  
7000      [6.38e-04, 4.17e-04, 1.05e-04, 1.06e+00, 1.06e+00]    [6.38e-04, 4.17e-04, 1.05e-04, 1.06e+00, 1.06e+00]    []  
3000      [8.93e-01, 9.43e-04, 5.95e-14, 1.90e+03, 1.90e+03]    [8.93e-01, 9.43e-04, 5.95e-14, 1.90e+03, 1.90e+03]    []  
34000     [2.73e-03, 1.68e-04, 1.41e-04, 3.20e-01, 3.20e-01]    [2.73e-03, 1.68e-04, 1.41e-04, 3.20e-01, 3.20e-01]    []  
41000     [4.54e-06, 7.52e-07, 1.97e-08, 2.20e-02, 2.20e-02]    [4.54e-06, 7.52e-07, 1.97e-08, 2.20e-02, 2.20e-02]    []  
5000      [1.34e-03, 1.15e-03, 1.55e-05, 2.39e+00, 2.39e+00]    [1.34e-03, 1.15e-03, 1.55e-05, 2.39e+00, 2.39e+00]    []  
8000      [4.31e-04, 2.73e-04, 1.81e-05, 4.33e-01, 4.33e-01]    [4.31e-04, 2.73e-04, 1.81e-05, 4.33e-01, 4.33e-01]    []  
4000      [3.37e-03, 5.39e-04, 2.17e-05, 1.21e+01, 1.21e+01]    [3.37e-03, 5.39e-04, 2.17e-05, 1.21e+01, 1.21e+01]    []  
35000     [3.88e-04, 1.62e-04, 1.06e-05, 4.05e-02, 4.05e-02]    [3.88e-04, 1.62e-04, 1.06e-05, 4.05e-02, 4.05e-02]    []  
9000      [3.40e-04, 1.75e-04, 1.16e-04, 1.93e+00, 1.93e+00]    [3.40e-04, 1.75e-04, 1.16e-04, 1.93e+00, 1.93e+00]    []  
6000      [1.21e-03, 1.06e-03, 8.05e-06, 3.85e+00, 3.85e+00]    [1.21e-03, 1.06e-03, 8.05e-06, 3.85e+00, 3.85e+00]    []  
42000     [1.61e-05, 1.79e-06, 1.32e-07, 9.25e-02, 9.25e-02]    [1.61e-05, 1.79e-06, 1.32e-07, 9.25e-02, 9.25e-02]    []  
5000      [2.01e-03, 4.18e-04, 2.21e-05, 6.98e+00, 6.98e+00]    [2.01e-03, 4.18e-04, 2.21e-05, 6.98e+00, 6.98e+00]    []  
10000     [2.44e-04, 1.54e-04, 3.30e-05, 3.52e-01, 3.52e-01]    [2.44e-04, 1.54e-04, 3.30e-05, 3.52e-01, 3.52e-01]    []  
36000     [4.03e-04, 1.68e-04, 2.76e-05, 3.63e-02, 3.63e-02]    [4.03e-04, 1.68e-04, 2.76e-05, 3.63e-02, 3.63e-02]    []  
7000      [1.00e-03, 9.96e-04, 1.61e-06, 8.87e-01, 8.87e-01]    [1.00e-03, 9.96e-04, 1.61e-06, 8.87e-01, 8.87e-01]    []  
43000     [4.03e-06, 4.29e-07, 5.06e-09, 2.35e-02, 2.35e-02]    [4.03e-06, 4.29e-07, 5.06e-09, 2.35e-02, 2.35e-02]    []  
11000     [7.62e-04, 9.36e-04, 2.13e-05, 4.78e-01, 4.78e-01]    [7.62e-04, 9.36e-04, 2.13e-05, 4.78e-01, 4.78e-01]    []  
6000      [1.25e-03, 5.69e-04, 1.45e-05, 4.95e+00, 4.95e+00]    [1.25e-03, 5.69e-04, 1.45e-05, 4.95e+00, 4.95e+00]    []  
37000     [4.61e-04, 1.66e-04, 1.21e-05, 6.23e-02, 6.23e-02]    [4.61e-04, 1.66e-04, 1.21e-05, 6.23e-02, 6.23e-02]    []  
8000      [1.05e-03, 1.15e-03, 1.27e-05, 1.07e+00, 1.07e+00]    [1.05e-03, 1.15e-03, 1.27e-05, 1.07e+00, 1.07e+00]    []  
12000     [6.46e-04, 6.50e-04, 4.15e-05, 4.25e-01, 4.25e-01]    [6.46e-04, 6.50e-04, 4.15e-05, 4.25e-01, 4.25e-01]    []  
44000     [5.86e-06, 2.48e-07, 2.10e-08, 3.24e-02, 3.24e-02]    [5.86e-06, 2.48e-07, 2.10e-08, 3.24e-02, 3.24e-02]    []  
7000      [1.50e-03, 5.34e-04, 2.78e-05, 3.47e+00, 3.47e+00]    [1.50e-03, 5.34e-04, 2.78e-05, 3.47e+00, 3.47e+00]    []  
38000     [4.33e-04, 1.74e-04, 3.30e-05, 3.29e-02, 3.29e-02]    [4.33e-04, 1.74e-04, 3.30e-05, 3.29e-02, 3.29e-02]    []  
9000      [9.13e-04, 1.04e-03, 4.34e-06, 7.88e-01, 7.88e-01]    [9.13e-04, 1.04e-03, 4.34e-06, 7.88e-01, 7.88e-01]    []  
13000     [6.08e-04, 6.26e-04, 9.90e-05, 5.03e-01, 5.03e-01]    [6.08e-04, 6.26e-04, 9.90e-05, 5.03e-01, 5.03e-01]    []  
8000      [1.74e-03, 7.99e-04, 7.21e-06, 2.68e+00, 2.68e+00]    [1.74e-03, 7.99e-04, 7.21e-06, 2.68e+00, 2.68e+00]    []  
45000     [3.40e-06, 4.81e-07, 6.08e-09, 1.58e-02, 1.58e-02]    [3.40e-06, 4.81e-07, 6.08e-09, 1.58e-02, 1.58e-02]    []  
14000     [5.20e-04, 4.38e-04, 6.40e-05, 1.57e-01, 1.57e-01]    [5.20e-04, 4.38e-04, 6.40e-05, 1.57e-01, 1.57e-01]    []  
39000     [4.08e-03, 2.21e-04, 1.63e-04, 4.68e-01, 4.68e-01]    [4.08e-03, 2.21e-04, 1.63e-04, 4.68e-01, 4.68e-01]    []  
10000     [7.74e-04, 9.83e-04, 3.96e-06, 7.85e-01, 7.85e-01]    [7.74e-04, 9.83e-04, 3.96e-06, 7.85e-01, 7.85e-01]    []  
9000      [1.51e-03, 1.20e-03, 1.51e-05, 2.00e+00, 2.00e+00]    [1.51e-03, 1.20e-03, 1.51e-05, 2.00e+00, 2.00e+00]    []  
15000     [4.55e-04, 3.29e-04, 2.42e-05, 1.14e-01, 1.14e-01]    [4.55e-04, 3.29e-04, 2.42e-05, 1.14e-01, 1.14e-01]    []  
46000     [1.81e-06, 5.26e-08, 4.03e-09, 2.55e-02, 2.55e-02]    [1.81e-06, 5.26e-08, 4.03e-09, 2.55e-02, 2.55e-02]    []  
11000     [7.36e-04, 5.58e-04, 8.21e-07, 7.50e-01, 7.50e-01]    [7.36e-04, 5.58e-04, 8.21e-07, 7.50e-01, 7.50e-01]    []  
40000     [6.13e-04, 2.00e-04, 2.54e-05, 3.39e-02, 3.39e-02]    [6.13e-04, 2.00e-04, 2.54e-05, 3.39e-02, 3.39e-02]    []  
16000     [4.58e-04, 3.04e-04, 3.29e-05, 5.20e-01, 5.20e-01]    [4.58e-04, 3.04e-04, 3.29e-05, 5.20e-01, 5.20e-01]    []  
10000     [9.06e-04, 1.50e-03, 5.00e-06, 1.71e+00, 1.71e+00]    [9.06e-04, 1.50e-03, 5.00e-06, 1.71e+00, 1.71e+00]    []  
47000     [4.78e-05, 4.49e-06, 1.38e-06, 6.91e-01, 6.91e-01]    [4.78e-05, 4.49e-06, 1.38e-06, 6.91e-01, 6.91e-01]    []  
12000     [5.44e-04, 4.12e-04, 7.80e-07, 7.49e-01, 7.49e-01]    [5.44e-04, 4.12e-04, 7.80e-07, 7.49e-01, 7.49e-01]    []  
41000     [6.87e-04, 2.20e-04, 1.09e-05, 2.88e-02, 2.88e-02]    [6.87e-04, 2.20e-04, 1.09e-05, 2.88e-02, 2.88e-02]    []  
17000     [4.21e-04, 2.95e-04, 1.11e-04, 1.08e-01, 1.08e-01]    [4.21e-04, 2.95e-04, 1.11e-04, 1.08e-01, 1.08e-01]    []  
11000     [1.00e-03, 1.35e-03, 5.02e-05, 3.60e+00, 3.60e+00]    [1.00e-03, 1.35e-03, 5.02e-05, 3.60e+00, 3.60e+00]    []  
48000     [2.86e-06, 9.54e-07, 1.58e-08, 1.39e-02, 1.39e-02]    [2.86e-06, 9.54e-07, 1.58e-08, 1.39e-02, 1.39e-02]    []  
13000     [6.65e-04, 3.07e-04, 6.36e-06, 5.47e-01, 5.47e-01]    [6.65e-04, 3.07e-04, 6.36e-06, 5.47e-01, 5.47e-01]    []  
18000     [1.15e-03, 8.38e-04, 5.97e-05, 1.83e+00, 1.83e+00]    [1.15e-03, 8.38e-04, 5.97e-05, 1.83e+00, 1.83e+00]    []  
42000     [8.34e-04, 1.37e-04, 5.35e-05, 6.03e-02, 6.03e-02]    [8.34e-04, 1.37e-04, 5.35e-05, 6.03e-02, 6.03e-02]    []  
12000     [6.66e-04, 1.26e-03, 7.41e-06, 1.12e+00, 1.12e+00]    [6.66e-04, 1.26e-03, 7.41e-06, 1.12e+00, 1.12e+00]    []  
19000     [3.63e-04, 2.66e-04, 7.23e-05, 1.14e-01, 1.14e-01]    [3.63e-04, 2.66e-04, 7.23e-05, 1.14e-01, 1.14e-01]    []  
14000     [5.11e-04, 3.31e-04, 4.89e-06, 4.63e-01, 4.63e-01]    [5.11e-04, 3.31e-04, 4.89e-06, 4.63e-01, 4.63e-01]    []  
49000     [1.31e-05, 1.54e-06, 9.86e-08, 7.40e-02, 7.40e-02]    [1.31e-05, 1.54e-06, 9.86e-08, 7.40e-02, 7.40e-02]    []  
43000     [7.67e-04, 2.35e-04, 4.92e-06, 5.65e-02, 5.65e-02]    [7.67e-04, 2.35e-04, 4.92e-06, 5.65e-02, 5.65e-02]    []  
13000     [6.11e-04, 1.21e-03, 1.08e-05, 1.08e+00, 1.08e+00]    [6.11e-04, 1.21e-03, 1.08e-05, 1.08e+00, 1.08e+00]    []  
20000     [3.71e-04, 2.80e-04, 5.66e-05, 7.49e-02, 7.49e-02]    [3.71e-04, 2.80e-04, 5.66e-05, 7.49e-02, 7.49e-02]    []  
15000     [4.93e-04, 2.73e-04, 6.96e-06, 1.16e+00, 1.16e+00]    [4.93e-04, 2.73e-04, 6.96e-06, 1.16e+00, 1.16e+00]    []  
44000     [1.41e-03, 2.00e-04, 9.69e-05, 3.19e-01, 3.19e-01]    [1.41e-03, 2.00e-04, 9.69e-05, 3.19e-01, 3.19e-01]    []  
50000     [3.95e-06, 2.32e-06, 8.87e-08, 1.32e-02, 1.32e-02]    [3.95e-06, 2.32e-06, 8.87e-08, 1.32e-02, 1.32e-02]    []  

Best model at step 50000:
  train loss: 2.63e-02
  test loss: 2.63e-02
  test metric: []

'train' took 27947.376627 s

[I 2023-10-10 13:21:59,043] Trial 71 finished with value: 0.6913051149596762 and parameters: {'num_domain': 91957, 'num_boundary': 5911, 'resampling_period': 27847, 'lr': 0.003332708179023552}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000109 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.72e+01, 5.89e-05, 1.10e-07, 3.59e+03, 3.59e+03]    [4.72e+01, 5.89e-05, 1.10e-07, 3.59e+03, 3.59e+03]    []  
21000     [3.36e-04, 1.59e-04, 3.00e-05, 5.88e-02, 5.88e-02]    [3.36e-04, 1.59e-04, 3.00e-05, 5.88e-02, 5.88e-02]    []  
14000     [5.85e-04, 1.25e-03, 9.83e-06, 7.82e-01, 7.82e-01]    [5.85e-04, 1.25e-03, 9.83e-06, 7.82e-01, 7.82e-01]    []  
16000     [4.24e-04, 2.13e-04, 6.93e-06, 5.49e-01, 5.49e-01]    [4.24e-04, 2.13e-04, 6.93e-06, 5.49e-01, 5.49e-01]    []  
22000     [6.96e-04, 1.51e-03, 3.24e-05, 4.43e-01, 4.43e-01]    [6.96e-04, 1.51e-03, 3.24e-05, 4.43e-01, 4.43e-01]    []  
45000     [9.45e-04, 1.97e-04, 5.16e-05, 2.36e-02, 2.36e-02]    [9.45e-04, 1.97e-04, 5.16e-05, 2.36e-02, 2.36e-02]    []  
1000      [1.59e-01, 2.52e-04, 1.98e-06, 1.33e+02, 1.33e+02]    [1.59e-01, 2.52e-04, 1.98e-06, 1.33e+02, 1.33e+02]    []  
15000     [5.22e-04, 1.18e-03, 2.24e-05, 6.47e-01, 6.47e-01]    [5.22e-04, 1.18e-03, 2.24e-05, 6.47e-01, 6.47e-01]    []  
17000     [3.24e-04, 1.86e-04, 8.98e-05, 5.12e-01, 5.12e-01]    [3.24e-04, 1.86e-04, 8.98e-05, 5.12e-01, 5.12e-01]    []  
23000     [6.65e-04, 9.21e-04, 1.80e-06, 7.36e-01, 7.36e-01]    [6.65e-04, 9.21e-04, 1.80e-06, 7.36e-01, 7.36e-01]    []  
46000     [9.28e-04, 1.75e-04, 6.11e-05, 2.58e-02, 2.58e-02]    [9.28e-04, 1.75e-04, 6.11e-05, 2.58e-02, 2.58e-02]    []  
16000     [5.80e-04, 9.80e-04, 4.66e-05, 8.47e-01, 8.47e-01]    [5.80e-04, 9.80e-04, 4.66e-05, 8.47e-01, 8.47e-01]    []  
2000      [6.77e-04, 8.46e-04, 2.14e-05, 1.35e+01, 1.35e+01]    [6.77e-04, 8.46e-04, 2.14e-05, 1.35e+01, 1.35e+01]    []  
24000     [4.72e-04, 2.41e-04, 1.67e-06, 1.20e-01, 1.20e-01]    [4.72e-04, 2.41e-04, 1.67e-06, 1.20e-01, 1.20e-01]    []  
18000     [4.26e-04, 1.48e-04, 6.84e-06, 6.23e-01, 6.23e-01]    [4.26e-04, 1.48e-04, 6.84e-06, 6.23e-01, 6.23e-01]    []  
17000     [5.67e-04, 8.07e-04, 1.58e-04, 2.44e+00, 2.44e+00]    [5.67e-04, 8.07e-04, 1.58e-04, 2.44e+00, 2.44e+00]    []  
47000     [3.71e-04, 1.75e-04, 4.85e-06, 2.46e-02, 2.46e-02]    [3.71e-04, 1.75e-04, 4.85e-06, 2.46e-02, 2.46e-02]    []  
25000     [3.88e-04, 1.41e-04, 2.93e-07, 2.38e-01, 2.38e-01]    [3.88e-04, 1.41e-04, 2.93e-07, 2.38e-01, 2.38e-01]    []  
3000      [7.87e-04, 4.05e-04, 1.67e-05, 6.67e+00, 6.67e+00]    [7.87e-04, 4.05e-04, 1.67e-05, 6.67e+00, 6.67e+00]    []  
19000     [3.21e-04, 1.45e-04, 6.72e-06, 1.85e-01, 1.85e-01]    [3.21e-04, 1.45e-04, 6.72e-06, 1.85e-01, 1.85e-01]    []  
18000     [4.26e-04, 5.78e-04, 6.08e-05, 6.97e-01, 6.97e-01]    [4.26e-04, 5.78e-04, 6.08e-05, 6.97e-01, 6.97e-01]    []  
26000     [1.37e-04, 2.53e-05, 3.89e-07, 9.90e-02, 9.90e-02]    [1.37e-04, 2.53e-05, 3.89e-07, 9.90e-02, 9.90e-02]    []  
48000     [3.34e-04, 1.70e-04, 1.12e-05, 2.01e-02, 2.01e-02]    [3.34e-04, 1.70e-04, 1.12e-05, 2.01e-02, 2.01e-02]    []  
4000      [5.19e-04, 4.08e-04, 4.77e-06, 4.95e+00, 4.95e+00]    [5.19e-04, 4.08e-04, 4.77e-06, 4.95e+00, 4.95e+00]    []  
20000     [2.46e-04, 1.14e-04, 7.66e-07, 1.00e-01, 1.00e-01]    [2.46e-04, 1.14e-04, 7.66e-07, 1.00e-01, 1.00e-01]    []  
27000     [1.16e-04, 1.81e-05, 8.61e-08, 2.59e-01, 2.59e-01]    [1.16e-04, 1.81e-05, 8.61e-08, 2.59e-01, 2.59e-01]    []  
19000     [4.57e-04, 4.76e-04, 1.67e-05, 5.63e-01, 5.63e-01]    [4.57e-04, 4.76e-04, 1.67e-05, 5.63e-01, 5.63e-01]    []  
49000     [3.34e-04, 1.46e-04, 9.03e-06, 1.82e-02, 1.82e-02]    [3.34e-04, 1.46e-04, 9.03e-06, 1.82e-02, 1.82e-02]    []  
5000      [3.60e-04, 2.26e-04, 8.05e-06, 3.09e+00, 3.09e+00]    [3.60e-04, 2.26e-04, 8.05e-06, 3.09e+00, 3.09e+00]    []  
21000     [1.85e-04, 1.03e-04, 6.70e-07, 7.91e-02, 7.91e-02]    [1.85e-04, 1.03e-04, 6.70e-07, 7.91e-02, 7.91e-02]    []  
28000     [1.09e-04, 1.04e-05, 5.13e-08, 3.73e+00, 3.73e+00]    [1.09e-04, 1.04e-05, 5.13e-08, 3.73e+00, 3.73e+00]    []  
20000     [4.33e-04, 4.13e-04, 1.54e-05, 4.36e-01, 4.36e-01]    [4.33e-04, 4.13e-04, 1.54e-05, 4.36e-01, 4.36e-01]    []  
50000     [3.57e-04, 1.29e-04, 1.87e-05, 1.75e-02, 1.75e-02]    [3.57e-04, 1.29e-04, 1.87e-05, 1.75e-02, 1.75e-02]    []  

Best model at step 50000:
  train loss: 3.55e-02
  test loss: 3.55e-02
  test metric: []

'train' took 25651.770938 s

[I 2023-10-10 14:13:28,785] Trial 74 finished with value: 0.5529740552426433 and parameters: {'num_domain': 45851, 'num_boundary': 2626, 'resampling_period': 31848, 'lr': 0.0016115316190688297}. Best is trial 0 with value: 0.4913720930752669.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000125 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [2.16e+01, 9.26e-05, 5.32e-08, 3.60e+03, 3.60e+03]    [2.16e+01, 9.26e-05, 5.32e-08, 3.60e+03, 3.60e+03]    []  
29000     [6.04e-05, 6.41e-06, 4.00e-08, 4.49e-02, 4.49e-02]    [6.04e-05, 6.41e-06, 4.00e-08, 4.49e-02, 4.49e-02]    []  
6000      [4.97e-04, 2.94e-04, 1.12e-05, 1.94e+00, 1.94e+00]    [4.97e-04, 2.94e-04, 1.12e-05, 1.94e+00, 1.94e+00]    []  
22000     [1.67e-04, 9.53e-05, 5.02e-06, 1.01e-01, 1.01e-01]    [1.67e-04, 9.53e-05, 5.02e-06, 1.01e-01, 1.01e-01]    []  
21000     [3.15e-04, 3.84e-04, 3.97e-05, 3.01e-01, 3.01e-01]    [3.15e-04, 3.84e-04, 3.97e-05, 3.01e-01, 3.01e-01]    []  
30000     [5.94e-05, 6.81e-06, 8.74e-08, 4.26e-02, 4.26e-02]    [5.94e-05, 6.81e-06, 8.74e-08, 4.26e-02, 4.26e-02]    []  
1000      [1.53e-10, 1.05e-03, 4.08e-17, 2.09e+03, 2.09e+03]    [1.53e-10, 1.05e-03, 4.08e-17, 2.09e+03, 2.09e+03]    []  
23000     [2.02e-04, 7.81e-05, 4.96e-05, 5.01e-01, 5.01e-01]    [2.02e-04, 7.81e-05, 4.96e-05, 5.01e-01, 5.01e-01]    []  
7000      [9.82e-04, 1.02e-03, 3.36e-05, 1.24e+00, 1.24e+00]    [9.82e-04, 1.02e-03, 3.36e-05, 1.24e+00, 1.24e+00]    []  
22000     [3.09e-04, 3.66e-04, 2.96e-05, 4.66e-01, 4.66e-01]    [3.09e-04, 3.66e-04, 2.96e-05, 4.66e-01, 4.66e-01]    []  
31000     [3.16e-05, 2.17e-06, 6.13e-08, 4.61e-01, 4.61e-01]    [3.16e-05, 2.17e-06, 6.13e-08, 4.61e-01, 4.61e-01]    []  
24000     [1.65e-04, 8.87e-05, 2.50e-07, 8.06e-02, 8.06e-02]    [1.65e-04, 8.87e-05, 2.50e-07, 8.06e-02, 8.06e-02]    []  
23000     [2.73e-04, 3.25e-04, 2.85e-05, 4.52e-01, 4.52e-01]    [2.73e-04, 3.25e-04, 2.85e-05, 4.52e-01, 4.52e-01]    []  
8000      [1.00e-03, 1.18e-03, 1.78e-05, 5.56e+00, 5.56e+00]    [1.00e-03, 1.18e-03, 1.78e-05, 5.56e+00, 5.56e+00]    []  
2000      [9.97e-11, 1.05e-03, 1.01e-17, 2.09e+03, 2.09e+03]    [9.97e-11, 1.05e-03, 1.01e-17, 2.09e+03, 2.09e+03]    []  
32000     [3.38e-05, 5.41e-06, 2.59e-08, 8.37e-02, 8.37e-02]    [3.38e-05, 5.41e-06, 2.59e-08, 8.37e-02, 8.37e-02]    []  
24000     [2.29e-04, 2.72e-04, 8.33e-06, 3.08e-01, 3.08e-01]    [2.29e-04, 2.72e-04, 8.33e-06, 3.08e-01, 3.08e-01]    []  
25000     [1.72e-04, 1.04e-04, 2.08e-05, 1.56e-01, 1.56e-01]    [1.72e-04, 1.04e-04, 2.08e-05, 1.56e-01, 1.56e-01]    []  
33000     [2.61e-05, 1.99e-06, 3.16e-07, 7.82e-02, 7.82e-02]    [2.61e-05, 1.99e-06, 3.16e-07, 7.82e-02, 7.82e-02]    []  
34000     [2.83e-05, 2.29e-06, 7.12e-08, 2.76e-02, 2.76e-02]    [2.83e-05, 2.29e-06, 7.12e-08, 2.76e-02, 2.76e-02]    []  
25000     [2.60e-04, 2.75e-04, 3.20e-05, 2.54e-01, 2.54e-01]    [2.60e-04, 2.75e-04, 3.20e-05, 2.54e-01, 2.54e-01]    []  
26000     [1.74e-04, 8.73e-05, 8.53e-06, 2.21e-01, 2.21e-01]    [1.74e-04, 8.73e-05, 8.53e-06, 2.21e-01, 2.21e-01]    []  
35000     [2.06e-05, 1.49e-06, 2.74e-08, 2.46e-02, 2.46e-02]    [2.06e-05, 1.49e-06, 2.74e-08, 2.46e-02, 2.46e-02]    []  
26000     [2.45e-04, 2.34e-04, 1.26e-05, 6.59e-01, 6.59e-01]    [2.45e-04, 2.34e-04, 1.26e-05, 6.59e-01, 6.59e-01]    []  
27000     [2.33e-04, 1.06e-04, 1.04e-05, 6.27e-01, 6.27e-01]    [2.33e-04, 1.06e-04, 1.04e-05, 6.27e-01, 6.27e-01]    []  
36000     [1.35e-03, 1.99e-03, 1.75e-05, 4.19e+00, 4.19e+00]    [1.35e-03, 1.99e-03, 1.75e-05, 4.19e+00, 4.19e+00]    []  
27000     [2.47e-04, 2.16e-04, 1.06e-05, 2.69e-01, 2.69e-01]    [2.47e-04, 2.16e-04, 1.06e-05, 2.69e-01, 2.69e-01]    []  
28000     [1.34e-04, 5.50e-05, 8.04e-07, 5.26e-02, 5.26e-02]    [1.34e-04, 5.50e-05, 8.04e-07, 5.26e-02, 5.26e-02]    []  
37000     [5.64e-04, 1.65e-03, 4.09e-05, 1.51e-01, 1.51e-01]    [5.64e-04, 1.65e-03, 4.09e-05, 1.51e-01, 1.51e-01]    []  
28000     [2.83e-04, 1.93e-04, 2.02e-05, 2.42e-01, 2.42e-01]    [2.83e-04, 1.93e-04, 2.02e-05, 2.42e-01, 2.42e-01]    []  
29000     [1.27e-04, 4.67e-05, 2.75e-07, 4.21e-02, 4.21e-02]    [1.27e-04, 4.67e-05, 2.75e-07, 4.21e-02, 4.21e-02]    []  
38000     [8.37e-04, 1.13e-03, 2.94e-05, 2.92e+00, 2.92e+00]    [8.37e-04, 1.13e-03, 2.94e-05, 2.92e+00, 2.92e+00]    []  
29000     [2.29e-04, 1.94e-04, 1.51e-05, 1.55e-01, 1.55e-01]    [2.29e-04, 1.94e-04, 1.51e-05, 1.55e-01, 1.55e-01]    []  
39000     [4.66e-04, 8.35e-04, 3.94e-05, 8.78e-02, 8.78e-02]    [4.66e-04, 8.35e-04, 3.94e-05, 8.78e-02, 8.78e-02]    []  
30000     [1.54e-04, 4.51e-05, 9.15e-07, 7.46e-02, 7.46e-02]    [1.54e-04, 4.51e-05, 9.15e-07, 7.46e-02, 7.46e-02]    []  
30000     [2.40e-04, 1.85e-04, 1.28e-05, 3.83e-01, 3.83e-01]    [2.40e-04, 1.85e-04, 1.28e-05, 3.83e-01, 3.83e-01]    []  
40000     [4.49e-04, 5.28e-04, 4.44e-05, 8.08e-02, 8.08e-02]    [4.49e-04, 5.28e-04, 4.44e-05, 8.08e-02, 8.08e-02]    []  
31000     [1.24e-04, 4.03e-05, 2.51e-06, 4.26e-02, 4.26e-02]    [1.24e-04, 4.03e-05, 2.51e-06, 4.26e-02, 4.26e-02]    []  
31000     [2.20e-04, 1.76e-04, 8.70e-06, 2.65e-01, 2.65e-01]    [2.20e-04, 1.76e-04, 8.70e-06, 2.65e-01, 2.65e-01]    []  
41000     [3.61e-04, 4.42e-04, 7.46e-06, 4.15e-02, 4.15e-02]    [3.61e-04, 4.42e-04, 7.46e-06, 4.15e-02, 4.15e-02]    []  
32000     [1.24e-04, 3.33e-05, 1.45e-07, 3.63e-02, 3.63e-02]    [1.24e-04, 3.33e-05, 1.45e-07, 3.63e-02, 3.63e-02]    []  
42000     [4.56e-04, 3.77e-04, 2.43e-05, 1.29e-01, 1.29e-01]    [4.56e-04, 3.77e-04, 2.43e-05, 1.29e-01, 1.29e-01]    []  
33000     [9.34e-05, 3.90e-05, 1.02e-06, 3.60e-02, 3.60e-02]    [9.34e-05, 3.90e-05, 1.02e-06, 3.60e-02, 3.60e-02]    []  
43000     [2.84e-04, 3.10e-04, 2.09e-05, 6.53e+00, 6.53e+00]    [2.84e-04, 3.10e-04, 2.09e-05, 6.53e+00, 6.53e+00]    []  
44000     [2.49e-03, 3.62e-04, 3.94e-05, 2.42e-01, 2.42e-01]    [2.49e-03, 3.62e-04, 3.94e-05, 2.42e-01, 2.42e-01]    []  
34000     [6.58e-05, 3.27e-05, 7.35e-08, 3.08e-02, 3.08e-02]    [6.58e-05, 3.27e-05, 7.35e-08, 3.08e-02, 3.08e-02]    []  
45000     [8.10e-04, 4.60e-04, 3.35e-05, 7.72e-02, 7.72e-02]    [8.10e-04, 4.60e-04, 3.35e-05, 7.72e-02, 7.72e-02]    []  
35000     [5.97e-05, 3.60e-05, 4.22e-07, 3.02e-02, 3.02e-02]    [5.97e-05, 3.60e-05, 4.22e-07, 3.02e-02, 3.02e-02]    []  
46000     [2.99e-04, 2.52e-04, 4.94e-06, 5.25e-02, 5.25e-02]    [2.99e-04, 2.52e-04, 4.94e-06, 5.25e-02, 5.25e-02]    []  
36000     [1.27e-04, 3.94e-05, 4.51e-06, 3.99e-01, 3.99e-01]    [1.27e-04, 3.94e-05, 4.51e-06, 3.99e-01, 3.99e-01]    []  
47000     [4.22e-04, 1.63e-04, 1.50e-05, 2.24e+00, 2.24e+00]    [4.22e-04, 1.63e-04, 1.50e-05, 2.24e+00, 2.24e+00]    []  
37000     [7.08e-05, 4.63e-05, 9.46e-06, 4.21e+00, 4.21e+00]    [7.08e-05, 4.63e-05, 9.46e-06, 4.21e+00, 4.21e+00]    []  
48000     [2.14e-04, 4.95e-05, 4.42e-06, 3.48e-01, 3.48e-01]    [2.14e-04, 4.95e-05, 4.42e-06, 3.48e-01, 3.48e-01]    []  
38000     [8.63e-05, 3.54e-05, 1.56e-07, 1.21e-01, 1.21e-01]    [8.63e-05, 3.54e-05, 1.56e-07, 1.21e-01, 1.21e-01]    []  
49000     [1.73e-03, 6.11e-04, 1.32e-05, 3.03e-01, 3.03e-01]    [1.73e-03, 6.11e-04, 1.32e-05, 3.03e-01, 3.03e-01]    []  
50000     [2.83e-04, 2.41e-04, 1.46e-06, 2.47e-02, 2.47e-02]    [2.83e-04, 2.41e-04, 1.46e-06, 2.47e-02, 2.47e-02]    []  

Best model at step 35000:
  train loss: 4.92e-02
  test loss: 4.92e-02
  test metric: []

'train' took 19081.086583 s

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1132, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 694, in do_commit
    dbapi_connection.commit()
sqlite3.OperationalError: database is locked

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 73, in _create_scoped_session
    session.commit()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1923, in commit
    trans.commit(_to_root=True)
  File "<string>", line 2, in commit
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1246, in commit
    trans.commit()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2615, in commit
    self._do_commit()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2720, in _do_commit
    self._connection_commit_impl()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2691, in _connection_commit_impl
    self.connection._commit_impl()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1134, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2339, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1132, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 694, in do_commit
    dbapi_connection.commit()
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked
(Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 212, in _run_trial
    frozen_trial = _tell_with_warning(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_tell.py", line 176, in _tell_with_warning
    study._storage.set_trial_state_values(frozen_trial._trial_id, state, values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_cached_storage.py", line 187, in set_trial_state_values
    return self._backend.set_trial_state_values(trial_id, state=state, values=values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 640, in set_trial_state_values
    with _create_scoped_session(self.scoped_session) as session:
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 90, in _create_scoped_session
    raise optuna.exceptions.StorageInternalError(message) from e
optuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 80, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 244, in _run_trial
    assert False, "Should not reach."
AssertionError: Should not reach.
39000     [5.49e-05, 3.47e-05, 6.81e-07, 2.51e-02, 2.51e-02]    [5.49e-05, 3.47e-05, 6.81e-07, 2.51e-02, 2.51e-02]    []  
40000     [7.35e-05, 4.27e-05, 5.49e-06, 1.71e-01, 1.71e-01]    [7.35e-05, 4.27e-05, 5.49e-06, 1.71e-01, 1.71e-01]    []  
41000     [5.66e-05, 4.36e-05, 4.32e-07, 2.46e-02, 2.46e-02]    [5.66e-05, 4.36e-05, 4.32e-07, 2.46e-02, 2.46e-02]    []  
42000     [6.59e-05, 5.37e-05, 3.93e-08, 2.03e-02, 2.03e-02]    [6.59e-05, 5.37e-05, 3.93e-08, 2.03e-02, 2.03e-02]    []  
43000     [5.15e-05, 7.24e-05, 2.10e-05, 2.42e-02, 2.42e-02]    [5.15e-05, 7.24e-05, 2.10e-05, 2.42e-02, 2.42e-02]    []  
44000     [4.93e-05, 4.70e-05, 3.71e-07, 3.62e-02, 3.62e-02]    [4.93e-05, 4.70e-05, 3.71e-07, 3.62e-02, 3.62e-02]    []  
45000     [5.09e-05, 3.01e-05, 4.23e-07, 1.86e-02, 1.86e-02]    [5.09e-05, 3.01e-05, 4.23e-07, 1.86e-02, 1.86e-02]    []  
46000     [5.85e-05, 3.27e-05, 4.33e-07, 1.73e-02, 1.73e-02]    [5.85e-05, 3.27e-05, 4.33e-07, 1.73e-02, 1.73e-02]    []  
47000     [5.84e-05, 3.46e-05, 5.76e-07, 1.74e-02, 1.74e-02]    [5.84e-05, 3.46e-05, 5.76e-07, 1.74e-02, 1.74e-02]    []  
48000     [5.22e-05, 3.11e-05, 9.77e-07, 1.79e-02, 1.79e-02]    [5.22e-05, 3.11e-05, 9.77e-07, 1.79e-02, 1.79e-02]    []  
49000     [4.26e-05, 3.55e-05, 1.16e-06, 1.56e-02, 1.56e-02]    [4.26e-05, 3.55e-05, 1.16e-06, 1.56e-02, 1.56e-02]    []  
50000     [8.01e-05, 2.93e-05, 1.47e-05, 3.38e-01, 3.38e-01]    [8.01e-05, 2.93e-05, 1.47e-05, 3.38e-01, 3.38e-01]    []  

Best model at step 49000:
  train loss: 3.14e-02
  test loss: 3.14e-02
  test metric: []

'train' took 21846.680862 s

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1132, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 694, in do_commit
    dbapi_connection.commit()
sqlite3.OperationalError: database is locked

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 73, in _create_scoped_session
    session.commit()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1923, in commit
    trans.commit(_to_root=True)
  File "<string>", line 2, in commit
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1246, in commit
    trans.commit()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2615, in commit
    self._do_commit()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2720, in _do_commit
    self._connection_commit_impl()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2691, in _connection_commit_impl
    self.connection._commit_impl()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1134, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2339, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1132, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 694, in do_commit
    dbapi_connection.commit()
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked
(Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 212, in _run_trial
    frozen_trial = _tell_with_warning(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_tell.py", line 176, in _tell_with_warning
    study._storage.set_trial_state_values(frozen_trial._trial_id, state, values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_cached_storage.py", line 187, in set_trial_state_values
    return self._backend.set_trial_state_values(trial_id, state=state, values=values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 640, in set_trial_state_values
    with _create_scoped_session(self.scoped_session) as session:
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 90, in _create_scoped_session
    raise optuna.exceptions.StorageInternalError(message) from e
optuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 80, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 244, in _run_trial
    assert False, "Should not reach."
AssertionError: Should not reach.
