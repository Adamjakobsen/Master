Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna.py", line 74, in <module>
    study = optuna.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
AttributeError: module 'optuna' has no attribute 'load_study'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna.py", line 74, in <module>
    study = optuna.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
AttributeError: module 'optuna' has no attribute 'load_study'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna.py", line 75, in <module>
    study = optuna.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
AttributeError: module 'optuna' has no attribute 'load_study'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna.py", line 75, in <module>
    study = optuna.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
AttributeError: module 'optuna' has no attribute 'load_study'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna.py", line 75, in <module>
    study = optuna.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
AttributeError: module 'optuna' has no attribute 'load_study'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000179 s

[W 2023-10-08 14:03:07,094] Trial 0 failed with parameters: {'num_domain': 66585, 'num_boundary': 2681, 'lr': 1.2990436759744085e-05} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 7.77 GiB already allocated; 71.62 MiB free; 7.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 63, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 610, in train
    self._test()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 797, in _test
    ) = self._outputs_losses(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 522, in _outputs_losses
    outs = outputs_losses(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 303, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 291, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/pde.py", line 127, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 92, in pde2d_vm
    dw_dt = dde.grad.jacobian(y, x, i=1, j=2)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 181, in jacobian
    return jacobian._Jacobians(ys, xs, i=i, j=j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 153, in __call__
    return self.Js[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 7.77 GiB already allocated; 71.62 MiB free; 7.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-08 14:03:07,192] Trial 0 failed with value None.
Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 76, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 63, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 610, in train
    self._test()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 797, in _test
    ) = self._outputs_losses(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 522, in _outputs_losses
    outs = outputs_losses(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 303, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 291, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/pde.py", line 127, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 92, in pde2d_vm
    dw_dt = dde.grad.jacobian(y, x, i=1, j=2)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 181, in jacobian
    return jacobian._Jacobians(ys, xs, i=i, j=j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 153, in __call__
    return self.Js[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 7.77 GiB already allocated; 71.62 MiB free; 7.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 2, in <module>
    import heart.MSc.deepxDE.optuna_tune as optuna_tune
ModuleNotFoundError: No module named 'heart'
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 76, in <module>
    study = optuna_tune.load_study(study_name="heartpinn", storage="sqlite:///heartpinn.db")
NameError: name 'optuna_tune' is not defined
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000150 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [5.57e+01, 7.77e-05, 3.29e-07, 3.61e+03, 3.61e+03]    [5.57e+01, 7.77e-05, 3.29e-07, 3.61e+03, 3.61e+03]    []  
[W 2023-10-08 14:10:45,221] Trial 1 failed with parameters: {'num_domain': 72357, 'num_boundary': 1093, 'lr': 0.006672856174473702} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 130.00 MiB (GPU 1; 10.75 GiB total capacity; 9.67 GiB already allocated; 19.62 MiB free; 9.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 64, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 335, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 303, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 291, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/pde.py", line 127, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 91, in pde2d_vm
    dv_dyy = dde.grad.hessian(y, x, component=0, i=1, j=1)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 283, in hessian
    return hessian._Hessians(ys, xs, component=component, i=i, j=j, grad_y=grad_y)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 251, in __call__
    return self.Hs[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 228, in __call__
    return self.H(i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 1; 10.75 GiB total capacity; 9.67 GiB already allocated; 19.62 MiB free; 9.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-08 14:10:45,254] Trial 1 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 64, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 624, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 641, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 539, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 341, in train_step
    self.opt.step(closure)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 335, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 303, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/model.py", line 291, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/data/pde.py", line 127, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 91, in pde2d_vm
    dv_dyy = dde.grad.hessian(y, x, component=0, i=1, j=1)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 283, in hessian
    return hessian._Hessians(ys, xs, component=component, i=i, j=j, grad_y=grad_y)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 251, in __call__
    return self.Hs[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 228, in __call__
    return self.H(i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 1; 10.75 GiB total capacity; 9.67 GiB already allocated; 19.62 MiB free; 9.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000123 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.15e+01, 1.29e-04, 2.05e-07, 3.61e+03, 3.61e+03]    [3.15e+01, 1.29e-04, 2.05e-07, 3.61e+03, 3.61e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000143 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.51e+02, 2.05e-03, 1.06e-07, 3.60e+03, 3.60e+03]    [1.51e+02, 2.05e-03, 1.06e-07, 3.60e+03, 3.60e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000148 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [6.07e+01, 1.08e-04, 1.47e-07, 3.61e+03, 3.61e+03]    [6.07e+01, 1.08e-04, 1.47e-07, 3.61e+03, 3.61e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000396 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.99e+01, 2.27e-05, 2.29e-07, 3.58e+03, 3.58e+03]    [3.99e+01, 2.27e-05, 2.29e-07, 3.58e+03, 3.58e+03]    []  
1000      [6.46e-07, 1.09e-03, 4.31e-16, 2.11e+03, 2.11e+03]    [6.46e-07, 1.09e-03, 4.31e-16, 2.11e+03, 2.11e+03]    []  
1000      [3.27e-07, 1.07e-03, 8.20e-16, 2.18e+03, 2.18e+03]    [3.27e-07, 1.07e-03, 8.20e-16, 2.18e+03, 2.18e+03]    []  
1000      [1.47e-07, 8.72e-04, 7.01e-13, 2.49e+03, 2.49e+03]    [1.47e-07, 8.72e-04, 7.01e-13, 2.49e+03, 2.49e+03]    []  
1000      [1.13e-03, 4.14e-04, 2.92e-05, 1.11e+01, 1.11e+01]    [1.13e-03, 4.14e-04, 2.92e-05, 1.11e+01, 1.11e+01]    []  
2000      [1.74e-09, 1.05e-03, 3.56e-16, 2.09e+03, 2.09e+03]    [1.74e-09, 1.05e-03, 3.56e-16, 2.09e+03, 2.09e+03]    []  
2000      [4.63e-08, 1.07e-03, 1.57e-16, 2.10e+03, 2.10e+03]    [4.63e-08, 1.07e-03, 1.57e-16, 2.10e+03, 2.10e+03]    []  
2000      [2.42e-07, 1.01e-03, 1.36e-14, 2.28e+03, 2.28e+03]    [2.42e-07, 1.01e-03, 1.36e-14, 2.28e+03, 2.28e+03]    []  
2000      [8.17e-05, 1.22e-04, 1.73e-08, 5.04e+00, 5.04e+00]    [8.17e-05, 1.22e-04, 1.73e-08, 5.04e+00, 5.04e+00]    []  
3000      [8.12e-11, 1.05e-03, 4.01e-17, 2.09e+03, 2.09e+03]    [8.12e-11, 1.05e-03, 4.01e-17, 2.09e+03, 2.09e+03]    []  
3000      [3.58e-10, 1.05e-03, 1.22e-16, 2.09e+03, 2.09e+03]    [3.58e-10, 1.05e-03, 1.22e-16, 2.09e+03, 2.09e+03]    []  
3000      [3.86e-04, 6.05e-05, 1.14e-07, 2.49e+00, 2.49e+00]    [3.86e-04, 6.05e-05, 1.14e-07, 2.49e+00, 2.49e+00]    []  
3000      [4.12e-07, 1.08e-03, 2.54e-15, 2.16e+03, 2.16e+03]    [4.12e-07, 1.08e-03, 2.54e-15, 2.16e+03, 2.16e+03]    []  
4000      [7.27e-11, 1.05e-03, 9.72e-17, 2.09e+03, 2.09e+03]    [7.27e-11, 1.05e-03, 9.72e-17, 2.09e+03, 2.09e+03]    []  
4000      [5.71e-11, 1.05e-03, 3.12e-17, 2.09e+03, 2.09e+03]    [5.71e-11, 1.05e-03, 3.12e-17, 2.09e+03, 2.09e+03]    []  
4000      [1.73e-04, 5.45e-05, 1.31e-06, 6.69e+00, 6.69e+00]    [1.73e-04, 5.45e-05, 1.31e-06, 6.69e+00, 6.69e+00]    []  
5000      [8.74e-11, 1.05e-03, 3.34e-16, 2.09e+03, 2.09e+03]    [8.74e-11, 1.05e-03, 3.34e-16, 2.09e+03, 2.09e+03]    []  
4000      [2.13e-07, 1.09e-03, 2.48e-15, 2.11e+03, 2.11e+03]    [2.13e-07, 1.09e-03, 2.48e-15, 2.11e+03, 2.11e+03]    []  
5000      [6.40e-11, 1.05e-03, 7.81e-18, 2.09e+03, 2.09e+03]    [6.40e-11, 1.05e-03, 7.81e-18, 2.09e+03, 2.09e+03]    []  
6000      [4.00e-03, 3.58e-04, 6.92e-07, 8.14e+01, 8.14e+01]    [4.00e-03, 3.58e-04, 6.92e-07, 8.14e+01, 8.14e+01]    []  
5000      [2.05e-04, 6.27e-05, 7.31e-07, 1.08e+00, 1.08e+00]    [2.05e-04, 6.27e-05, 7.31e-07, 1.08e+00, 1.08e+00]    []  
5000      [4.41e-08, 1.07e-03, 6.33e-16, 2.10e+03, 2.10e+03]    [4.41e-08, 1.07e-03, 6.33e-16, 2.10e+03, 2.10e+03]    []  
6000      [6.15e-11, 1.05e-03, 2.36e-18, 2.09e+03, 2.09e+03]    [6.15e-11, 1.05e-03, 2.36e-18, 2.09e+03, 2.09e+03]    []  
7000      [3.63e-03, 2.68e-04, 2.83e-05, 1.20e+01, 1.20e+01]    [3.63e-03, 2.68e-04, 2.83e-05, 1.20e+01, 1.20e+01]    []  
6000      [1.38e-04, 4.19e-05, 7.51e-07, 6.03e-01, 6.03e-01]    [1.38e-04, 4.19e-05, 7.51e-07, 6.03e-01, 6.03e-01]    []  
7000      [6.67e-11, 1.05e-03, 6.11e-19, 2.09e+03, 2.09e+03]    [6.67e-11, 1.05e-03, 6.11e-19, 2.09e+03, 2.09e+03]    []  
6000      [3.00e-09, 1.05e-03, 2.25e-16, 2.09e+03, 2.09e+03]    [3.00e-09, 1.05e-03, 2.25e-16, 2.09e+03, 2.09e+03]    []  
8000      [1.56e-03, 6.06e-04, 1.24e-05, 9.78e+00, 9.78e+00]    [1.56e-03, 6.06e-04, 1.24e-05, 9.78e+00, 9.78e+00]    []  
8000      [5.84e-11, 1.05e-03, 7.66e-19, 2.09e+03, 2.09e+03]    [5.84e-11, 1.05e-03, 7.66e-19, 2.09e+03, 2.09e+03]    []  
7000      [2.25e-04, 2.66e-05, 8.16e-07, 6.42e+00, 6.42e+00]    [2.25e-04, 2.66e-05, 8.16e-07, 6.42e+00, 6.42e+00]    []  
7000      [1.28e-10, 1.05e-03, 6.22e-17, 2.09e+03, 2.09e+03]    [1.28e-10, 1.05e-03, 6.22e-17, 2.09e+03, 2.09e+03]    []  
9000      [1.36e-03, 5.31e-04, 2.18e-05, 8.59e+00, 8.59e+00]    [1.36e-03, 5.31e-04, 2.18e-05, 8.59e+00, 8.59e+00]    []  
9000      [2.61e-01, 5.42e-04, 2.77e-03, 3.62e+02, 3.62e+02]    [2.61e-01, 5.42e-04, 2.77e-03, 3.62e+02, 3.62e+02]    []  
8000      [8.11e-05, 3.04e-05, 6.66e-07, 1.66e+00, 1.66e+00]    [8.11e-05, 3.04e-05, 6.66e-07, 1.66e+00, 1.66e+00]    []  
8000      [6.79e-11, 1.05e-03, 5.03e-17, 2.09e+03, 2.09e+03]    [6.79e-11, 1.05e-03, 5.03e-17, 2.09e+03, 2.09e+03]    []  
10000     [6.55e-04, 4.75e-04, 6.19e-06, 7.31e+00, 7.31e+00]    [6.55e-04, 4.75e-04, 6.19e-06, 7.31e+00, 7.31e+00]    []  
10000     [2.82e-02, 1.82e-04, 3.26e-04, 2.73e+01, 2.73e+01]    [2.82e-02, 1.82e-04, 3.26e-04, 2.73e+01, 2.73e+01]    []  
9000      [4.65e-07, 1.29e-06, 2.26e-09, 1.01e+00, 1.01e+00]    [4.65e-07, 1.29e-06, 2.26e-09, 1.01e+00, 1.01e+00]    []  
9000      [7.87e-11, 1.05e-03, 2.21e-17, 2.09e+03, 2.09e+03]    [7.87e-11, 1.05e-03, 2.21e-17, 2.09e+03, 2.09e+03]    []  
11000     [5.38e-04, 3.58e-04, 6.81e-06, 6.76e+00, 6.76e+00]    [5.38e-04, 3.58e-04, 6.81e-06, 6.76e+00, 6.76e+00]    []  
11000     [5.39e-03, 6.37e-04, 9.18e-05, 9.96e+00, 9.96e+00]    [5.39e-03, 6.37e-04, 9.18e-05, 9.96e+00, 9.96e+00]    []  
10000     [2.61e-06, 1.19e-07, 9.75e-10, 7.44e+00, 7.44e+00]    [2.61e-06, 1.19e-07, 9.75e-10, 7.44e+00, 7.44e+00]    []  
10000     [7.38e-11, 1.05e-03, 1.92e-16, 2.09e+03, 2.09e+03]    [7.38e-11, 1.05e-03, 1.92e-16, 2.09e+03, 2.09e+03]    []  
12000     [5.63e-04, 3.47e-04, 1.44e-05, 5.14e+00, 5.14e+00]    [5.63e-04, 3.47e-04, 1.44e-05, 5.14e+00, 5.14e+00]    []  
12000     [1.95e-03, 8.53e-04, 6.63e-05, 8.48e+00, 8.48e+00]    [1.95e-03, 8.53e-04, 6.63e-05, 8.48e+00, 8.48e+00]    []  
11000     [9.50e-07, 5.81e-08, 9.87e-09, 3.27e-01, 3.27e-01]    [9.50e-07, 5.81e-08, 9.87e-09, 3.27e-01, 3.27e-01]    []  
13000     [6.14e-04, 3.34e-04, 1.30e-06, 3.74e+00, 3.74e+00]    [6.14e-04, 3.34e-04, 1.30e-06, 3.74e+00, 3.74e+00]    []  
13000     [2.21e-03, 1.01e-03, 5.07e-05, 7.46e+00, 7.46e+00]    [2.21e-03, 1.01e-03, 5.07e-05, 7.46e+00, 7.46e+00]    []  
11000     [2.81e-01, 9.85e-04, 1.29e-03, 7.81e+02, 7.81e+02]    [2.81e-01, 9.85e-04, 1.29e-03, 7.81e+02, 7.81e+02]    []  
12000     [1.78e-06, 2.26e-07, 1.25e-09, 2.46e-01, 2.46e-01]    [1.78e-06, 2.26e-07, 1.25e-09, 2.46e-01, 2.46e-01]    []  
14000     [6.78e-04, 4.96e-04, 5.03e-06, 3.75e+00, 3.75e+00]    [6.78e-04, 4.96e-04, 5.03e-06, 3.75e+00, 3.75e+00]    []  
14000     [1.10e-03, 1.02e-03, 1.57e-05, 6.30e+00, 6.30e+00]    [1.10e-03, 1.02e-03, 1.57e-05, 6.30e+00, 6.30e+00]    []  
12000     [1.26e-02, 4.18e-04, 6.29e-04, 3.36e+02, 3.36e+02]    [1.26e-02, 4.18e-04, 6.29e-04, 3.36e+02, 3.36e+02]    []  
13000     [4.73e-06, 5.55e-07, 1.27e-09, 1.83e-01, 1.83e-01]    [4.73e-06, 5.55e-07, 1.27e-09, 1.83e-01, 1.83e-01]    []  
15000     [9.39e-04, 1.15e-03, 3.41e-06, 5.20e+00, 5.20e+00]    [9.39e-04, 1.15e-03, 3.41e-06, 5.20e+00, 5.20e+00]    []  
15000     [6.57e-04, 4.89e-04, 9.50e-07, 2.42e+00, 2.42e+00]    [6.57e-04, 4.89e-04, 9.50e-07, 2.42e+00, 2.42e+00]    []  
                                                                                                                                                                                          13000     [2.91e-02, 3.22e-04, 7.55e-03, 1.59e+02, 1.59e+02]    [2.91e-02, 3.22e-04, 7.55e-03, 1.59e+02, 1.59e+02]    []  
                                                                                                                                                                                          16000     [1.10e-03, 1.34e-03, 2.32e-06, 4.05e+00, 4.05e+00]    [1.10e-03, 1.34e-03, 2.32e-06, 4.05e+00, 4.05e+00]    []  
14000     [1.33e-05, 8.77e-07, 7.81e-09, 1.87e-01, 1.87e-01]    [1.33e-05, 8.77e-07, 7.81e-09, 1.87e-01, 1.87e-01]    []  
16000     [6.56e-04, 4.90e-04, 1.53e-06, 2.09e+00, 2.09e+00]    [6.56e-04, 4.90e-04, 1.53e-06, 2.09e+00, 2.09e+00]    []  
14000     [1.36e-02, 3.46e-04, 1.68e-03, 7.13e+01, 7.13e+01]    [1.36e-02, 3.46e-04, 1.68e-03, 7.13e+01, 7.13e+01]    []  
17000     [1.40e-03, 1.07e-03, 2.47e-06, 3.09e+00, 3.09e+00]    [1.40e-03, 1.07e-03, 2.47e-06, 3.09e+00, 3.09e+00]    []  
17000     [6.31e-04, 5.08e-04, 6.18e-07, 1.88e+00, 1.88e+00]    [6.31e-04, 5.08e-04, 6.18e-07, 1.88e+00, 1.88e+00]    []  
15000     [3.09e-05, 5.24e-06, 3.24e-06, 3.14e+00, 3.14e+00]    [3.09e-05, 5.24e-06, 3.24e-06, 3.14e+00, 3.14e+00]    []  
15000     [5.78e-03, 5.73e-04, 1.44e-03, 3.00e+01, 3.00e+01]    [5.78e-03, 5.73e-04, 1.44e-03, 3.00e+01, 3.00e+01]    []  
18000     [1.24e-03, 1.12e-03, 1.50e-06, 2.50e+00, 2.50e+00]    [1.24e-03, 1.12e-03, 1.50e-06, 2.50e+00, 2.50e+00]    []  
18000     [6.37e-04, 4.91e-04, 1.27e-06, 1.57e+00, 1.57e+00]    [6.37e-04, 4.91e-04, 1.27e-06, 1.57e+00, 1.57e+00]    []  
16000     [5.67e-05, 7.91e-06, 9.06e-10, 3.42e+00, 3.42e+00]    [5.67e-05, 7.91e-06, 9.06e-10, 3.42e+00, 3.42e+00]    []  
19000     [1.28e-03, 1.15e-03, 3.43e-06, 2.15e+00, 2.15e+00]    [1.28e-03, 1.15e-03, 3.43e-06, 2.15e+00, 2.15e+00]    []  
16000     [5.80e-03, 9.64e-04, 1.02e-03, 1.30e+01, 1.30e+01]    [5.80e-03, 9.64e-04, 1.02e-03, 1.30e+01, 1.30e+01]    []  
19000     [8.59e-04, 7.09e-04, 1.02e-06, 1.40e+00, 1.40e+00]    [8.59e-04, 7.09e-04, 1.02e-06, 1.40e+00, 1.40e+00]    []  
17000     [1.69e-05, 5.21e-07, 1.62e-07, 1.85e-01, 1.85e-01]    [1.69e-05, 5.21e-07, 1.62e-07, 1.85e-01, 1.85e-01]    []  
20000     [1.10e-03, 9.75e-04, 5.97e-06, 1.80e+00, 1.80e+00]    [1.10e-03, 9.75e-04, 5.97e-06, 1.80e+00, 1.80e+00]    []  
20000     [8.28e-04, 8.64e-04, 4.21e-07, 1.22e+00, 1.22e+00]    [8.28e-04, 8.64e-04, 4.21e-07, 1.22e+00, 1.22e+00]    []  
17000     [4.72e-03, 1.06e-03, 9.11e-04, 8.13e+00, 8.13e+00]    [4.72e-03, 1.06e-03, 9.11e-04, 8.13e+00, 8.13e+00]    []  
18000     [5.96e-06, 3.12e-07, 2.66e-09, 6.86e-02, 6.86e-02]    [5.96e-06, 3.12e-07, 2.66e-09, 6.86e-02, 6.86e-02]    []  
21000     [9.28e-04, 7.84e-04, 1.02e-05, 1.60e+00, 1.60e+00]    [9.28e-04, 7.84e-04, 1.02e-05, 1.60e+00, 1.60e+00]    []  
21000     [1.05e-03, 8.02e-04, 8.51e-06, 1.57e+00, 1.57e+00]    [1.05e-03, 8.02e-04, 8.51e-06, 1.57e+00, 1.57e+00]    []  
18000     [6.68e-03, 1.06e-03, 4.60e-04, 6.85e+00, 6.85e+00]    [6.68e-03, 1.06e-03, 4.60e-04, 6.85e+00, 6.85e+00]    []  
19000     [6.37e-06, 1.64e-07, 4.16e-09, 1.28e-01, 1.28e-01]    [6.37e-06, 1.64e-07, 4.16e-09, 1.28e-01, 1.28e-01]    []  
22000     [8.71e-04, 6.82e-04, 1.10e-05, 1.32e+00, 1.32e+00]    [8.71e-04, 6.82e-04, 1.10e-05, 1.32e+00, 1.32e+00]    []  
22000     [7.31e-04, 6.44e-04, 6.29e-07, 1.21e+00, 1.21e+00]    [7.31e-04, 6.44e-04, 6.29e-07, 1.21e+00, 1.21e+00]    []  
19000     [2.46e-03, 1.00e-03, 4.63e-05, 6.00e+00, 6.00e+00]    [2.46e-03, 1.00e-03, 4.63e-05, 6.00e+00, 6.00e+00]    []  
                                                               20000     [8.09e-06, 7.51e-07, 3.61e-07, 8.35e-02, 8.35e-02]    [8.09e-06, 7.51e-07, 3.61e-07, 8.35e-02, 8.35e-02]    []  
                           23000     [7.09e-04, 6.65e-04, 1.23e-05, 1.09e+00, 1.09e+00]    [7.09e-04, 6.65e-04, 1.23e-05, 1.09e+00, 1.09e+00]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000288 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.93e+01, 3.37e-05, 1.26e-07, 3.61e+03, 3.61e+03]    [4.93e+01, 3.37e-05, 1.26e-07, 3.61e+03, 3.61e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000339 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.38e+01, 1.45e-04, 1.14e-07, 3.60e+03, 3.60e+03]    [1.38e+01, 1.45e-04, 1.14e-07, 3.60e+03, 3.60e+03]    []  
                                                                                                                           Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000358 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.37e+01, 1.79e-04, 2.80e-07, 3.60e+03, 3.60e+03]    [1.37e+01, 1.79e-04, 2.80e-07, 3.60e+03, 3.60e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000404 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.27e+01, 6.08e-05, 2.00e-07, 3.61e+03, 3.61e+03]    [4.27e+01, 6.08e-05, 2.00e-07, 3.61e+03, 3.61e+03]    []  
[W 2023-10-08 15:20:39,151] Trial 9 failed with parameters: {'num_domain': 62221, 'num_boundary': 3084, 'lr': 0.0037607501004752394} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 80.00 MiB (GPU 1; 39.43 GiB total capacity; 5.40 GiB already allocated; 40.31 MiB free; 5.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 64, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 632, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 650, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 544, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 346, in train_step
    self.opt.step(closure)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/optim/adam.py", line 92, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 340, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 308, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 296, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/data/pde.py", line 140, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 91, in pde2d_vm
    dv_dyy = dde.grad.hessian(y, x, component=0, i=1, j=1)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 283, in hessian
    return hessian._Hessians(ys, xs, component=component, i=i, j=j, grad_y=grad_y)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 251, in __call__
    return self.Hs[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 228, in __call__
    return self.H(i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 1; 39.43 GiB total capacity; 5.40 GiB already allocated; 40.31 MiB free; 5.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[W 2023-10-08 15:20:39,231] Trial 9 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 64, in objective
    losshistory,train_state = model.train(epochs=n_epochs,display_every=1000)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 632, in train
    self._train_sgd(iterations, display_every)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 650, in _train_sgd
    self._train_step(
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 544, in _train_step
    self.train_step(inputs, targets)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 346, in train_step
    self.opt.step(closure)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/optim/adam.py", line 92, in step
    loss = closure()
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 340, in closure
    losses = outputs_losses_train(inputs, targets)[1]
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 308, in outputs_losses_train
    return outputs_losses(True, inputs, targets, self.data.losses_train)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/model.py", line 296, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/data/pde.py", line 140, in losses
    f = self.pde(inputs, outputs_pde)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/heartpinn.py", line 91, in pde2d_vm
    dv_dyy = dde.grad.hessian(y, x, component=0, i=1, j=1)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 283, in hessian
    return hessian._Hessians(ys, xs, component=component, i=i, j=j, grad_y=grad_y)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 251, in __call__
    return self.Hs[key](i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 228, in __call__
    return self.H(i, j)
  File "/itf-fi-ml/home/adamjak/.local/lib/python3.9/site-packages/deepxde/gradients.py", line 50, in __call__
    self.J[i] = torch.autograd.grad(
  File "/storage/software/PyTorch-bundle/1.10.0-MKL-bundle-pre-optimised/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 1; 39.43 GiB total capacity; 5.40 GiB already allocated; 40.31 MiB free; 5.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       21000     [9.17e-04, 8.10e-04, 5.78e-05, 4.39e+00, 4.39e+00]    [9.17e-04, 8.10e-04, 5.78e-05, 4.39e+00, 4.39e+00]    []  
22000     [3.29e-06, 6.79e-07, 2.03e-09, 1.39e-01, 1.39e-01]    [3.29e-06, 6.79e-07, 2.03e-09, 1.39e-01, 1.39e-01]    []  
25000     [4.37e-04, 1.63e-04, 1.24e-06, 6.61e-01, 6.61e-01]    [4.37e-04, 1.63e-04, 1.24e-06, 6.61e-01, 6.61e-01]    []  
                                                                                                                           26000     [3.16e-04, 3.44e-04, 9.51e-06, 6.53e-01, 6.53e-01]    [3.16e-04, 3.44e-04, 9.51e-06, 6.53e-01, 6.53e-01]    []  
22000     [1.13e-03, 5.73e-04, 3.71e-04, 3.64e+00, 3.64e+00]    [1.13e-03, 5.73e-04, 3.71e-04, 3.64e+00, 3.64e+00]    []  
23000     [4.93e-06, 3.45e-07, 5.37e-09, 5.40e-02, 5.40e-02]    [4.93e-06, 3.45e-07, 5.37e-09, 5.40e-02, 5.40e-02]    []  
26000     [2.95e-04, 8.12e-05, 1.22e-06, 7.22e-01, 7.22e-01]    [2.95e-04, 8.12e-05, 1.22e-06, 7.22e-01, 7.22e-01]    []  
                                                                                                                           27000     [3.32e-04, 3.43e-04, 7.11e-06, 5.98e-01, 5.98e-01]    [3.32e-04, 3.43e-04, 7.11e-06, 5.98e-01, 5.98e-01]    []  
                                                                                                                           23000     [1.05e-03, 5.20e-04, 2.65e-04, 2.91e+00, 2.91e+00]    [1.05e-03, 5.20e-04, 2.65e-04, 2.91e+00, 2.91e+00]    []  
24000     [5.26e-06, 2.20e-07, 6.22e-09, 4.68e-02, 4.68e-02]    [5.26e-06, 2.20e-07, 6.22e-09, 4.68e-02, 4.68e-02]    []  
27000     [2.20e-04, 5.28e-05, 4.09e-07, 9.60e-01, 9.60e-01]    [2.20e-04, 5.28e-05, 4.09e-07, 9.60e-01, 9.60e-01]    []  
28000     [2.93e-04, 3.17e-04, 8.50e-06, 5.25e-01, 5.25e-01]    [2.93e-04, 3.17e-04, 8.50e-06, 5.25e-01, 5.25e-01]    []  
28000     [1.73e-04, 4.37e-05, 4.93e-07, 4.83e-01, 4.83e-01]    [1.73e-04, 4.37e-05, 4.93e-07, 4.83e-01, 4.83e-01]    []  
25000     [5.90e-06, 1.53e-07, 1.57e-09, 5.38e-02, 5.38e-02]    [5.90e-06, 1.53e-07, 1.57e-09, 5.38e-02, 5.38e-02]    []  
24000     [1.16e-03, 6.90e-04, 2.20e-04, 2.38e+00, 2.38e+00]    [1.16e-03, 6.90e-04, 2.20e-04, 2.38e+00, 2.38e+00]    []  
29000     [2.56e-04, 2.93e-04, 7.29e-06, 4.78e-01, 4.78e-01]    [2.56e-04, 2.93e-04, 7.29e-06, 4.78e-01, 4.78e-01]    []  
                                                                                                                                                                                                                                                      29000     [1.75e-04, 4.55e-05, 3.35e-07, 4.34e-01, 4.34e-01]    [1.75e-04, 4.55e-05, 3.35e-07, 4.34e-01, 4.34e-01]    []  
26000     [9.65e-06, 5.24e-07, 7.15e-09, 5.50e-02, 5.50e-02]    [9.65e-06, 5.24e-07, 7.15e-09, 5.50e-02, 5.50e-02]    []  
25000     [9.22e-04, 1.26e-03, 1.82e-04, 2.04e+00, 2.04e+00]    [9.22e-04, 1.26e-03, 1.82e-04, 2.04e+00, 2.04e+00]    []  
30000     [2.47e-04, 2.63e-04, 8.39e-06, 4.86e-01, 4.86e-01]    [2.47e-04, 2.63e-04, 8.39e-06, 4.86e-01, 4.86e-01]    []  
30000     [1.64e-04, 4.15e-05, 4.09e-07, 4.30e-01, 4.30e-01]    [1.64e-04, 4.15e-05, 4.09e-07, 4.30e-01, 4.30e-01]    []  
27000     [8.30e-06, 3.89e-07, 4.83e-09, 4.41e-02, 4.41e-02]    [8.30e-06, 3.89e-07, 4.83e-09, 4.41e-02, 4.41e-02]    []  
31000     [2.28e-04, 2.22e-04, 7.00e-06, 4.10e-01, 4.10e-01]    [2.28e-04, 2.22e-04, 7.00e-06, 4.10e-01, 4.10e-01]    []  
26000     [1.14e-03, 1.08e-03, 1.92e-04, 1.81e+00, 1.81e+00]    [1.14e-03, 1.08e-03, 1.92e-04, 1.81e+00, 1.81e+00]    []  
                                                                                                                           31000     [1.45e-04, 3.51e-05, 5.88e-07, 5.63e-01, 5.63e-01]    [1.45e-04, 3.51e-05, 5.88e-07, 5.63e-01, 5.63e-01]    []  
32000     [1.89e-04, 1.89e-04, 7.11e-06, 3.83e-01, 3.83e-01]    [1.89e-04, 1.89e-04, 7.11e-06, 3.83e-01, 3.83e-01]    []  
28000     [2.76e-05, 4.87e-07, 6.83e-09, 4.97e-01, 4.97e-01]    [2.76e-05, 4.87e-07, 6.83e-09, 4.97e-01, 4.97e-01]    []  
27000     [9.40e-04, 1.24e-03, 1.79e-04, 1.61e+00, 1.61e+00]    [9.40e-04, 1.24e-03, 1.79e-04, 1.61e+00, 1.61e+00]    []  
32000     [1.35e-04, 3.36e-05, 2.68e-07, 3.50e-01, 3.50e-01]    [1.35e-04, 3.36e-05, 2.68e-07, 3.50e-01, 3.50e-01]    []  
5000      [1.45e-02, 1.05e-03, 4.11e-24, 2.09e+03, 2.09e+03]    [1.45e-02, 1.05e-03, 4.11e-24, 2.09e+03, 2.09e+03]    []  
5000      [1.02e-04, 1.05e-03, 3.75e-24, 2.09e+03, 2.09e+03]    [1.02e-04, 1.05e-03, 3.75e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            4000      [6.67e-04, 1.00e-03, 1.49e-05, 8.15e+00, 8.15e+00]    [6.67e-04, 1.00e-03, 1.49e-05, 8.15e+00, 8.15e+00]    []  
6000      [3.80e-07, 1.05e-03, 5.71e-23, 2.09e+03, 2.09e+03]    [3.80e-07, 1.05e-03, 5.71e-23, 2.09e+03, 2.09e+03]    []  
6000      [8.77e-02, 1.03e-03, 3.73e-24, 2.09e+03, 2.09e+03]    [8.77e-02, 1.03e-03, 3.73e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       7000      [6.17e-11, 1.05e-03, 1.02e-24, 2.09e+03, 2.09e+03]    [6.17e-11, 1.05e-03, 1.02e-24, 2.09e+03, 2.09e+03]    []  
7000      [1.32e+00, 9.01e-04, 3.61e-24, 2.09e+03, 2.09e+03]    [1.32e+00, 9.01e-04, 3.61e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                      5000      [6.30e-04, 5.48e-04, 4.21e-06, 6.62e+00, 6.62e+00]    [6.30e-04, 5.48e-04, 4.21e-06, 6.62e+00, 6.62e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            8000      [6.48e-01, 9.78e-04, 3.55e-24, 2.09e+03, 2.09e+03]    [6.48e-01, 9.78e-04, 3.55e-24, 2.09e+03, 2.09e+03]    []  
8000      [5.41e-01, 9.91e-04, 1.65e-24, 2.09e+03, 2.09e+03]    [5.41e-01, 9.91e-04, 1.65e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       6000      [4.63e-04, 3.28e-04, 5.75e-06, 6.83e+00, 6.83e+00]    [4.63e-04, 3.28e-04, 5.75e-06, 6.83e+00, 6.83e+00]    []  
                                                                                                                           9000      [6.28e-04, 1.05e-03, 3.54e-24, 2.09e+03, 2.09e+03]    [6.28e-04, 1.05e-03, 3.54e-24, 2.09e+03, 2.09e+03]    []  
9000      [4.32e-10, 1.05e-03, 7.46e-24, 2.09e+03, 2.09e+03]    [4.32e-10, 1.05e-03, 7.46e-24, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             10000     [2.81e-02, 1.04e-03, 3.80e-24, 2.09e+03, 2.09e+03]    [2.81e-02, 1.04e-03, 3.80e-24, 2.09e+03, 2.09e+03]    []  
10000     [2.16e-02, 1.05e-03, 1.54e-24, 2.09e+03, 2.09e+03]    [2.16e-02, 1.05e-03, 1.54e-24, 2.09e+03, 2.09e+03]    []  
7000      [3.32e-04, 2.00e-04, 1.59e-06, 4.14e+00, 4.14e+00]    [3.32e-04, 2.00e-04, 1.59e-06, 4.14e+00, 4.14e+00]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       11000     [3.54e-04, 1.05e-03, 3.53e-24, 2.09e+03, 2.09e+03]    [3.54e-04, 1.05e-03, 3.53e-24, 2.09e+03, 2.09e+03]    []  
11000     [4.71e-06, 1.05e-03, 5.91e-30, 2.09e+03, 2.09e+03]    [4.71e-06, 1.05e-03, 5.91e-30, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            8000      [2.72e-04, 9.44e-05, 3.59e-06, 3.46e+00, 3.46e+00]    [2.72e-04, 9.44e-05, 3.59e-06, 3.46e+00, 3.46e+00]    []  
                                                                                                                           12000     [2.02e-03, 1.05e-03, 5.46e-24, 2.09e+03, 2.09e+03]    [2.02e-03, 1.05e-03, 5.46e-24, 2.09e+03, 2.09e+03]    []  
12000     [1.47e-04, 1.05e-03, 1.03e-29, 2.09e+03, 2.09e+03]    [1.47e-04, 1.05e-03, 1.03e-29, 2.09e+03, 2.09e+03]    []  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       13000     [1.63e-04, 1.05e-03, 1.23e-23, 2.09e+03, 2.09e+03]    [1.63e-04, 1.05e-03, 1.23e-23, 2.09e+03, 2.09e+03]    []  
48000     [8.69e-05, 8.77e-05, 3.45e-06, 2.07e-01, 2.07e-01]    [8.69e-05, 8.77e-05, 3.45e-06, 2.07e-01, 2.07e-01]    []  
40000     [2.90e-04, 2.08e-04, 9.23e-05, 4.05e-01, 4.05e-01]    [2.90e-04, 2.08e-04, 9.23e-05, 4.05e-01, 4.05e-01]    []  
42000     [1.14e-04, 1.72e-05, 1.25e-08, 4.26e-02, 4.26e-02]    [1.14e-04, 1.72e-05, 1.25e-08, 4.26e-02, 4.26e-02]    []  
47000     [9.25e-05, 3.37e-05, 3.66e-07, 1.72e-01, 1.72e-01]    [9.25e-05, 3.37e-05, 3.66e-07, 1.72e-01, 1.72e-01]    []  
                                                                                                                           49000     [1.01e-04, 8.68e-05, 3.95e-06, 2.37e-01, 2.37e-01]    [1.01e-04, 8.68e-05, 3.95e-06, 2.37e-01, 2.37e-01]    []  
41000     [2.90e-04, 1.90e-04, 9.92e-05, 4.57e-01, 4.57e-01]    [2.90e-04, 1.90e-04, 9.92e-05, 4.57e-01, 4.57e-01]    []  
43000     [6.10e-05, 7.50e-06, 4.18e-08, 4.04e-02, 4.04e-02]    [6.10e-05, 7.50e-06, 4.18e-08, 4.04e-02, 4.04e-02]    []  
48000     [9.06e-05, 3.54e-05, 3.20e-07, 1.63e-01, 1.63e-01]    [9.06e-05, 3.54e-05, 3.20e-07, 1.63e-01, 1.63e-01]    []  
                                                                                                                           50000     [9.01e-05, 8.39e-05, 3.74e-06, 2.07e-01, 2.07e-01]    [9.01e-05, 8.39e-05, 3.74e-06, 2.07e-01, 2.07e-01]    []  

Best model at step 47000:
  train loss: 4.04e-01
  test loss: 4.04e-01
  test metric: []

'train' took 8262.175883 s

[W 2023-10-08 16:32:51,387] Trial 5 failed with parameters: {'num_domain': 9081, 'num_boundary': 9551, 'lr': 0.00016031268529168992} because of the following error: ValueError('operands could not be broadcast together with shapes (423763,) (847526,) ').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 71, in objective
    RMSE = np.sqrt(np.mean((v_test - v_pred_test)**2))
ValueError: operands could not be broadcast together with shapes (423763,) (847526,) 
[W 2023-10-08 16:32:51,391] Trial 5 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 71, in objective
    RMSE = np.sqrt(np.mean((v_test - v_pred_test)**2))
ValueError: operands could not be broadcast together with shapes (423763,) (847526,) 
                                                                                                                           42000     [2.34e-04, 1.77e-04, 8.36e-05, 3.50e-01, 3.50e-01]    [2.34e-04, 1.77e-04, 8.36e-05, 3.50e-01, 3.50e-01]    []  
44000     [3.99e-05, 4.18e-06, 5.38e-09, 3.94e-02, 3.94e-02]    [3.99e-05, 4.18e-06, 5.38e-09, 3.94e-02, 3.94e-02]    []  
49000     [9.11e-05, 3.69e-05, 3.14e-07, 2.19e-01, 2.19e-01]    [9.11e-05, 3.69e-05, 3.14e-07, 2.19e-01, 2.19e-01]    []  
                                                                                                                                                                                                                                                      43000     [2.42e-04, 1.68e-04, 8.42e-05, 3.70e-01, 3.70e-01]    [2.42e-04, 1.68e-04, 8.42e-05, 3.70e-01, 3.70e-01]    []  
45000     [2.55e-05, 1.48e-06, 5.13e-08, 4.73e-02, 4.73e-02]    [2.55e-05, 1.48e-06, 5.13e-08, 4.73e-02, 4.73e-02]    []  
50000     [9.16e-05, 3.96e-05, 3.79e-07, 1.79e-01, 1.79e-01]    [9.16e-05, 3.96e-05, 3.79e-07, 1.79e-01, 1.79e-01]    []  

Best model at step 48000:
  train loss: 3.26e-01
  test loss: 3.26e-01
  test metric: []

'train' took 8645.890795 s

[W 2023-10-08 16:37:26,557] Trial 2 failed with parameters: {'num_domain': 27385, 'num_boundary': 3765, 'lr': 0.0002225195523262972} because of the following error: ValueError('operands could not be broadcast together with shapes (423763,) (847526,) ').
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 71, in objective
    RMSE = np.sqrt(np.mean((v_test - v_pred_test)**2))
ValueError: operands could not be broadcast together with shapes (423763,) (847526,) 
[W 2023-10-08 16:37:26,562] Trial 2 failed with value None.
Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 71, in objective
    RMSE = np.sqrt(np.mean((v_test - v_pred_test)**2))
ValueError: operands could not be broadcast together with shapes (423763,) (847526,) 
                                                                                                                           46000     [2.80e-05, 2.43e-06, 1.14e-07, 5.83e-02, 5.83e-02]    [2.80e-05, 2.43e-06, 1.14e-07, 5.83e-02, 5.83e-02]    []  
44000     [2.04e-04, 1.53e-04, 7.81e-05, 3.15e-01, 3.15e-01]    [2.04e-04, 1.53e-04, 7.81e-05, 3.15e-01, 3.15e-01]    []  
                                                                                                                           47000     [3.02e-01, 9.31e-03, 5.35e-07, 1.03e+02, 1.03e+02]    [3.02e-01, 9.31e-03, 5.35e-07, 1.03e+02, 1.03e+02]    []  
45000     [1.76e-04, 1.44e-04, 8.09e-05, 2.96e-01, 2.96e-01]    [1.76e-04, 1.44e-04, 8.09e-05, 2.96e-01, 2.96e-01]    []  
                                                                                                                           48000     [5.17e-04, 5.95e-04, 1.04e-06, 2.36e+00, 2.36e+00]    [5.17e-04, 5.95e-04, 1.04e-06, 2.36e+00, 2.36e+00]    []  
46000     [1.60e-04, 1.40e-04, 8.03e-05, 2.82e-01, 2.82e-01]    [1.60e-04, 1.40e-04, 8.03e-05, 2.82e-01, 2.82e-01]    []  
                                                                                                                           49000     [3.79e-04, 4.49e-04, 1.23e-06, 1.14e-01, 1.14e-01]    [3.79e-04, 4.49e-04, 1.23e-06, 1.14e-01, 1.14e-01]    []  
47000     [1.48e-04, 1.36e-04, 7.90e-05, 2.69e-01, 2.69e-01]    [1.48e-04, 1.36e-04, 7.90e-05, 2.69e-01, 2.69e-01]    []  
                                                                                                                                                                                                                                                      50000     [9.76e-05, 1.70e-05, 2.15e-07, 9.27e-02, 9.27e-02]    [9.76e-05, 1.70e-05, 2.15e-07, 9.27e-02, 9.27e-02]    []  

Best model at step 29000:
  train loss: 6.86e-02
  test loss: 6.86e-02
  test metric: []

'train' took 9491.563651 s

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1965, in _exec_single_context
    self.dialect.do_execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 921, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 73, in _create_scoped_session
    session.commit()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1923, in commit
    trans.commit(_to_root=True)
  File "<string>", line 2, in commit
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1239, in commit
    self._prepare_impl()
  File "<string>", line 2, in _prepare_impl
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1214, in _prepare_impl
    self.session.flush()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4179, in flush
    self._flush(objects)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4314, in _flush
    with util.safe_reraise():
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4275, in _flush
    flush_context.execute()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 85, in save_obj
    _emit_update_statements(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 909, in _emit_update_statements
    c = connection.execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1412, in execute
    return meth(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 516, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1635, in _execute_clauseelement
    ret = self._execute_context(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1844, in _execute_context
    return self._exec_single_context(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1984, in _exec_single_context
    self._handle_dbapi_exception(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2339, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1965, in _exec_single_context
    self.dialect.do_execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 921, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE trials SET state=?, datetime_complete=? WHERE trials.trial_id = ?]
[parameters: ('FAIL', '2023-10-08 16:53:12.095482', 5)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 212, in _run_trial
    frozen_trial = _tell_with_warning(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_tell.py", line 176, in _tell_with_warning
    study._storage.set_trial_state_values(frozen_trial._trial_id, state, values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_cached_storage.py", line 187, in set_trial_state_values
    return self._backend.set_trial_state_values(trial_id, state=state, values=values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 640, in set_trial_state_values
    with _create_scoped_session(self.scoped_session) as session:
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 90, in _create_scoped_session
    raise optuna.exceptions.StorageInternalError(message) from e
optuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 244, in _run_trial
    assert False, "Should not reach."
AssertionError: Should not reach.
48000     [1.37e-04, 1.32e-04, 7.05e-05, 2.57e-01, 2.57e-01]    [1.37e-04, 1.32e-04, 7.05e-05, 2.57e-01, 2.57e-01]    []  
                                                                                                                           49000     [1.26e-04, 1.26e-04, 6.12e-05, 2.47e-01, 2.47e-01]    [1.26e-04, 1.26e-04, 6.12e-05, 2.47e-01, 2.47e-01]    []  
                                                                                                                                                                                                                                                      50000     [1.17e-04, 1.21e-04, 5.55e-05, 2.36e-01, 2.36e-01]    [1.17e-04, 1.21e-04, 5.55e-05, 2.36e-01, 2.36e-01]    []  

Best model at step 50000:
  train loss: 4.72e-01
  test loss: 4.72e-01
  test metric: []

'train' took 9958.460586 s

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1965, in _exec_single_context
    self.dialect.do_execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 921, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 73, in _create_scoped_session
    session.commit()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1923, in commit
    trans.commit(_to_root=True)
  File "<string>", line 2, in commit
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1239, in commit
    self._prepare_impl()
  File "<string>", line 2, in _prepare_impl
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1214, in _prepare_impl
    self.session.flush()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4179, in flush
    self._flush(objects)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4314, in _flush
    with util.safe_reraise():
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4275, in _flush
    flush_context.execute()
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 85, in save_obj
    _emit_update_statements(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 909, in _emit_update_statements
    c = connection.execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1412, in execute
    return meth(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 516, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1635, in _execute_clauseelement
    ret = self._execute_context(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1844, in _execute_context
    return self._exec_single_context(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1984, in _exec_single_context
    self._handle_dbapi_exception(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2339, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1965, in _exec_single_context
    self.dialect.do_execute(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 921, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) attempt to write a readonly database
[SQL: UPDATE trials SET state=?, datetime_complete=? WHERE trials.trial_id = ?]
[parameters: ('FAIL', '2023-10-08 17:00:36.068653', 4)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 212, in _run_trial
    frozen_trial = _tell_with_warning(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_tell.py", line 176, in _tell_with_warning
    study._storage.set_trial_state_values(frozen_trial._trial_id, state, values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_cached_storage.py", line 187, in set_trial_state_values
    return self._backend.set_trial_state_values(trial_id, state=state, values=values)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 640, in set_trial_state_values
    with _create_scoped_session(self.scoped_session) as session:
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/storages/_rdb/storage.py", line 90, in _create_scoped_session
    raise optuna.exceptions.StorageInternalError(message) from e
optuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/itf-fi-ml/home/adamjak/heart/MSc/deepxDE/optuna_tune.py", line 77, in <module>
    study.optimize(objective, n_trials=10)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/study.py", line 442, in optimize
    _optimize(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/itf-fi-ml/home/adamjak/.conda/envs/heartpinn/lib/python3.10/site-packages/optuna/study/_optimize.py", line 244, in _run_trial
    assert False, "Should not reach."
AssertionError: Should not reach.
19000     [2.58e-03, 1.05e-03, 7.79e-31, 2.09e+03, 2.09e+03]    [2.58e-03, 1.05e-03, 7.79e-31, 2.09e+03, 2.09e+03]    []  
20000     [3.65e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [3.65e-05, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
20000     [6.05e-04, 1.05e-03, 1.02e-31, 2.09e+03, 2.09e+03]    [6.05e-04, 1.05e-03, 1.02e-31, 2.09e+03, 2.09e+03]    []  
14000     [6.86e-04, 5.55e-04, 5.40e-06, 1.11e+00, 1.11e+00]    [6.86e-04, 5.55e-04, 5.40e-06, 1.11e+00, 1.11e+00]    []  
21000     [2.08e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.08e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
21000     [9.13e-02, 1.04e-03, 9.89e-32, 2.09e+03, 2.09e+03]    [9.13e-02, 1.04e-03, 9.89e-32, 2.09e+03, 2.09e+03]    []  
22000     [2.02e-02, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [2.02e-02, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
15000     [5.77e-04, 4.64e-04, 6.94e-06, 7.15e-01, 7.15e-01]    [5.77e-04, 4.64e-04, 6.94e-06, 7.15e-01, 7.15e-01]    []  
22000     [3.98e-02, 1.04e-03, 5.52e-32, 2.09e+03, 2.09e+03]    [3.98e-02, 1.04e-03, 5.52e-32, 2.09e+03, 2.09e+03]    []  
23000     [8.16e-02, 1.03e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [8.16e-02, 1.03e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
23000     [1.43e-03, 1.05e-03, 3.80e-32, 2.09e+03, 2.09e+03]    [1.43e-03, 1.05e-03, 3.80e-32, 2.09e+03, 2.09e+03]    []  
16000     [3.80e-04, 3.74e-04, 2.20e-05, 5.53e-01, 5.53e-01]    [3.80e-04, 3.74e-04, 2.20e-05, 5.53e-01, 5.53e-01]    []  
24000     [1.26e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [1.26e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
24000     [7.82e-05, 1.05e-03, 7.07e-33, 2.09e+03, 2.09e+03]    [7.82e-05, 1.05e-03, 7.07e-33, 2.09e+03, 2.09e+03]    []  
25000     [8.94e-01, 9.50e-04, 0.00e+00, 2.09e+03, 2.09e+03]    [8.94e-01, 9.50e-04, 0.00e+00, 2.09e+03, 2.09e+03]    []  
17000     [2.76e-04, 3.27e-04, 9.84e-06, 8.52e-01, 8.52e-01]    [2.76e-04, 3.27e-04, 9.84e-06, 8.52e-01, 8.52e-01]    []  
25000     [1.13e-01, 1.03e-03, 3.57e-33, 2.09e+03, 2.09e+03]    [1.13e-01, 1.03e-03, 3.57e-33, 2.09e+03, 2.09e+03]    []  
26000     [9.29e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    [9.29e-03, 1.05e-03, 0.00e+00, 2.09e+03, 2.09e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:40:03,624] A new study created in RDB with name: heartpinn
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:40:12,500] Using an existing study with name 'heartpinn' instead of creating a new one.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000145 s

[I 2023-10-08 17:40:16,703] Using an existing study with name 'heartpinn' instead of creating a new one.
Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [9.16e+01, 4.54e-04, 3.89e-07, 3.62e+03, 3.62e+03]    [9.16e+01, 4.54e-04, 3.89e-07, 3.62e+03, 3.62e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:40:21,132] Using an existing study with name 'heartpinn' instead of creating a new one.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000146 s

Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000230 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.79e+01, 3.83e-05, 3.71e-07, 3.60e+03, 3.60e+03]    [3.79e+01, 3.83e-05, 3.71e-07, 3.60e+03, 3.60e+03]    []  
Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [8.35e+01, 4.63e-04, 2.68e-07, 3.61e+03, 3.61e+03]    [8.35e+01, 4.63e-04, 2.68e-07, 3.61e+03, 3.61e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:40:29,679] Using an existing study with name 'heartpinn' instead of creating a new one.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000146 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [2.12e+01, 1.59e-04, 1.81e-07, 3.60e+03, 3.60e+03]    [2.12e+01, 1.59e-04, 1.81e-07, 3.60e+03, 3.60e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:40:39,076] Using an existing study with name 'heartpinn' instead of creating a new one.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000583 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [5.08e+00, 2.39e-04, 1.90e-07, 3.60e+03, 3.60e+03]    [5.08e+00, 2.39e-04, 1.90e-07, 3.60e+03, 3.60e+03]    []  
[I 2023-10-08 17:40:45,561] Using an existing study with name 'heartpinn' instead of creating a new one.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000124 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [1.89e+01, 9.80e-05, 4.34e-07, 3.59e+03, 3.59e+03]    [1.89e+01, 9.80e-05, 4.34e-07, 3.59e+03, 3.59e+03]    []  
[I 2023-10-08 17:40:49,444] Using an existing study with name 'heartpinn' instead of creating a new one.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000337 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [3.28e+01, 5.58e-05, 8.19e-08, 3.60e+03, 3.60e+03]    [3.28e+01, 5.58e-05, 8.19e-08, 3.60e+03, 3.60e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000185 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [6.53e+01, 1.77e-04, 9.70e-08, 3.62e+03, 3.62e+03]    [6.53e+01, 1.77e-04, 9.70e-08, 3.62e+03, 3.62e+03]    []  
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:41:34,996] Using an existing study with name 'heartpinn' instead of creating a new one.
Using backend: pytorch
Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.
paddle supports more examples now and is recommended.
[I 2023-10-08 17:41:43,787] Using an existing study with name 'heartpinn' instead of creating a new one.
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000487 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.67e+01, 5.48e-05, 3.96e-08, 3.61e+03, 3.61e+03]    [4.67e+01, 5.48e-05, 3.96e-08, 3.61e+03, 3.61e+03]    []  
Set the default float type to float32
self.triangles shape: (141944, 3)
Compiling model...
'compile' took 0.000363 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

Step      Train loss                                            Test loss                                             Test metric
0         [4.03e+01, 5.01e-05, 3.43e-07, 3.61e+03, 3.61e+03]    [4.03e+01, 5.01e-05, 3.43e-07, 3.61e+03, 3.61e+03]    []  
